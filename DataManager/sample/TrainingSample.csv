Python$C$C++$backend$embeddedsystems,"Python vs C/C++.The case for Python
Python is the most popular introductory programming language at the top computer science (CS) departments in the United States.It's dramatically more likely that a recent graduate will understand how to code in Python than in C/C++.The Barr Group study shows that over 95% of the code for embedded systems is written in C/C++. This legacy on embedded systems might be hard for Python to overcome, but its attributes and the sheer number of people coding in Python might help it overtake C/C++ in the future.
While C/C++ is slow to write, error prone, and frequently unreadable, Python is known for its writability, error reduction, and readability. The importance of readability can't be overemphasized: when you're working in a team, readability is paramount to maintaining the code. It has to be easily decipherable unless you're willing to shell out more time and money on debugging and quality assurance. The design reuse of Python far outclasses C/C++, and in today's Agile environments design reuse can be the difference between staying ahead or falling behind the competition."
gaming$coder,"How gaming turned me into a coder
I think the first word I learned to type fast—and I mean really fast—was ""fireball.""
Like most of us, I started my typing career with a ""hunt-and-peck"" technique, using my index fingers and keeping my eyes focused on the keyboard to find letters as I needed them. It's not a technique that allows you to read and write at the same time; you might call it half-duplex. It was okay for typing cd and dir, but it wasn't nearly fast enough to get ahead in the game. Especially if that game was a MUD.
Gaming with multi-user dungeons
MUD is short for multi-user dungeon. Or multi-user domain, depending on who (and when) you ask. MUDs are text-based adventure games, like Colossal Cave Adventure and Zork, which you may have heard about in Season 2 Episode 1 of Command Line Heroes. But MUDs have an extra twist: you aren't the only person playing them. They allow you to group with others to tackle particularly nasty beasts, trade goods, and make new friends. They were the great granddaddies of modern massively multiplayer online role-playing games (MMORPGs) like Everquest and World of Warcraft. And, for an aspiring command-line hero, they offered an experience those modern games still don't.
My ""home MUD"" was NyxMud, which you could access by telnetting to port 2000 of nyx.cs.du.edu. It was the first command line I ever mastered. In a lot of ways, it allowed me to be a hero—or at least play the part of one.
One special quality of NyxMud was that every time you connected to play, you started with an empty inventory. The gold you collected was still there from your last session, but none of your hard-won weapons, armor, or magical items were. So, at the end of every session, you had to make it back to a store to sell everything… and you would get a fraction of what you paid. If you were killed, the first player who encountered your lifeless body could take everything you had.
This made the game extremely sticky. Selling everything and quitting was a horrible thing to do, fiscally speaking. It meant that your session had to be profitable. If you didn't earn enough gold through looting and quests between the time you bought and sold your gear, you wouldn't be able to equip yourself as well the next time you played. If you died, it was even worse: You might find yourself killing balls of slime with a newbie sword as you scraped together enough gold for better gear.
I never wanted to ""pay the store tax"" by selling my gear, which meant a lot of late nights and sleeping through morning biology classes. Every modern game designer wants you to say, ""I can't have dinner now, Dad, I have to keep playing or I'm in big trouble."" NyxMud had me so hooked that I was saying that several decades ago.
So when it came time to ""cast fireball"" or die an imminent and ruinous death, I was forced to learn how to type properly. It also forced me to take a social approach to the game—having friends around to fight off scavengers allowed me to reclaim my gear when I died.
Command-line heroes all have some things in common: They work with others and they type wicked fast. NyxMud trained me to do both.
From gamer to creator 
NyxMud was not the largest MUD by any measure. But it was still an expansive world filled with hundreds of areas and dozens of epic adventures, each one tailored to a different level of a player's advancement. Over time, it became apparent that not all these areas were created by the same person. The term ""user-generated content"" was yet to be invented, but the concept was dead simple even to my young mind: This entire world was created by a group of people, other players.
Once you completed each of the challenging quests and achieved level 20, you became a wizard. This was a singularity of sorts, beyond which existed a reality known only to a few. During lunch breaks at school, my circle of friends would muse about the powers of a wizard; you see, we knew wizards could create rooms, beasts, items, and quests. We knew they could kill players at will. We really didn't know much else about their powers. The whole thing was shrouded in mystery.
In our group of high school friends, Eddie was the first to become a wizard. His flaunting and taunting threw us into overdrive, and Jared was quick to follow. I was last, but only by a day or two. Now that 25 years have passed, let's just call it a three-way tie. We discovered it was pretty much what we thought. We could create rooms, beasts, items, and quests. We could kill players. Oh, and we could become invisible. In NyxMud, that was just about it.
Wizards used the Wand of Creation, an item invented by Quasi (rhymed with ""crazy""), the grand wizard. He alone had access to the code for the engine, due to a strict policy set by the administrator of the Nyx system where it ran. So, he created a complicated, magical object that would allow users to generate new game elements. This wand, when invoked, ran the wizard through a menu-based workflow for creating rooms and objects, establishing quest objectives, and designing terrible monsters.
Having that magical wand was enough. I immediately set to work creating new lands and grand adventures across a series of islands, each with a different, exotic climate and theme. I found immense pleasure in hovering, invisible, as the savage beasts from my imagination would slay intrepid adventurers over and over again. But it was even better to see players persevere after a hard battle, knowing I had tweaked and tuned my quests to be just within the realm of possibility.
Being accepted into this elite group of creators was one of the more rewarding and satisfying moments of my young life. Each new wizard would have to pass my test, spending countless hours and sleepless nights, just as I did, to complete the quests of the wizards before me. I had proven my value through dedication and contribution. It was just a game, but it was also a community—the first one I encountered, and the one that showed me how powerful a properly run meritocracy could be.
From creator to coder
NyxMud was based on the LPMud codebase, which was created by Lars Pensjö. LPMud was not the first MUD software developed, but it contained one very important innovation: It allowed players to code the game from within the game. It accomplished this by separating the mudlib, which contained all the content and user-facing functionality, from the driver, which acted as a real-time interpreter for the mudlib and provided access to basic network and storage resources. This architecture meant the mudlib could be edited on-the-fly by virtually untrusted people (e.g., players like me) who could augment the game experience without being able to do anything particularly harmful to the server it was running on. The driver provided an ""air gap.""
This air gap was not enough for NyxMud; it was allowed to exist only if a single person could be trusted to write all the code. In most LPMud systems, players who became wizards could use ls, cd, and ed to traverse the mudlib and modify files, all from the same command line they had used countless times for casting fireballs and drinking potions. Quasi went to great lengths to modify the Nyx mudlib so wizards couldn't traipse around the system with a full set of sharp tools. The Wand of Creation was born.







As a wizard who hadn't played any other MUDs, I didn't miss what I never had. Besides, I didn't have a way to access any systems at the time—telnet was disabled on Nyx, which was my only connection to the internet. But I did have access to Usenet, which provided me with The Totally Unofficial List of Internet Muds. It was clear there was more of the MUD universe for me to discover. I read all the documentation about mudlibs I could get my hands on and got some exposure to LPC, the niche programming language used to create new content.
I convinced my dad to make an investment in my future by paying for a shell account at Netcom (remember that?). With that account, I could connect to any MUD I wanted, and, based on several strong recommendations, I chose Viking MUD. It still exists today. It was a real MUD, the bleeding edge, and it showcased the true potential of a universe built with code instead of the limited menu system of a magical wand. But, to be honest, I never got very far as a player. I really wanted to learn how to code, and I didn't want to slay slimeballs with a noobsword for hours to get there.
There was a very small window of time—between February and August 1992, according to Lauren P. Burka's Mud Timeline—where the perfect place existed for my exploration. The Mud Institute (TMI for short) was a very special MUD designed to teach people how to program in LPC, illuminating the darkest corners of the mudlib. It offered immediate omnipotence to all who applied and built a community for the development of a new generation of LPMuds.
This was my first exposure to C programming, as LPC was essentially a flavor of C that shared the same types, control structures, and syntax. It was C with training wheels, designed for rapid creation of content but allowing coders to develop intricate game scenarios (if they had the chops). I had always seen the curly brace on my keyboard, and now I knew what it was used for. The only thing I can remember creating was a special vending machine, somewhat inspired by the Wand of Creation, that would create the monster of your choice on-the-spot.
TMI was not a long-lasting phenomenon; in fact, it was gone almost before I had a chance to discover it. It quickly abandoned its educational charter, although its efforts were ultimately productive with the release of MudOS—which still lives through its modern-day descendant, FluffOS. But what a treasure trove of knowledge about a highly specific subject! Immediately after logging in, I was presented with a complete set of developer tools, a library of instructional materials, and a ton of interesting sample code to learn from.
I never talked to anyone or asked for any help, and I never had to. The community had published just enough resources for me to get started by myself. I was able to learn the basics of structured programming without a textbook or teacher, all within the context of a fantastical computer game. As a result, I have had a long and (mostly) fulfilling career in technology.
The line from Field of Dreams, ""if you build it, they will come,"" is almost certainly untrue for communities. The folks at The Mud Institute built the makings of a great community, but I can't say they were successful. They didn't become a widely known wizarding school—in fact, it's really hard to find any information about TMI at all. If you build it, they may not come; if they do, you may still fail. But it still accomplished something wonderful that its creators never thought to predict: It got me excited about programming."
3Dprinting$technologies,"The first open source 3D printer filament
Until very recently, 3D printing companies sold only proprietary hardware and materials (including plastic filament) to print with them. This plastic was sold at exorbitant prices (up to hundreds of U.S. dollars per kg), even if it was a common material like ABS (the plastic used for Lego blocks). These companies were following the path of traditional desktop printing companies that rake in large profits selling toner and ink. Some companies still attempt to extort their customers by threatening warranty loss if they use filament from other manufacturers.
Fortunately, along with the radical price declines and more rapid innovation that came with the release of the open RepRap 3D printer, there was an explosion of filament manufacturers that provided a wide array of new filaments for these machines.
Open source RepRap 3D printers and their commercial derivatives now dominate the desktop 3D printing market. This was a great start, but speaking as a material scientist, the fact that all of the filaments were proprietary was a major problem. Most people do not know what they are printing with, which is a challenge as we begin to rely more and more on distributed manufacturing of our own products. Even if you make the filament yourself with a recyclebot from waste plastic, you only know the basic polymer, not anything about additives, coloring agents, etc.
A detailed list and OpenSCAD source code for polymer recycling codes have been developed for 3D printer users to integrate directly into their own designs for eventual recycling. For the detailed recycling code system to work perfectly, you still need to know what you are printing with.
Fortunately, IC3D has set a precedent with IC3D ABS, the first Open Source Hardware Association (OSHWA) certified filament for 3D printing. Just as open source produced rapid innovation in 3D printers, IC3D's filament promises to advance material consistency, precision, diversity, and quality across the industry. To see exactly what goes into its filament and how it's processed, you can follow the complete source code on GitHub.
While it is unlikely individuals will recreate IC3D's industrial line, those developing the next variants of smaller open source recyclebots can learn a lot from IC3D. Innovations from the community will feed back to IC3D as it moves to more advanced materials, which will benefit the entire 3D printing community.
What makes IC3D's release particularly interesting is that it is now possible to use a completely libre 3D printing toolchain to cut the costs of commercial products. You can use free and open source software like OpenSCAD or FreeCAD to make your design and slice it with an open slicer (e.g., Cura or Slic3r) running on your open source laptop with your favorite version of Linux. Then you can use an open source 3D printer controller like Franklin or Printrun to print out your masterpiece on your open source 3D printer (e.g., a Lulzbot or Prusa) using IC3D's filament.
IC3D created this open source filament because it fits its mission ""to enable curious tinkerers and inspire young inventors with the high-quality 3D printing gear they will enjoy, while exploring and expressing their inner creativity to the world.""
For those who appreciate quality filament as well as the value of open source, by choosing IC3D ABS filament you can help support innovation and freedom. I look forward to seeing IC3D make the rest of its filament line open source and begin to work on advanced composite and high-performance filaments. Hopefully, we will also see other filament manufacturers follow IC3D's lead, also, as its value becomes clear."
JavaScript$Chart.js$webdevelopment$frontend,"3 top open source JavaScript chart libraries.Chart.js
Chart.js is an open source JavaScript library that allows you to create animated, beautiful, and interactive charts on your application. It's available under the MIT License.
With Chart.js, you can create various impressive charts and graphs, including bar charts, line charts, area charts, linear scale, and scatter charts. It is completely responsive across various devices and utilizes the HTML5 Canvas element for rendering."
JavaScript$Chartist.js$frontend$webdevelopment,"3 top open source JavaScript chart libraries.Chartist.js
Chartist.js is a simple JavaScript animation library that allows you to create customizable and beautiful responsive charts and other designs. The open source library is available under the WTFPL or MIT License.
The library was developed by a group of developers who were dissatisfied with existing charting tools, so it offers wonderful functionalities to designers and developers.
After including the Chartist.js library and its CSS files in your project, you can use them to create various types of charts, including animations, bar charts, and line charts. It utilizes SVG to render the charts dynamically."
JavaScript$D3.js$frontend$webdevelopment,"3 top open source JavaScript chart libraries.D3.js
D3.js is another great open source JavaScript chart library. It's available under the BSD license. D3 is mainly used for manipulating and adding interactivity to documents based on the provided data.
You can use this amazing 3D animation library to visualize your data using HTML5, SVG, and CSS and make your website appealing. Essentially, D3 enables you to bind data to the Document Object Model (DOM) and then use data-based functions to make changes to the document.The main concept in using the D3 library is to first apply CSS-style selections to point to the DOM nodes and then apply operators to manipulate them—just like in other DOM frameworks like jQuery."
Arduino$RaspberryPi$electronics,"Arduino or Raspberry Pi: Which is best for beginners?
So you want to get started with hobbyist electronics? The world is rich with options, two of the most prominent being the Raspberry Pi and the Arduino. But which one makes sense for you?
Here are some important questions you should ask yourself which may help you make your decision:
Is this for a child? Consider not only that child's age, experience, and patience, but also your own. How much time are you willing to spend working with the child on the projects?
Are there parts you'll need that are easier to obtain for one than the other? There's a vast Arduino community that has created shields (add-on parts that perform specific roles) for just about anything you can imagine. The Raspberry Pi community is large and growing, but you may find is easier to find particular parts for a given project for one than the other.
Do you need or want any/all the built-in goodness of a Raspberry Pi, like HDMI and USB ports?
What's your final goal? Are you hoping to learn programming? Do you want to play with a device for a while and then repurpose it? (The Arduino is good at doing one thing at a time, but not so good at multiple functions if you decide you want to move on to a more complicated project without buying a new board.)
"
Python$backend$C$C++,"Improving Python's speed
So the main case against Python versus C/C++ is its runtime speed, but there are several ways to optimize the code so it runs more efficiently. Aside from libraries like Theano, there are optimizing extensions for Python like Cython, which is essentially Python with static typing and keywords to run math more quickly. Because Cython is statically typed, you can easily compile to C/C++ and run at C/C++ speeds.
Just-In-Time (JIT) compilers are another good way to improve Python's runtime speed. These compilers work in parallel with Python's interpreter to generate compiled machine instructions for code inside loops. This allows subsequent passes by the interpreter to execute faster. The PyPy JIT compiler is able to increase Python's execution speed by nearly a factor of two. JIT compilers should only be used if there's enough space, though, and embedded systems don't usually have a lot of that to spare. The best optimization is to use better data structures and algorithms, but this is the hardest task in software design and implementation, so it might be best to utilize one of the tools above, depending on your skill level.
"
PHP$Laravel$webdevelopment$PHPframework$frontend,"Laravel
Laravel, which is known as the ""PHP framework for Web Artisans,"" offers an excellent community and wins as the most popular framework. (One of the leading Laravel developers on Livecoding.tv is Sfiskell.)
In May 2015 Laravel announced that version 5.1 will offer long-term support for two years. Version 5.2 rolled out in December 2015. Many hosting companies provide Laravel support and offer hosting solutions for Laravel applications. Check out the Built with Laravel site to see great example projects."
Python$Qt$PyQt$backend$GUI$Pythonframework ,"PyQt
PyQt implements the popular Qt library, and so if you are familiar with Qt development in another language, perhaps from developing native applications for KDE or another Qt-using desktop environment, you may already be familiar with Qt. This opens up the possibility of developing applications in Python which have a familiar look and feel across many platforms, while taking advantage of the tools and knowledge of the large Qt community.
PyQt is dual licensed under both a commercial and GPL license, not unlike Qt project itself, and the primary company supporting PyQt offers a license FAQ to help understand what this means for your application. For another option to use the Qt libraries with Python, consider checking out PySide, which is available under the LPGL."
Python$C$C++$backend$C/C++$embeddedsystems,"Python vs C/C++.The case for C/C++
The case for C/C++ is pretty obvious: it creates more compact and faster runtime code, and it's already the language of choice for 95% of embedded system code, so it has a whole legacy that Python will have to overcome. When it comes to speed, however, runtime speed isn't the only aspect of development to consider—you also have to consider development speed. While Python may be less efficient than C/C++ at runtime, during development it's much more efficient. Interpreters read each line of code, parse it, do runtime checks and call routines in order to execute the operations in the code. This is a lot more activity than what you get from running C/C++ code, where the same line of code might be compiled into just a couple of instructions. This can lead to slower runtime speeds and higher energy consumption with Python."
Python$MachineLearning$DataAnalysis ,"Python's machine learning and data analysis packages
Even though Python is naturally disposed toward machine learning, it has packages that further optimize this attribute. PyBrain is a modular machine learning library that offers powerful algorithms for machine learning tasks. The algorithms are intuitive and flexible, but the library also has a variety of environments to test and compare your machine learning algorithms.
Scikit-learn is the most popular machine learning library for Python. Built on NumPy and SciPy, scikit-learn offers tools for data mining and analysis that bolster Python's already-superlative machine learning usability. NumPy and SciPy impress on their own. They are the core of data analysis in Python and any serious data analyst is likely using them raw, without higher-level packages on top, but scikit-learn pulls them together in a machine learning library with a lower barrier to entry.
When it comes to data analysis, Python receives a welcome boost from several different packages. Pandas, one of its most well-known data analysis packages, gives Python high-performance structures and data analysis tools. As is the case with many of Python's packages, it shortens the time between starting a project and doing meaningful work within that project. If you really want to stick with Python and get as much R functionality as you can, RPy2 offers all of R's major functionality. This gives you the best of R in Python natively."
R$Python$MachineLearning$DataAnalysis ,"R's machine learning and data analysis packages
R, like Python, has plenty of packages to boost its performance. When it comes to approaching parity with Python in machine learning, Nnet improves R by supplying the ability to easily model neural networks. Caret is another package that bolsters R's machine learning capabilities, in this case by offering a set of functions that increase the efficiency of predictive model creation.
But data analysis is R's domain, and there are packages to improve it beyond its already-stellar capabilities. Packages for the pre-modeling, modeling, and post-modeling stages of data analysis are available. These packages are directed at specific tasks like data visualization, continuous regression, and model validation. With all of these cross-functional libraries and packages, which language should you drag into the data battlefield with you?"
PHP$Symfony$webdevelopment$PHPframework$frontend,"Symfony
Symfony is a set of reusable PHP components, enabling the developer to create scalable, high-performance applications. With 30 components from which to choose, the developer has the complete freedom to experiment and work in a RAD environment. Symfony APIs also enable easy integration with third-party applications, and it can be used with popular front-end frameworks, such as AngularJS.
Many popular projects, including Drupal and phpBB, also use a Symfony framework. In fact, Laravel, the most popular PHP framework, is build off of Symfony."
Python$Tkinter$backend$GUI$Pythonframework ,"Tkinter
If there were a single package which might be called the ""standard"" GUI toolkit for Python, it would be Tkinter. Tkinter is a wrapper around Tcl/Tk, a popular graphical interface and language pairing first popularized in the early 90s. The advantage of choosing Tkinter is the vast number of resources, including books and code samples, as well as a large community of users who may be able to help you out if you have questions. Simple examples are easy to get started with and fairly human-readable."
Python$backend$embeddedsystems,"Using Python to communicate with embedded systems
Python might be at its strongest when used as a communication middleman between the user and the embedded system they're working with. Sending messages through Python to or from an embedded system allows the user to automate testing. Python scripts can put the system into different states, set configurations, and test all sorts of real-world use cases. Python can also be used to receive embedded system data that can be stored for analysis. Programmers can then use Python to develop parameters and other methods of analyzing that data.
Currently the main debate about the merits of Python and C/C++ comes down to what's more important to your team: development speed or runtime speed. In the future, though, it might not be up to Python programmers to make their case for its use in embedded systems, but rather for embedded systems designers to figure out how to accommodate the relentlessly increasing popularity of Python."
Python$WxPython$GUI$Pythonframework$backend,"WxPython
WxPython brings the wxWidgets cross-platform GUI library from its native C++ to Python. WxPython is a slightly more modern approach to, which looks a little more native than Tkinter across different operating systems as it does not attempt to create its own set of widgets (although these can be themed to look much like native components). It's fairly easy to get started with as well, and has a growing developer community. You may need to bundle wxPython with your applications, as it is not automatically installed with Python.
Working with Python 3? Check out wxPython's Project Phoenix, a rewrite of the project which will work with the newest version of Python.
WxPython uses the wxWindows Library License of its parent project, which is OSI approved."
PHP$Yii$webdevelopment$PHPframework$frontend,"Yii
Created by Qiang Xue in 2008, Yii is a secure, fast, high-performance application/web-development framework. Yii utilizes the Composer dependency manager for PHP for handling different dependencies and installations (more on it later). Yii also is the fastest PHP framework, thanks to the lazy loading technique.
Another great feature of Yii is jQuery integration. The integration enables front-end developers to embrace the framework quickly, and it uses scaffolding to generate code. Similar to Symfony, Yii also utilizes components to enable rapid application development."
DevOps$coworking$teamwork,"6 DevOps mistakes to avoid
Adopting DevOps practices is fast becoming essential for digital businesses. Here are 6 common pitfalls, and how to avoid them.
As DevOps is increasingly recognized as a pillar of digital transformation, CIOs are becoming more enthusiastic about how DevOps and open source can transform enterprise culture. DevOps refers to a group of concepts that, while not all new, have catalyzed into a movement that is rapidly spreading throughout the technical community. Just look at the number of books and resources that are available to help you take your DevOps initiatives and practices to the next level.
Still, many people don't fully understand what DevOps means. And without the right knowledge and understanding, many DevOps initiatives fail to get off the ground. Here are six common mistakes—and how to avoid them—as you start your DevOps journey.
1. Creating a single DevOps team
The most common mistake organizations make is to create a brand-new team, tasked with addressing all the burdens of a DevOps initiative. It’s complicated enough for development and operations to deal with a new team that must coordinate with everyone. DevOps started with the idea of improving collaboration between teams involved in the development of software such as security, QA, and DBMS; it's not just about development and operations. If you create a new team to address DevOps, you’re just making things more complicated.
The secret sauce here is simplicity. Focus on culture by fostering a mindset of automation, quality, and stability. For example, you might involve everyone in a conversation about your architecture, or about common problems found in production environments in which all relevant players need to be aware of how their work affects others. DevOps is not just about a single dedicated team, but about organizations that evolve together as a DevOps team.
2. Focusing on too many tools
There are many tools available to help you implement DevOps initiatives. Don’t start your DevOps strategy by arguing about and selecting a bunch of different tools. You will soon see it is difficult to find the right tools for team and organizational processes because each team (developers, IT operations, security, etc.) will want to use a specific tool for their DevOps practices, even if it makes it harder to collaborate with other teams. Also, new tools are emerging all the time—there’s even one that will help integrate other tools.
Of course, you need to have the right tools for agile software development, continuous integration, deployment, version control, and so on. Not having the right tools can prevent teams from seeing the maximum benefit from their DevOps efforts. But simply buying a continuous deployment tool or deploying application containers isn’t enough to transition your organization to DevOps.
You will likely hear some vendors claim to have the perfect tool for your DevOps practices, but take an agnostic approach and keep in mind that there’s no single tool that can possibly cover everything you need.
3. Focusing on speed rather than safety and quality
Many organizations adopt a CI/CD strategy as a part of their DevOps initiatives because they need to reduce the amount of time it takes them to develop and deploy new application code. However, DevOps practitioners say that increasing speed at the expense of security and quality is a big mistake. Even if you build, test, and deploy new applications much faster in production, what if those applications don’t function as intended?

Many enterprises make the mistake of not following their security practices well in advance

To keep security and quality high, development teams should conduct testing as early in the development process as possible. More importantly, prove that the release candidate is ready for continuous delivery before deploying.
4. Allowing too many branches
In agile software development and DevOps practices, software (trunk) should always be deployable so developers are able to check into trunk—not feature branches—at least daily. If the build breaks, it can be fixed in ten minutes and a new developer can be on-boarded in one day, with a production-like environment on their developer workstation.
It can be difficult to break developers out of the habit of using branches if they are accustomed to a traditional waterfall environment, but limiting branches can be highly beneficial. If you favor trunk-based development, have developers work at all times in a mostly coherent, single version of your codebase.
According to Puppet's 2017 State of DevOps report: “We found that having branches or forks with very short lifetimes (less than a day) before merged into trunk, and less than three active branches in total, are important aspects of continuous delivery, and all contribute to higher performance. So does merging code into trunk or master on a daily basis.”
DevOps automates many aspects of how you treat your code between developers’ machines and production environments. Keeping many different conceptual flavors of your codebase around makes DevOps concerns an order of magnitude more complex.
5. Not involving the security team
DevOps involves more than simply putting the development and operations teams together; it is a continuous process of software development and automation, including security, audit, and compliance. Many enterprises make the mistake of not following their security practices well in advance.
In fact, a CA Technologies survey found that security concerns were the number-one obstacle to DevOps, cited by 38% of respondents. In the same vein, the Puppet survey found that high-performing DevOps teams “spend 50% less time remediating security issues than low performers.” Clearly, these high-performing teams have found ways to communicate their security objectives and to build security early into phases of their development process.
The DevOps practitioner should understand the processes, evaluate the controls, and identify the risks. In the end, security is always a part of DevOps practices like DevSecOps. For example, if you have any security issues in production, you can address them within your DevOps pipeline through the tools that the security team already uses. DevOps and security practices must be strictly followed. There should be no compromises.
6. Not preparing for culture change
Once you have the right tools for DevOps practices, you will probably face a new foundational challenge: Trying to make your teams use the tools for faster development, automated testing, continuous delivery, and monitoring. Is your Dev or Ops culture is ready for the changes?
For example, agile methodologies generally mandate that you ship new code once a week, or even once a day. This can result in a lot of awkward, halting, and failed adoptions of agile. You face the same conceptual issues with DevOps. It can be like pulling onto a nice smooth new road with a car that has no gas.
To avoid this, plan for a transition period. Leave enough time for the Dev and Ops teams to get used to your new practices. Make sure they have a chance to get hands-on experience with the new processes and tools. Before adopting DevOps, make sure you’ve matured your Dev and Ops culture.
Conclusion
Once you overcome the challenges and adopt DevOps practices, your organization will enjoy greater agility, improved customer satisfaction and employee morale, and increased productivity—all of which should help grow your business."
books$sysadmin$Linux$Database,"8 books for sysadmins
Applied Cryptography: Protocols, Algorithms and Source Code in C
By: Bruce Schneier
This book has been incredibly useful in understanding the very basics of cryptography systems and algorithms all the way up to much more advanced concepts. Produced in the early days of public key encryption detailing the inner workings of encryption systems. An absolute must-read for anyone wanting to understand more about the privacy systems that protect our data. (Recommendation and review by Brian Whetten)
Database Reliability Engineering
By: Laine Campbell and Charity Majors
The infrastructure-as-code revolution in IT is also affecting database administration. With this practical book, developers, system administrators, and junior to mid-level DBAs will learn how the modern practice of site reliability engineering applies to the craft of database architecture and operations. Authors Laine Campbell and Charity Majors provide a framework for professionals looking to join the ranks of today’s database reliability engineers (DBRE).
You'll begin by exploring core operational concepts that DBREs need to master. Then you’ll examine a wide range of database persistence options, including how to implement key technologies to provide resilient, scalable, and performant data storage and retrieval. With a firm foundation in database reliability engineering, you’ll be ready to dive into the architecture and operations of any modern database. (Recommendation by Chris Short | Review by Google Books)
Mastering Ubuntu Server
By: Jay LaCroix
Disclaimer, this is my latest book, but I worked really hard to make the most relevant book possible for running Ubuntu on servers. Think of this book as a passion project of mine. It was expanded from the first edition to be up to date for Ubuntu 18.04, and cover additional topics, such as Ansible, LXD, and more.
""For both simple and complex server deployments, Ubuntu's flexible nature can be easily adapted to meet the needs of your organization. With this book as your guide, you will learn all about Ubuntu Server, from initial deployment to creating production-ready resources for your network."" (Recommendation and review by Jay LaCroix)
The Container Commandos Coloring Book
By: Dan Walsh and Mairin Duffy
It's summer, and you should take the time to do something fun and learn something new. Fortunately, you can do both with this book, clocking in at just over a dozen pages. How? By coloring as you read about how Linux containers and the tools that support them are saving the world.
Introduce yourself to the superheroes of this story—Skopeo, Podman, Buildah, CRI-O, and OpenShift—and learn how the superpowers of each can protect the planet, or your data center, from disaster through the power of decentralization and resiliency.(Recommendation and review by Jason Baker)
The Information: A History, A Theory, A Flood
By: James Gleick
This book covers the history of information and how we have changed as a culture and consume information. From deciphering the language of talking drums in Africa and the introduction of Morse code to the development of the written word as a foundation of all information through social media, and how we consume and categorize information today.
As much as the subject matter looks like it would make for a dull read, I found this to be thoroughly engaging. There are fascinating accounts of people who's contribution to how we perceive and consume information. The story about the deciphering of talking drums was both memorable and made me smile.
James Gleick also gives a talk on the subject. (Recommendation and review by Andy Thornton)
The Linux Philosophy for SysAdmins
By: David Both
Linux has a strong historical connection with Unix, not just in terms of its technology but especially to its philosophy. My new book, The Linux Philosophy for SysAdmins, honors that connection while developing a new philosophy that is uniquely applicable to the Linux System Administrator. The Linux Philosophy for System Administrators is not about learning new commands, processes, or procedures. Rather it is about becoming a better SysAdmin through understanding the power of Linux as a function of the philosophies that built it.
This book uses a relatively few common Linux commands to illustrate practical, usable aspects of the philosophy. Readers will learn a philosophical approach to system administration that will unlock the power of the knowledge they already have. This book takes place on the Linux command line, but it is not about the commands themselves. The commands are only the tools through which the beauty of the underlying structure of Linux is revealed with real-world experiments you can perform. Inspired by my real mentors, and dedicated to them, this book is a mentor to SysAdmins everywhere. (Recommendation and review by David Both)
Time Management for System Administrators
By: Thomas Limoncelli
There are a lot of great books that cover the various technical aspects and systems of being a system administrator, but we sometimes lose sight of the human side of it. This book covers time management with a specific focus on sysadmins and the common time issues they encounter. As the tagline puts it: ""Stop working late and start working smart.""
It covers how to deal with continual interruptions, managing calendars and creating routines, and how to focus and prioritize what's important, among a host of other topics. It even gives a shout out to the often neglected art of documentation. It's a great book for anyone who always finds themselves busy, but never feel like they are getting anything done. (Recommendation and review by David Critch)
Unix Linux System Administration Handbook
By Evi Nemeth, Garth Snyder, Trent R. Hein, Ben Whaley, and Dan Mackin
I think the latest edition of this book goes back to 2011, which seems like yesterday.
One could argue the job of the sysadmin has evolved quite a bit in the past seven years with the growth of DevOps, cloud, and PasS, among other things. But, I think this book still has a lot of great information, some of which are best practices that transcend development models and technologies. It is a beast of a book, with 1500 pages, but it is one of the best sysadmin-oriented books I've ever read. (Recommendation and review by Anderson Silva)"
Python$RaspberryPi$Education,"Despite year-round schools and education of all types and stripes?from open courses and textbooks to online learning?this is a good time of year to consider new, innovative learning solutions. From software to hardware, we've got you covered with a list of books recommended by our writer community at Opensource.com. 
6 books for the life-long learner
20 Easy Raspberry Pi Projects

by Rui Santos and Sara Santos

This is an easy to read and follow book complete with code examples, pictures and diagrams of all the projects included in the book. Twenty step-by-step projects compatible with Raspberry Pi 2 and 3 are detailed in the book. The book is suitable for beginners but all the projects will help you learn more about your Raspberry Pi even if you?re more experienced with the platform. You can build a digital drum set, a weather forecaster, smoke alarm, home surveillance camera, temperature and humidity data logger and more.

(Recommended and reviewed by Don Watkins)
Lifelong Kindergarten

by Mitchel Resnick

Puppet theater, papier-m?ch? pets, and chunky wooden blocks with peeling paint?there's magic in the creative anarchy of a Kindergarten classroom. If you think your academic achievements peaked in Kindergarten, you might be right. Imaginative play strengthens cognitive and social development in children. And in Lifelong Kindergarten, MIT professor Mitchel Resnick argues that schools, and institutions, should be organized more like playful Kindergarten classrooms, and less like automated learning factories.

Resnick is an expert in educational technology and the leader of the Lifelong Kindergarten Group at the MIT Media Lab. He and his team created Scratch, the block-based, visual programming tool and online community for budding programmers of all ages.

Building on his experiences with the Scratch project and community, Resnick explores how playing, sharing, imagining, and inventing promotes critical thinking, problem-solving, and community sharing?all essential skills for learning and working in the 21st Century.

Resnick's book brims with stories of children programming all manner of elaborate projects?games, animations, and visual stories?all driven by passion and purpose. And readers will recognize an overlap with open source ideas and principles. For example, the online Scratch community promotes the sharing and remixing of student project source code?think GitHub for kids.

Lifelong Kindergarten is packed with ideas for educators and teachers. However, forward-looking leaders keen to optimize their teams for maximum creativity and innovation will discover inspiration in the wisdom of children.

(Recommended and reviewed by Charlie Reisinger)
Python Crash Course

by Eric Matthes

I found myself easily following this fast-paced introduction to Python programming. This book will have you writing your own programs very quickly. Information is clearly presented in ""byte-size"" chunks and the book is full of examples and ""Try-it-Yourself"" sections after each concept introduction. The book covers game creation, data visualization, web scraping, generating data, downloading data, getting started with Django, building and deploying and app and working with Git. In the process you will use Python libraries and tools like matplotlib, NumPy and PyGal. Follow Eric Matthes on Twitter and take a look his resources for the book on Github.

(Recommended and reviewed by Don Watkins)
Raspberry Pi Cookbook

by Simon Monk

This cookbook is ideal for programmers and hobbyists familiar with the Pi through resources, including Getting Started with Raspberry Pi (O?Reilly). Python and other code examples from the book are available on GitHub.

(Recommended and reviewed by Daniel Oh)
Raspberry Pi 3: Setup, Programming and Developing Amazing Projects with Raspberry Pi for Beginners

by Steve McCarthy

This book includes all the materials you will need for each project and task at the beginning of the developing with Raspberry Pi for ultimate beginners.

(Recommended and reviewed by Daniel Oh)
The Open Schoolhouse

by Charlie Reisinger

To best prepare students for the future, we must think deeply and openly about our vision for school technology today. I believe every student, in every school, deserves equal and open access to computers. Students should have the freedom to explore and experiment with their school-issued devices. In an open schoolhouse, every student is trusted with learning technology and empowered to rewire and reshape the world. Charlie Reisinger's seminal work on the unique opportunity school leaders and teachers have as they prepare our youth for the future of life and work is a must read. This book is a real page turner and will have you questioning the validity of your curriculum in an increasingly open world. If you are a constructivist and believe that students learn best by doing then you need to read The Open Schoolhouse. It is a practical guide for implementing open thinking, open design, open source software and open education principles in your classroom and school system.

(Recommended and reviewed by Don Watkins)
Bonus Project: PiDeck

PiDeck is a hands-on, hardware hacking project suitable for young people, from five or six years old on upwards. It uses a Raspberry Pi to play digital music controlled by a vinyl turntable, via the xwax application running on a tailored Debian distribution for the armhf architecture. Videos are available to explain how it works. While aspiring to the classic DJ techniques of Bronx DJs such as Kool Herc, Grandmaster Flash. and Grand Wizard Theodore is possible with practice, very young children just like to play with the tactile interface that the turntable provides: speeding up or slowing down the music, and making the sound go backward and forward with their fingertips. Any music format supported by FFMPEG can be used, although FLAC is recommended for the best possible sound quality.

(Recommended and reviewed by Daniel James)"
Python$Scikitlearn$datascience,"How to use the Scikit-learn Python library for data science projects
Versatile Python library offers powerful machine learning tools for data analysis and data mining.
The Scikit-learn Python library, initially released in 2007, is commonly used in solving machine learning and data science problems?from the beginning to the end. The versatile library offers an uncluttered, consistent, and efficient API and thorough online documentation.
What is Scikit-learn?

Scikit-learn is an open source Python library that has powerful tools for data analysis and data mining. It's available under the BSD license and is built on the following machine learning libraries:

    NumPy, a library for manipulating multi-dimensional arrays and matrices. It also has an extensive compilation of mathematical functions for performing various calculations.
    SciPy, an ecosystem consisting of various libraries for completing technical computing tasks.
    Matplotlib, a library for plotting various charts and graphs.

Scikit-learn offers an extensive range of built-in algorithms that make the most of data science projects.

Here are the main ways the Scikit-learn library is used.
1. Classification

The classification tools identify the category associated with provided data. For example, they can be used to categorize email messages as either spam or not.

More Python Resources

    What is Python?
    Top Python IDEs
    Top Python GUI frameworks
    Latest Python content
    More developer resources

Classification algorithms in Scikit-learn include:

    Support vector machines (SVMs)
    Nearest neighbors
    Random forest

2. Regression

Regression involves creating a model that tries to comprehend the relationship between input and output data. For example, regression tools can be used to understand the behavior of stock prices.

Regression algorithms include:

    SVMs
    Ridge regression
    Lasso

3. Clustering

The Scikit-learn clustering tools are used to automatically group data with the same characteristics into sets. For example, customer data can be segmented based on their localities.

Clustering algorithms include:

    K-means
    Spectral clustering
    Mean-shift

4. Dimensionality reduction

Dimensionality reduction lowers the number of random variables for analysis. For example, to increase the efficiency of visualizations, outlying data may not be considered.

Dimensionality reduction algorithms include:

    Principal component analysis (PCA)
    Feature selection
    Non-negative matrix factorization

5. Model selection

Model selection algorithms offer tools to compare, validate, and select the best parameters and models to use in your data science projects.

Model selection modules that can deliver enhanced accuracy through parameter tuning include:

    Grid search
    Cross-validation
    Metrics

6. Preprocessing

The Scikit-learn preprocessing tools are important in feature extraction and normalization during data analysis. For example, you can use these tools to transform input data?such as text?and apply their features in your analysis.

Preprocessing modules include:

    Preprocessing
    Feature extraction

A Scikit-learn library example

Let's use a simple example to illustrate how you can use the Scikit-learn library in your data science projects.

We'll use the Iris flower dataset, which is incorporated in the Scikit-learn library. The Iris flower dataset contains 150 details about three flower species:

    Setosa?labeled 0
    Versicolor?labeled 1
    Virginica?labeled 2

The dataset includes the following characteristics of each flower species (in centimeters):

    Sepal length
    Sepal width
    Petal length
    Petal width

Step 1: Importing the library

Since the Iris dataset is included in the Scikit-learn data science library, we can load it into our workspace as follows:

from sklearn import datasets
iris = datasets.load_iris()
These commands import the datasets module from sklearn, then use the load_digits() method from datasets to include the data in the workspace.
Step 2: Getting dataset characteristics

The datasets module contains several methods that make it easier to get acquainted with handling data.

In Scikit-learn, a dataset refers to a dictionary-like object that has all the details about the data. The data is stored using the .data key, which is an array list.
For instance, we can utilize iris.data to output information about the Iris flower dataset.

print(iris.data)

Here is the output (the results have been truncated):

[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]
 [5.4 3.9 1.7 0.4]
 [4.6 3.4 1.4 0.3]
 [5.  3.4 1.5 0.2]
 [4.4 2.9 1.4 0.2]
 [4.9 3.1 1.5 0.1]
 [5.4 3.7 1.5 0.2]
 [4.8 3.4 1.6 0.2]
 [4.8 3.  1.4 0.1]
 [4.3 3.  1.1 0.1]
 [5.8 4.  1.2 0.2]
 [5.7 4.4 1.5 0.4]
 [5.4 3.9 1.3 0.4]
 [5.1 3.5 1.4 0.3]

Let's also use iris.target to give us information about the different labels of the flowers.

print(iris.target)

Here is the output:

[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]

If we use iris.target_names, we'll output an array of the names of the labels found in the dataset.

print(iris.target_names)

Here is the result after running the Python code:

['setosa' 'versicolor' 'virginica']

Step 3: Visualizing the dataset

We can use the box plot to produce a visual depiction of the Iris flower dataset. The box plot illustrates how the data is distributed over the plane through their quartiles.

Here's how to achieve this:

import seaborn as sns
box_data = iris.data #variable representing the data array
box_target = iris.target #variable representing the labels array
sns.boxplot(data = box_data,width=0.5,fliersize=5)
sns.set(rc={'figure.figsize':(2,15)})
Let's see the result:

scikit_boxplot.png
Box plot

On the horizontal axis:

    0 is sepal length
    1 is sepal width
    2 is petal length
    3 is petal width

The vertical axis is dimensions in centimeters.
Wrapping up

Here is the entire code for this simple Scikit-learn data science tutorial.

from sklearn import datasets
iris = datasets.load_iris()
print(iris.data)
print(iris.target)
print(iris.target_names)
import seaborn as sns
box_data = iris.data #variable representing the data array
box_target = iris.target #variable representing the labels array
sns.boxplot(data = box_data,width=0.5,fliersize=5)
sns.set(rc={'figure.figsize':(2,15)})

Scikit-learn is a versatile Python library you can use to efficiently complete data science projects."
Python$Django$packages,"8 Python packages that will simplify your life with Django
This month's Python column looks at Django packages that will benefit your work, personal, or side projects.


Django developers, we're devoting this month's Python column to packages that will help you. These are our favorite Django libraries for saving time, cutting down on boilerplate code, and generally simplifying our lives. We've got six packages for Django apps and two for Django's REST Framework, and we're not kidding when we say these packages show up in almost every project we work on.

But first, see our tips for making the Django Admin more secure and an article on 5 favorite open source Django packages.
A kitchen sink of useful time-savers: django-extensions

Django-extensions is a favorite Django package chock full of helpful tools like these management commands:

    shell_plus starts the Django shell with all your database models already loaded. No more importing from several different apps to test one complex relationship!
    clean_pyc removes all .pyc projects from everywhere inside your project directory.
    create_template_tags creates a template tag directory structure inside the app you specify.
    describe_form displays a form definition for a model, which you can then copy/paste into forms.py. (Note that this produces a regular Django form, not a ModelForm.)
    notes displays all comments with stuff like TODO, FIXME, etc. throughout your project.

Django-extensions also includes useful abstract base classes to use for common patterns in your own models. Inherit from these base classes when you create your models to get their:

    TimeStampedModel: This base class includes the fields created and modified and a save() method that automatically updates these fields appropriately.
    ActivatorModel: If your model will need fields like status, activate_date, and deactivate_date, use this base class. It comes with a manager that enables .active() and .inactive() querysets.
    TitleDescriptionModel and TitleSlugDescriptionModel: These include the title and description fields, and the latter also includes a slug field. The slug field will automatically populate based on the title field.

Django-extensions has more features you may find useful in your projects, so take a tour through its docs!
12-factor-app settings: django-environ

Django-environ allows you to use 12-factor app methodology to manage your settings in your Django project. It collects other libraries, including envparse and honcho. Once you install django-environ, create an .env file at your project's root. Define in that module any settings variables that may change between environments or should remain secret (like API keys, debug status, and database URLs).

Then, in your project's settings.py file, import environ and set up variables for environ.PATH() and environ.Env() according to the example. Access settings variables defined in your .env file with env('VARIABLE_NAME').
Creating great management commands: django-click

Django-click, based on Click (which we have recommended before? twice), helps you write Django management commands. This library doesn't have extensive documentation, but it does have a directory of test commands in its repository that are pretty useful. A basic Hello World command would look like this:

# app_name.management.commands.hello.py
import djclick as click

@click.command()
@click.argument('name')
def command(name):
    click.secho(f'Hello, {name}')

Then in the command line, run:

>> ./manage.py hello Lacey
Hello, Lacey

Handling finite state machines: django-fsm

Django-fsm adds support for finite state machines to your Django models. If you run a news website and need articles to process through states like Writing, Editing, and Published, django-fsm can help you define those states and manage the rules and restrictions around moving from one state to another.

Django-fsm provides an FSMField to use for the model attribute that defines the model instance's state. Then you can use django-fsm's @transition decorator to define methods that move the model instance from one state to another and handle any side effects from that transition.

Although django-fsm is light on documentation, Workflows (States) in Django is a gist that serves as an excellent introduction to both finite state machines and django-fsm.
Contact forms: #django-contact-form

A contact form is such a standard thing on a website. But don't write all that boilerplate code yourself?set yours up in minutes with django-contact-form. It comes with an optional spam-filtering contact form class (and a regular, non-filtering class) and a ContactFormView base class with methods you can override or customize, and it walks you through the templates you will need to create to make your form work.
Registering and authenticating users: django-allauth

Django-allauth is an app that provides views, forms, and URLs for registering users, logging them in and out, resetting their passwords, and authenticating users with outside sites like GitHub or Twitter. It supports email-as-username authentication and is extensively documented. It can be a little confusing to set up the first time you use it; follow the installation instructions carefully and read closely when you customize your settings to make sure you're using all the settings you need to enable a specific feature.
Handling user authentication with Django REST Framework: django-rest-auth

If your Django development includes writing APIs, you're probably using Django REST Framework (DRF). If you're using DRF, you should check out django-rest-auth, a package that enables endpoints for user registration, login/logout, password reset, and social media authentication (by adding django-allauth, which works well with django-rest-auth).
Visualizing a Django REST Framework API: django-rest-swagger

Django REST Swagger provides a feature-rich user interface for interacting with your Django REST Framework API. Once you've installed Django REST Swagger and added it to installed apps, add the Swagger view and URL pattern to your urls.py file; the rest is taken care of in the docstrings of your APIs.

The UI for your API will include all your endpoints and available methods broken out by app. It will also list available operations for those endpoints and enable you to interact with the API (adding/deleting/fetching records, for example). It uses the docstrings in your API views to generate documentation for each endpoint, creating a set of API documentation for your project that's useful to you, your frontend developers, and your users."
Python$datascience$NymPu$Pandas$Matplotlib,"3 top Python libraries for data science
Turn Python into a scientific data analysis and modeling tool with these libraries.
Python's many attractions?such as efficiency, code readability, and speed?have made it the go-to programming language for data science enthusiasts. Python is usually the preferred choice for data scientists and machine learning experts who want to escalate the functionalities of their applications. (For example, Andrey Bulezyuk used the Python programming language to create an amazing machine learning application.)
Because of its extensive usage, Python has a huge number of libraries that make it easier for data scientists to complete complicated tasks without many coding hassles. Here are the top 3 Python libraries for data science; check them out if you want to kickstart your career in the field.
1. NumPy
NumPy (short for Numerical Python) is one of the top libraries equipped with useful resources to help data scientists turn Python into a powerful scientific analysis and modelling tool. The popular open source library is available under the BSD license. It is the foundational Python library for performing tasks in scientific computing. NumPy is part of a bigger Python-based ecosystem of open source tools called SciPy.
The library empowers Python with substantial data structures for effortlessly performing multi-dimensional arrays and matrices calculations. Besides its uses in solving linear algebra equations and other mathematical calculations, NumPy is also used as a versatile multi-dimensional container for different types of generic data.
Furthermore, it integrates flawlessly with other programming languages like C/C++ and Fortran. The versatility of the NumPy library allows it to easily and swiftly coalesce with an extensive range of databases and tools. For example, let's see how NumPy (abbreviated np) can be used for multiplying two matrices.
Let's start by importing the library (we'll be using the Jupyter notebook for these examples).
import numpy as np
Next, let's use the eye() function to generate an identity matrix with the stipulated dimensions.
matrix_one = np.eye(3)

matrix_one
Here is the output:
array([[1., 0., 0.],

       [0., 1., 0.],

       [0., 0., 1.]])
Let's generate another 3x3 matrix.
We'll use the arange([starting number], [stopping number]) function to arrange numbers. Note that the first parameter in the function is the initial number to be listed and the last number is not included in the generated results.
Also, the reshape() function is applied to modify the dimensions of the originally generated matrix into the desired dimension. For the matrices to be ""multiply-able,"" they should be of the same dimension.
matrix_two = np.arange(1,10).reshape(3,3)

matrix_two
Here is the output:
array([[1, 2, 3],

       [4, 5, 6],

       [7, 8, 9]])
Let's use the dot() function to multiply the two matrices.
matrix_multiply = np.dot(matrix_one, matrix_two)

matrix_multiply
Here is the output:
array([[1., 2., 3.],

       [4., 5., 6.],

       [7., 8., 9.]])
Great!
We managed to multiply two matrices without using vanilla Python.
Here is the entire code for this example:
import numpy as np

#generating a 3 by 3 identity matrix

matrix_one = np.eye(3)

matrix_one

#generating another 3 by 3 matrix for multiplication

matrix_two = np.arange(1,10).reshape(3,3)

matrix_two

#multiplying the two arrays

matrix_multiply = np.dot(matrix_one, matrix_two)

matrix_multiply
2. Pandas
Pandas is another great library that can enhance your Python skills for data science. Just like NumPy, it belongs to the family of SciPy open source software and is available under the BSD free software license.
Pandas offers versatile and powerful tools for munging data structures and performing extensive data analysis. The library works well with incomplete, unstructured, and unordered real-world data?and comes with tools for shaping, aggregating, analyzing, and visualizing datasets.
There are three types of data structures in this library:
? Series: single-dimensional, homogeneous array
? DataFrame: two-dimensional with heterogeneously typed columns
? Panel: three-dimensional, size-mutable array
For example, let's see how the Panda Python library (abbreviated pd) can be used for performing some descriptive statistical calculations.
Let's start by importing the library.
import pandas as pd
Let's create a dictionary of series.
d = {'Name':pd.Series(['Alfrick','Michael','Wendy','Paul','Dusan','George','Andreas',

   'Irene','Sagar','Simon','James','Rose']),

   'Years of Experience':pd.Series([5,9,1,4,3,4,7,9,6,8,3,1]),

   'Programming Language':pd.Series(['Python','JavaScript','PHP','C++','Java','Scala','React','Ruby','Angular','PHP','Python','JavaScript'])

    }
Let's create a DataFrame.
df = pd.DataFrame(d)
Here is a nice table of the output:
      Name Programming Language  Years of Experience

0   Alfrick               Python                    5

1   Michael           JavaScript                    9

2     Wendy                  PHP                    1

3      Paul                  C++                    4

4     Dusan                 Java                    3

5    George                Scala                    4

6   Andreas                React                    7

7     Irene                 Ruby                    9

8     Sagar              Angular                    6

9     Simon                  PHP                    8

10    James               Python                    3

11     Rose           JavaScript                    1
Here is the entire code for this example:
import pandas as pd

#creating a dictionary of series

d = {'Name':pd.Series(['Alfrick','Michael','Wendy','Paul','Dusan','George','Andreas',

   'Irene','Sagar','Simon','James','Rose']),

   'Years of Experience':pd.Series([5,9,1,4,3,4,7,9,6,8,3,1]),

   'Programming Language':pd.Series(['Python','JavaScript','PHP','C++','Java','Scala','React','Ruby','Angular','PHP','Python','JavaScript'])

    }



#Create a DataFrame

df = pd.DataFrame(d)

print(df)
3. Matplotlib
Matplotlib is also part of the SciPy core packages and offered under the BSD license. It is a popular Python scientific library used for producing simple and powerful visualizations. You can use the Python framework for data science for generating creative graphs, charts, histograms, and other shapes and figures?without worrying about writing many lines of code. For example, let's see how the Matplotlib library can be used to create a simple bar chart.
Let's start by importing the library.
from matplotlib import pyplot as plt
Let's generate values for both the x-axis and the y-axis.
x = [2, 4, 6, 8, 10]

y = [10, 11, 6, 7, 4]
Let's call the function for plotting the bar chart.
plt.bar(x,y)
Let's show the plot.
plt.show()

Here is the entire code for this example:
#importing Matplotlib Python library 

from matplotlib import pyplot as plt

#same as import matplotlib.pyplot as plt

 

#generating values for x-axis 

x = [2, 4, 6, 8, 10]

 

#generating vaues for y-axis 

y = [10, 11, 6, 7, 4]

 

#calling function for plotting the bar chart

plt.bar(x,y)

 

#showing the plot

plt.show()
Wrapping up
The Python programming language has always done a good job in data crunching and preparation, but less so for complicated scientific data analysis and modeling. The top Python frameworks for data science help fill this gap, allowing you to carry out complex mathematical computations and create sophisticated models that make sense of your data."
Python$book$MachineLearning,"18 Python programming books for beginners and veterans
Get started with this popular language or buff up on your coding skills with this curated book list.
Who knew there were so many helpful books out there for Python programmers? This curated list is just a drop in the bucket. As you may know, Python is soaring in popularity.

Let's just say, it might be a good idea to get started learning more about Python soon or buffing up on your Python skills. So, I asked our writer community to share their top recommendations. Surprisingly, I only received one duplicate out of nineteen responses.

I didn't break this list down into beginner books and advanced books because I recently listened to an insightful podcast about how the more experienced in a task or subject we become the more likely we are to believe we know everything about it. Yet, refreshing ourselves on the basics and performing simple acts like running through a checklist is important for all of us, for the beginner and advanced user alike.

Or, maybe you prefer to start out in the deep end. Either way, let's dive in.
18 Python programming books
Automate the Boring Stuff with Python

by Al Sweigart (Recommendation and review by Don Watkins)

This book is a great resource for those who want to begin to learn and use Python. It's a practical introduction to programming and has been released with a Creative Commons license. In addition to the book, the author has created a website where the entire book is available. In addition, the online course on Udemy.com covers most of the content of the book. If you prefer a video format, the entire book is on YouTube and narrated by Al.
Effective Python: 59 Specific Ways to Write Better Python

by Brett Slatkin (Recommended and reviewed by Daniel Oh)
Effective Python will help you harness the full power of Python to write exceptionally robust, efficient, maintainable, and well-performing code. Utilizing the concise, scenario-driven style pioneered in Scott Meyers?s best-selling Effective C++, Brett Slatkin brings together 59 Python best practices, tips, shortcuts, and realistic code examples from expert programmers. 
Fluent Python: Clear, Concise, and Effective Programming

by Luciano Ramalho (Recommended and reviewed by Daniel Oh)

With this hands-on guide, you?ll learn how to write effective, idiomatic Python code by leveraging its best features. You will go through Python's core language features and libraries, and this book shows you how to make your code shorter, faster, and more readable at the same time. With this book, Python programmers will thoroughly learn how to become proficient in Python 3.
Hello Web App

by Tracy Osborn (Recommended and reviewed by Katie McLaughlin)

This is a series of books that show you how to build your first web app, which happens to use Django (recently updated for Django 2.0). It is extremely accessible for beginners, has a separate book specifically on intermediate topics, and the third in the series about Design is also very good. 
Invent Your Own Computer Games with Python

by Al Sweigart (Recommended and reviewed by Moshe Zadka)

In learning to program, motivation is often a limiting factor. What can be more motivating than, literally, programming fun? The rewards are immediate and easy to show off to friends and family.
Learning Python

by Mark Lutz and David Ascher (Recommended and reviewed by Greg Pittman)

When I need a command I haven't used or haven't used in a while, this is my go-to book. I have yet to find an online resource that helps me find things I need as fast.
Learning Python: Learn to code like a professional with Python

by Fabrizio Romano (Recommended and reviewed by Jay LaCroix)

This book is a handy way of learning Python, easing readers into the language. This is a good starting point for beginners.
Learn to Program with Python 3

by Irv Kalb (Recommended and reviewed by Moshe Zadka)

The two advantages of this book are that it starts from a modern technology (Python 3) and builds on the experience the author has in teaching real students. Those make it a great first programming book for people who want to learn how to program from scratch.
Programming Arcade Games with Python and Pygame

by Paul Craven (Recommended and reviewed by Jay LaCroix)

Programming computer games is a great way of learning Python and is perhaps the most fun way of doing so. You'll see your code literally come to life and animate on the screen, giving you a great way to learn object-oriented programming concepts. It's a good idea to understand the basics first (see my first recommendation) but this book also goes over the basics.
Python 101

by Mike Driscoll (Recommended and reviewed by Adam Miller)

This book is great for newcomers, the content is approachable and the lessons teach idiomatic Python so that when a developer breaks out into the world from simple projects to more advanced topics, they are already coding stylistically how other Pythonistas will expect and code the reader encounters will (most likely) follow similar and familiar patterns. The book does a good job of covering the basics and offering the reader a solid foundation of knowledge.
Python 3 Object-oriented Programming

by Dusty Phillips (Recommended and reviewed by Jay LaCroix)

After you've learned the basics and some intermediate Python skills, this book is a great way to take your knowledge to the next level and learn Python in greater detail. You'll learn more advanced concepts regarding object-oriented programming.
Python Cookbook

by David Beazley and Brian K. Jones (Recommended and reviewed by Daniel Oh)

This book is geared towards professional Python programmers. It covers, in one comprehensive volume, tutorials on the most common programming tasks. Code examples in the book show you how things are done in idiomatic Python 3 code. The book explains why and how the code works, which is very helpful. Inside, you will find guides on topics like data encoding, data structures, algorithms, meta-programming, and concurrency.
Python Scripting with Scribus*

by Greg Pittman (Recommended and reviewed by the author)

The sources that I find most useful are those which show some detailed, concrete examples, and these examples should be useful things to do. Things like accessing a file, sorting the contents into lists, then manipulating those lists in further useful ways. To that end, I wrote a Python book of my own, which is focused on Python scripting for Scribus, taking a variety of scripts I have written and explaining what various parts of the script are accomplishing. The idea was to show a variety of Scribus Scripter commands so that people might mix and match what parts they need for their own use.

*This book is not online. To read, download the PDF attached to this article.
Python Tricks: The Book

by Dan Bader (Recommended and reviewed by Adam Miller)

This book walks programmers through some interesting and often untraveled areas of the languages syntax as well as execution side effects of the official reference Python interpreter (CPython). By going through these exercises programmers learn clever ways to make their code more performant through optimizations as well as corner cases of the language to avoid that could cause unintended behavior in software.
Scaling Python

by Julien Danjou (Recommended and reviewed by the author)

While it's easy to learn Python and start building applications with it, creating software that will work correctly for a large number of users is another story. Scaling Python focuses on writing largely scalable and highly-distributed Python applications. You'll learn what works and what does not work when using Python to write your next big project. The book is illustrated with seven interviews with prominent open source developers who talk about their battlefield experience and give great advise.
The Hacker's Guide to Python

by Julien Danjou (Recommended and reviewed by the author)

There are tons of books that teach the basics of Python. Once you read them, you're usually familiar enough to start writing your first application. But then comes a ton of other questions about, how to organize your project, how to distribute it so others can use it, how to achieve decent performances, how to test, etc. The Hacker's Guide to Python answers to all those questions and more by providing concrete answers to those issues. The author shares his 10+ years of experience with Python and provides ready-to-go solutions. The book is also illustrated with eight interviews from software engineers, CPython developers, and open source hackers.
The Quick Python Book

by Naomi Ceder (Recommended and reviewed by Moshe Zadka)

Naomi has been part of the Python community for a long time, and it shows in her book.  Now in its third edition, the book is a comprehensive reference to Python and full of deep insights.
Treading on Python: Volume 2 Intermediate Python

by Matt Harrison (Recommended and reviewed by Adam Miller)

Intermediate Python is exactly as the book calls it, it's intermediate-to-advanced topics about the Python programming language in a short and concise writing style. There's no fluff, it's to the point, but full of valuable information. This book is definitely recommended for Python programmers looking to rapidly increase their knowledge about intermediate and slightly advanced topics in Python programming."
Python$book$Education$Science,"11 resources for teaching and learning Python
If you're looking to teach, tutor, or mentor beginning programmers, you've got your work cut out for you. Different learning styles, varying levels of knowledge, and a subject area that's a moving target all conspire to see you run ragged as an instructor. Luckily, there is help available?lots of help. It comes in the form of open source textbooks, tools, and even games?all created to make being a teacher (and a learner) easier than ever before.

A bit of context: I completed a master's degree in information and computer science last summer and started teaching my first university class (Python programming) about two weeks later. I'm a self-taught programmer who mostly learned on the job. I've never taken a Python class. To add to the fun, my students ranged from true beginners who had never written a line of code to experienced programmers trying out a new language. By rights, I should have been doomed from the start.
Open source courseware to the rescue

Despite the challenges presented by my students' diversity of experience and interests, we managed to get through the semester with minimal soul-crushing boredom. This was facilitated entirely by the wealth of open source educational materials and tools available in the Python universe.

Just like open source software, open source courseware refers to books, tutorials, games, and quizzes that are free to use, share, and even modify. There are a number of open source licenses available to creators of content, including the GNU Public License, the MIT and BSD licenses, and several variations of the Creative Commons license. They vary in what they permit, but what's important is that they're all free to use in your classroom.

Instead of forcing students to spend $150 on a dry textbook and type through examples that confuse half of the class while boring the other half to tears, I was able to lift the textbook requirement entirely. Instead of one overpriced book, the class used several high-quality, up-to-date, free books. Instead of trudging through the same old ""implement a linked list"" or ""build a library lending app"" examples that so failed to inspire me during my own education, I was able to assign students tasks like ""simulate a galaxy,"" ""design primers for a biological experiment,"" ""make a platformer video game,"" or ""build a web service.""
Open source Python textbooks

Help for the beginners in the class came in the form of Charles Severance's book, Python for Informatics (Creative Commons license). This is the same textbook used in Coursera's ""Programming for Everybody"" course. I offered points to any student who typed the code examples from the book and blogged about what they learned.

More experienced programmers who wanted a different kind of introduction to Python were able to jump right in to game programming thanks to the GPL-licensed Pygame module and Al Sweigart's Creative Commons-licensed textbook Invent Your Own Computer Games with Python. The students who opted for this approach got points for typing example code from the textbook and eventually learned to modify the code to create their own games.
Open source libraries and docs

For true beginners, following the first few chapters of a textbook is sufficient for a semester's work. But what about those experienced programmers who signed up for an easy A? The open source community provided a ton of fun for them, too.

One of Python's greatest strengths as a language is the variety of robust, well supported, thoroughly documented libraries and packages available. For the students who had no need to learn about lists and functions, there was no end to the projects available.

For the science majors, there are several options, including the following BSD-licensed packages:

    Astropy, which contains modules (and sample code) for astronomical constants, coordinate systems, model fitting, and more.
    Bokeh, a visualization library. The documentation is extensive (and beautiful). Any scientist or science major should be able to acquire some interesting test data in her field, and if she can do that, she can turn it into a gorgeous graph just by following the Bokeh examples.
    Vispy, yet another visualization library that ships with a number of complex, impressive example projects. One beginner was able to find, run, read, comprehend, and successfully modify a 3D spiral galaxy simulation. How many of you did something that cool in your first semester of coding?

While the scientists crunched data in one corner, a number of students interested in learning about web technologies were able to build their own relevant projects thanks to these great resources:

    BeautifulSoup is an MIT-licensed package that makes parsing HTML as enjoyable as browsing the rendered version. The docs are a great introduction to Python, parsing, and HTML all in one stop.
    Flask is a BSD-licensed web microframework. The quick, to-the-point tutorial takes newcomers through topics such as templates, requests and responses, database operations, and testing. For the intermediate programmer, this single walkthrough is a more complete and engaging introduction to Python than any beginner textbook could offer.

Games and interactive tutorials

The class also benefited from plenty of gamified resources available to students of Python and programming in general. Here are a couple of Creative Commons-licensed options:

    Project Euler is a website with coding challenges that increase in difficulty. They're mostly short, mathematical in nature, and perfect for practicing a new language. Students were able to complete a set of these problems and present their solution in class for points.
    Slice Like a Ninja is a web game I built that presents players with strings and a terminal. The objective is to use Python slice syntax to convert the input string into a given output string. Rather than present a chapter and a test on slices, I waited until students encountered a problem that could be solved with slices, introduced the topic, then sent them to play this game.

Python doesn't happen in a vacuum, and familiarity with the programming environment itself is often the biggest impediment for beginning coders. Some students walked into my class as established command line and version control wizards, but the majority needed onboarding. Here are some amazing (MIT-licensed) resources for learning the command line and related tools:

    Bashy is a short game that introduces basic command line usage while providing visual feedback for those who are having a hard time breaking away from GUI-land. (Full disclosure: I built Bashy in grad school. I don't make money from it, though!)
    OpenVim is an interactive tutorial for complete beginners to the powerful Vim text editor.
    VimGolf is a command line tool and website that implements a competitive game that helps Vim users of all levels improve their efficiency.
    learnGitBranching is a rich, thorough, interactive web app that covers beginning and intermediate Git commands with beautiful visualizations and encouraging feedback. I refuse to teach programming without requiring version control, so this was a priceless resource. It beats lecturing for hours on end about Git!

Version control in particular is a complicated topic that rewards deep study. For students who need more than a web game to get the hang of it, there is the Creative Commons-licensed Pro Git textbook by Scott Chacon and Ben Straub, available to read free online.
Lessons learned

The open source classroom achieved mixed results. For more advanced students, it was a huge success. They appreciated the opportunity to explore their own interests rather than being tied to the examples in a single textbook, and felt as though they received a solid introduction to Python. Beginners were less enthusiastic, in some cases complaining of feeling overwhelmed or confused.

Perhaps this was a weak point of the class, but perhaps feeling overwhelmed and confused is a natural response to entering into a world as diverse and full of possibility as the open source Python universe. It is overwhelming to think that there are nearly a million Python projects on GitHub, and it's a bit confusing to think that one language can power websites, games, and spaceships.

Making sense of the open source landscape may simply require more than a semester's worth of effort, but every student left with a clear idea of just how many great resources are out there, and how much power they offer us. The days of being tied to one textbook are gone, and good riddance! Open source courseware allows instructors and learners to design a rich, customized, interactive learning experience, all for free. Now let's write some code!"
Python$Pyspider$MechanicalSoup$Cola$Demiurge$Feedparser$Lassie$RoboBrowser ,"3 Python web scrapers and crawlers
Check out these great Python tools for crawling and scraping the web, and parsing out the data you need.
In a perfect world, all of the data you need would be cleanly presented in an open and well-documented format that you could easily download and use for whatever purpose you need.

In the real world, data is messy, rarely packaged how you need it, and often out-of-date.

Often, the information you need is trapped inside of a website. While some websites make an effort to present data in a clean, structured data format, many do not. Crawling, scraping, processing, and cleaning data is a necessary activity for a whole host of activities from mapping a website's structure to collecting data that's in a web-only format, or perhaps, locked away in a proprietary database.

Sooner or later, you're going to find a need to do some crawling and scraping to get the data you need, and almost certainly you're going to need to do a little coding to get it done right. How you do this is up to you, but I've found the Python community to be a great provider of tools, frameworks, and documentation for grabbing data off of websites.

Before we jump in, just a quick request: think before you do, and be nice. In the context of scraping, this can mean a lot of things. Don't crawl websites just to duplicate them and present someone else's work as your own (without permission, of course). Be aware of copyrights and licensing, and how each might apply to whatever you have scraped. Respect robots.txt files. And don't hit a website so frequently that the actual human visitors have trouble accessing the content.

With that caution stated, here are some great Python tools for crawling and scraping the web, and parsing out the data you need.
Pyspider

Let's kick things off with pyspider, a web-crawler with a web-based user interface that makes it easy to keep track of multiple crawls. It's an extensible option, with multiple backend databases and message queues supported, and several handy features baked in, from prioritization to the ability to retry failed pages, crawling pages by age, and others. Pyspider supports both Python 2 and 3, and for faster crawling, you can use it in a distributed format with multiple crawlers going at once.

Pyspyder's basic usage is well documented including sample code snippets, and you can check out an online demo to get a sense of the user interface. Licensed under the Apache 2 license, pyspyder is still being actively developed on GitHub.
MechanicalSoup

MechanicalSoup is a crawling library built around the hugely-popular and incredibly versatile HTML parsing library Beautiful Soup. If your crawling needs are fairly simple, but require you to check a few boxes or enter some text and you don't want to build your own crawler for this task, it's a good option to consider.

MechanicalSoup is licensed under an MIT license. For more on how to use it, check out the example source file example.py on the project's GitHub page. Unfortunately, the project does not have robust documentation at this time
Scrapy

Scrapy is a scraping framework supported by an active community with which you can build your own scraping tool. In addition to scraping and parsing tools, it can easily export the data it collects in a number of formats like JSON or CSV and store the data on a backend of your choosing. It also has a number of built-in extensions for tasks like cookie handling, user-agent spoofing, restricting crawl depth, and others, as well as an API for easily building your own additions.

For an introduction to Scrapy, check out the online documentation or one of their many community resources, including an IRC channel, Subreddit, and a healthy following on their StackOverflow tag. Scrapy's code base can be found on GitHub under a 3-clause BSD license.

If you're not all that comfortable with coding, Portia provides a visual interface that makes it easier. A hosted version is available at scrapinghub.com.
Others

    Cola describes itself as a ?high-level distributed crawling framework? that might meet your needs if you're looking for a Python 2 approach, but note that it has not been updated in over two years.

    Demiurge, which supports both Python 2 and Python 3, is another potential candidate to look at, although development on this project is relatively quiet as well.

    Feedparser might be a helpful project to check out if the data you are trying to parse resides primarily in RSS or Atom feeds.

    Lassie makes it easy to retrieve basic content like a description, title, keywords, or a list of images from a webpage.

    RoboBrowser is another simple library for Python 2 or 3 with basic functionality, including button-clicking and form-filling. Though it hasn't been updated in a while, it's still a reasonable choice.

 

This is far from a comprehensive list, and of course, if you're a master coder you may choose to take your own approach rather than use one of these frameworks. Or, perhaps, you've found a great alternative built for a different language. For example, Python coders would probably appreciate checking out the Python bindings for Selenium for sites that are trickier to crawl without using an actual web browser."
Python$Eclipse$PyDev$Eric$PyCharm$PyScripter$LeoEditor $PTK$Bluefish$Geany$Spyder,"Top 3 open source Python IDEs
When it comes to Python programming, you have many options for great integrated development environments.
Python is everywhere. These days, it seems it powers everything from major websites to desktop utilities to enterprise software. Python has been used to write all, or parts of, popular software projects like dnf/yum, OpenStack, OpenShot, Blender, Calibre, and even the original BitTorrent client.

It also happens to be one of my favorite programming languages. Personally, Python has been my go-to language through the years for everything from class projects in college to tiny scripts to help me automate recurring tasks. It's one of few languages out there that is both easy to get started with for beginners yet incredibly powerful when beginners graduate to working on real-world projects.

To edit Python programs, you have a number of options. Some people still prefer a basic text editor, like Emacs, VIM, or Gedit, all of which can be extended with features like syntax highlighting and autocomplete. But a lot of power users working on large projects with complex code bases prefer an integrated development environment (IDE) to the text editor plus terminal combination. The line between an advanced text editor and a slim IDE isn't always clear, and we'll leave it up to you to decide exactly which features you require for your development needs.

Let's look at some of the most popular options available to Python developers and see how they stack up. All three are cross-platform and can be used on your operating system of choice.
Eclipse with PyDev

It's hard to write anything about open source integrated development environments without covering Eclipse, which has a huge developer community and countless plugins available allowing you to customize it to meet nearly any need you can imagine. But this kitchen sink approach is also one of Eclipse's downsides. Many criticize it as bloated, and performance on low spec systems certainly can be an issue.

That said, if you're coming to Python from a background in a different language, particularly Java, Eclipse may already be your go to IDE. And if you make use of its many features, you may find life without them difficult.

PyDev adds a huge number of features to Eclipse, far beyond simple code highlighting. It handles code completion, integrates Python debugging, adds a token browser, refactoring tools, and much more. For those working with the popular Django Python web framework, PyDev will allow you to create new Django projects, execute Django actions via hotkeys, and use a separate run configuration just for Django.

Eric

Eric is my personal favorite IDE for Python editing. Named after Monty Python's Eric Idle, Eric is actually written in Python using the Qt framework.

Eric makes use of Scintilla, a source code editing component which is used in a number of different IDEs and editors which is also available as the stand-alone SciTE editor.

The features of Eric are similar to other IDEs: brace matching, code completion, a class browser, integrated unit tests, etc. It also has a Qt form preview function, which is useful if you're developing a Qt GUI for your application, and I personally like the integrated task list function.

I've heard some criticisms of Eric's documentation, which primarily being delivered through a massive PDF does leave something to be desired, but if you take the time to learn it, I find Eric to be a lighweight yet full-featured programming environment.
PyCharm

PyCharm is another popular Python editor and rounds out my top three. Pycharm is a commercial product, but the makers also offer a community edition which is free and open source under the Apache 2.0 license.

PyCharm features pretty much everything one might hope for in an IDE: integrated unit testing, code inspection, integrated version control, code refactoring tools, a variety of tools for project navigation, as well as the highlighting and automated completion features you would expect with any IDE.

To me, the main drawback of PyCharm is its open core model. Many of PyCharm's advanced features are not available under an open source license, and for me, that's a deal breaker. However, if you're not looking to use the more advanced features included in the closed source verion, having the features left out may leave PyCharm as a lighter weight choice for Python editing.

Other great options

The list of open source Python editors and integrated development environments is lengthy. Here are a few other interesting standouts.

    PyScripter, LeoEditor, and PTK (the Python Tool Kit) are all lesser-known tools for working with Python code. And of course there's IDLE, the default IDE packaged with Python.
    Bluefish and Geany are two great general purpose IDEs with strong Python support.
    Spyder is an IDE specifically designed for working with scientific Python development and the libraries commonly associated with this type of work."
Python$RaspberryPi$Linux$OpenWest,"Brewing beer with Linux, Python, and Raspberry Pi
A handy how-to for building a homemade homebrew setup with Python and the Raspberry Pi.
I started brewing my own beer more than 10 years ago. Like most homebrewers, I started in my kitchen making extract-based brews. This required the least equipment and still resulted in really tasty beer. Eventually I stepped up to all-grain brewing using a big cooler for my mash tun. For several years I was brewing 5 gallons at a time, but brewing 10 gallons takes the same amount of time and effort (and only requires slightly larger equipment), so a few years ago I stepped it up. After moving up to 10 gallons, I stumbled across StrangeBrew Elsinore and realized what I really needed to do was convert my whole system to be all-electric, and run it with a Raspberry Pi.

There is a ton of great information available for building your own all-electric homebrew system, and most brewers start out at TheElectricBrewery.com. Just putting together the control panel can get pretty complicated, although the simplest approach is outlined well there. Of course you can also take a less expensive approach and still end up with the same result?a boil kettle and hot liquor tank powered by heating elements and managed by a PID controller. I think that's a little too boring though (and it also means you don't get neat graphs of your brew process).

Hardware supplies

Before I talked myself out of the project, I decided to start buying parts. My basic design was a Hot Liquor Tank (HLT) and boil kettle with 5500w heating elements in them, plus a mash tun with a false bottom. I would use a pump to recirculate the mash through a 50' stainless coil in the HLT (a ""heat exchanger recirculating mash system"", known as HERMS). I would need a second pump to circulate the water in the HLT, and to help with transferring water to the mash tun. All of the electrical components would be controlled with a Raspberry Pi.

Building my electric brew system and automating as much of it as possible meant I was going to need the following:

    HLT with a 5500w electric heating element
    HERMS coil (50' 1/2"" stainless steel) in the HLT
    boil kettle with a 5500w electric heating element
    multiple solid-state relays to switch the heaters on and off
    2 high-temp food-grade pumps
    relays for switching the pumps on and off
    fittings and high-temp silicon tubing
    stainless ball valves
    1-wire temperature probes
    lots of wire
    electrical box to hold everything

The details of building out the electrical side of the system are really well covered by The Electric Brewery, so I won't repeat their detailed information. You can read through and follow their suggestions while planning to replace the PID controllers with a Raspberry Pi.

One important thing to note is the solid-state relay (SSR) signal voltage. Many tutorials suggest using SSRs that need a 12-volt signal to close the circuit. The Raspberry Pi GPIO pins will only output 3v, however. Be sure to purchase relays that will trigger on 3 volts.

To run your brew system, your Pi must do two key things: sense temperature from a few different places, and turn relays on and off to control the heating elements. The Raspberry Pi easily is able to handle these tasks.

There are a few different ways to connect temp sensors to a Pi, but I've found the most convenient approach is to use the 1-Wire bus. This allows for multiple sensors to share the same wire (actually three wires), which makes it a convenient way to instrument multiple components in your brew system. If you look for waterproof DS18B20 temperature sensors online, you'll find lots of options available. I used Hilitchi DS18B20 Waterproof Temperature Sensors for my project.

To control the heating elements, the Raspberry Pi includes several General Purpose IO (GPIO) pins that are software addressable. This allows you to send 3.3v to a relay by simply putting a 1 or a 0 in a file. The Raspberry Pi?Driving a Relay using GPIO tutorial was the most helpful for me when I was first learning how all this worked. The GPIO controls multiple solid-state relays, turning on and off the heating elements as directed by the brewing software.

I first started working on the box to hold all the components. Because this would all be on a rolling cart, I wanted it to be relatively portable rather than permanently mounted. If I had a spot (for example, inside a garage, utility room, or basement), I would have used a larger electrical box mounted on the wall. Instead I found a decent-size waterproof project box that I expected I could shoehorn everything into. In the end, it turned out to be a little bit of a tight fit, but it worked out. In the bottom left corner is the Pi with a breakout board for connecting the GPIO to the 1-Wire temperature probes and the solid state relays.

To keep the 240v SSRs cool, I cut holes in the case and stacked copper shims with CPU cooling grease between them and heat sinks mounted on the outside of the box. It worked out well and there haven't been any cooling issues inside the box. On the cover I put two switches for 120v outlets, plus two 240v LEDs to show which heating element was energized. I used dryer plugs and outlets for all connections so disconnecting a kettle from everything is easy. Everything worked right on the first try, too. (Sketching a wiring diagram first definitely pays off.)

The pictures are from the ""proof-of-concept"" version?the final production system should have two more SSRs so that both legs of the 240v circuit would be switched. The other thing I would like to switch via software is the pumps. Right now they're controlled via physical switches on the front of the box, but they could easily be controlled with relays.

The only other thing I needed that was a little tricky to find was a compression fitting for the temperature probes. The probes were mounted in T fittings before the valve on the lowest bulkhead in both the HLT and the mash tun. As long as the liquid is flowing past the temp sensor, it's going to be accurate. I thought about adding a thermowell into the kettles as well, but realized that's not going to be useful for me based on my brewing process. Anyway, I purchased 1/4"" compression fittings and they worked out perfectly.
Software

Once the hardware was sorted out, I had time to play with the software. I ran the latest Raspbian distribution on the Pi; nothing special was required on the operating-system side.

I started with Strangebrew Elsinore brewing software, which I had discovered when a friend asked whether I had heard of Hosehead, a Raspberry Pi-based brewing controller. I thought Hosehead looked great, but rather than buying a brewing controller, I wanted the challenge of building my own.

Setting up Strangebrew Elsinore was straightforward?the documentation was thorough and I did not encounter any problems. Even though Strangebrew Elsinore was working fine, Java seemed to be taxing my first-generation Pi sometimes, and it crashed on me more than once. I also was sad to see development stall and there did not seem to be a big community of additional contributors (although there were?and still are?plenty of people using it).
CraftBeerPi

Then I stumbled across CraftBeerPI, which is written in Python and supported by a development community of active contributors. The original author (and current maintainer) Manuel Fritsch is great about handling contributions and giving feedback on issues that folks open. Cloning the repo and getting started only took me a few minutes. The README also has a good example of connecting DS1820 temp sensors, along with notes on interfacing hardware to a Pi or a C.H.I.P. computer.

On startup, CraftBeerPi walks users through a configuration process that discovers the temperature probes available and lets you specify which GPIO pins are managing which pieces of equipment.

Running a brew with this system is easy. I can count on it holding temperatures reliably, and I can input steps for a multi-temp step mash. Using CraftBeerPi has made my brew days a little bit boring, but I'm happy to trade off the ""excitement"" of traditional manually managed propane burners for the efficiency and consistency of this system.

CraftBeerPI's user-friendliness inspired me to set up another controller to run a ""fermentation chamber."" In my case, that was a second-hand refrigerator I found for US$ 50 plus a $25 heater) on the inside. CraftBeerPI easily can control the cooling and heating elements, and you can set up multiple temperature steps. For instance, this graph shows the fermentation temperatures for a session IPA I made recently. The fermentation chamber held the fermenting wort at 67F for four days, then ramped up one degree every 12 hours until it was at 72F. That temp was held for a two-day diacetyl rest. After that it was set to drop down to 65F for five days, during which time I ""dry hopped"" the beer. Finally, the beer was cold-crashed down to 38F. CraftBeerPI made adding each step and letting the software manage the fermentation easy.

I have also been experimenting with the TILT hydrometer to monitor the gravity of the fermenting beer via a Bluetooth-connected floating sensor. There are integration plans for this to get it working with CraftBeerPI, but for now it logs the gravity to a Google spreadsheet. Once this hydrometer can talk to the fermentation controller, setting automated fermentation profiles that take action directly based on the yeast activity would be easy?rather than banking on primary fermentation completing in four days, you can set the temperature ramp to kick off after the gravity is stable for 24 hours.

As with any project like this, imaging and planning improvements and additional components is easy. Still, I'm happy with where things stand today. I've brewed a lot of beer with this setup and am hitting the expected mash efficiency every time, and the beer has been consistently tasty. My most important customer?me!?is pleased with what I've been putting on tap in my kitchen."
Python$Django$Library,"8 great Python libraries for side projects
These Python libraries make it easy to scratch that personal project itch.
We have a saying in the Python/Django world: We came for the language and stayed for the community. That is true for most of us, but something else that has kept us in the Python world is how easy it is to have an idea and quickly work through it over lunch or in a few hours at night.

This month we're diving into Python libraries we love to use to quickly scratch those side-project or lunchtime itches.
To save data in a database on the fly: Dataset

Dataset is our go-to library when we quickly want to collect data and save it into a database before we know what our final database tables will look like. Dataset has a simple, yet powerful API that makes it easy to put data in and sort it out later.

Dataset is built on top of SQLAlchemy, so extending it will feel familiar. The underlying database models are a breeze to import into Django using Django's built-in inspectdb management command. This makes working with existing databases pretty painless.
To scrape data from web pages: Beautiful Soup

Beautiful Soup (BS4 as of this writing) makes extracting information out of HTML pages easy. It's our go-to anytime we need to turn unstructured or loosely structured HTML into structured data. It's also great for working with XML data that might otherwise not be readable.
To work with HTTP content: Requests

Requests is arguably one of the gold standard libraries for working with HTTP content. Anytime we need to consume an HTML page or even an API, Requests has us covered. It's also very well documented.
To write command-line utilities: Click

When we need to write a native Python script, Click is our favorite library for writing command-line utilities. The API is straightforward, well thought out, and there are only a few patterns to remember. The docs are great, which makes looking up advanced features easy.

To name things: Python Slugify

As we all know, naming things is hard. Python Slugify is a useful library for turning a title or description into a unique(ish) identifier. If you are working on a web project and you want to use SEO-friendly URLs, Python Slugify makes this easier.
To work with plugins: Pluggy

Pluggy is relatively new, but it's also one of the best and easiest ways to add a plugin system to your existing application. If you have ever worked with pytest, you have used pluggy without knowing it.
To convert CSV files into APIs: Datasette

Datasette, not to be confused with Dataset, is an amazing tool for easily turning CSV files into full-featured read-only REST JSON APIs. Datasette has tons of features, including charting and geo (for creating interactive maps), and it's easy to deploy via a container or third-party web host.
To handle environment variables and more: Envparse

If you need to parse environment variables because you don't want to save API keys, database credentials, or other sensitive information in your source code, then envparse is one of your best bets. Envparse handles environment variables, ENV files, variable types, and even pre- and post-processors (in case you want to ensure that a variable is always upper or lower case, for instance)."
Python$EduBlocks$Education,"Learn Python programming the easy way with EduBlocks
EduBlocks brings a Scratch-like GUI to writing Python 3 code.
If you are you looking for a way to move your students (or yourself) from programming in Scratch to learning Python, I recommend you look into EduBlocks. It brings a familiar drag-and-drop graphical user interface (GUI) to Python 3 programming.

One of the barriers when transitioning from Scratch to Python is the absence of the drag-and-drop GUI that has made Scratch the go-to application in K-12 schools. EduBlocks' drag-and-drop version of Python 3 changes that paradigm. It aims to ""help teachers to introduce text-based programming languages, like Python, to children at an earlier age.""

The hardware requirements for EduBlocks are quite modest?a Raspberry Pi and an internet connection?and should be available in many classrooms.

EduBlocks was developed by Joshua Lowe, a 14-year-old Python developer from the United Kingdom. I saw Joshua demonstrate his project at PyCon 2018 in May 2018.
Getting started

It's easy to install EduBlocks. The website provides clear installation instructions, and you can find detailed screenshots in the project's GitHub repository.

Install EduBlocks from the Raspberry Pi command line by issuing the following command: 

curl -sSL get.edublocks.org | bash

Programming EduBlocks

Once the installation is complete, launch EduBlocks from either the desktop shortcut or the Programming menu on the Raspberry Pi.


Once you launch the application, you can start creating Python 3 code with EduBlocks' drag-and-drop interface. Its menus are clearly labeled. You can start with sample code by clicking the Samples menu button. You can also choose a different color scheme for your programming palette by clicking Theme. With the Save menu, you can save your code as you work, then Download your Python code. Click Run to execute and test your code.

You can see your code by clicking the Blockly button at the far right. It allows you to toggle between the ""Blockly"" interface and the normal Python code view (as you would see in any other Python editor).

EduBlocks comes with a range of code libraries, including EduPython, Minecraft, Sonic Pi, GPIO Zero, and Sense Hat."
Python$Mu$Begginer,"Getting started with Mu, a Python editor for beginners
Meet Mu, an open source editor that makes it easy for students to learn to code Python.
Mu is a Python editor for beginning programmers, designed to make the learning experience more pleasant. It gives students the ability to experience success early on, which is important anytime you're learning something new.

If you have ever tried to teach young people how to program, you will immediately grasp the importance of Mu. Most programming tools are written by developers for developers and aren't well-suited for beginning programmers, regardless of their age. Mu, however, was written by a teacher for students.
Mu's origins

Mu is the brainchild of Nicholas Tollervey (who I heard speak at PyCon2018 in May). Nicholas is a classically trained musician who became interested in Python and development early in his career while working as a music teacher. He also wrote Python in Education, a free book you can download from O'Reilly.
Nicholas was looking for a simpler interface for Python programming. He wanted something without the complexity of other editors?even the IDLE3 editor that comes with Python?so he worked with Carrie Ann Philbin, director of education at the Raspberry Pi Foundation (which sponsored his work), to develop Mu.

Mu is an open source application (licensed under GNU GPLv3) written in Python. It was originally developed to work with the Micro:bit mini-computer, but feedback and requests from other teachers spurred him to rewrite Mu into a generic Python editor.
Inspired by music

Nicholas' inspiration for Mu came from his approach to teaching music. He wondered what would happen if we taught programming the way we teach music and immediately saw the disconnect. Unlike with programming, we don't have music boot camps and we don't learn to play an instrument from a book on, say, how to play the flute.

Nicholas says, Mu ""aims to be the real thing,"" because no one can learn Python in 30 minutes. As he developed Mu, he worked with teachers, observed coding clubs, and watched secondary school students as they worked with Python. He found that less is more and keeping things simple improves the finished product's functionality. Mu is only about 3,000 lines of code, Nicholas says.
Using Mu

To try it out, download Mu and follow the easy installation instructions for Linux, Windows, and Mac OS. If, like me, you want to install it on Raspberry Pi, enter the following in the terminal:

$ sudo apt-get update
$ sudo apt-get install mu

Launch Mu from the Programming menu. Then you'll have a choice about how you will use Mu.

I chose Python 3, which launches an environment to write code; the Python shell is directly below, which allows you to see the code execution.

The menu is very simple to use and understand, which achieves Mu's purpose?making coding easy for beginning programmers."
Python$Library,"7 Python libraries for more maintainable code
Check your Python code's health and make it easier to maintain with these external libraries.
It's easy to let readability and coding standards fall by the wayside when a software project moves into ""maintenance mode."" (It's also easy to never establish those standards in the first place.) But maintaining consistent style and testing standards across a codebase is an important part of decreasing the maintenance burden, ensuring that future developers are able to quickly grok what's happening in a new-to-them project and safeguarding the health of the app over time.
A great way to protect the future maintainability of a project is to use external libraries to check your code health for you. These are a few of our favorite libraries for linting code (checking for PEP 8 and other style errors), enforcing a consistent style, and ensuring acceptable test coverage as a project reaches maturity.
PEP 8 is the Python code style guide, and it sets out rules for things like line length, indentation, multi-line expressions, and naming conventions. Your team might also have your own style rules that differ slightly from PEP 8. The goal of any code style guide is to enforce consistent standards across a codebase to make it more readable, and thus more maintainable. Here are three libraries to help prettify your code.
1. Pylint
Pylint is a library that checks for PEP 8 style violations and common errors. It integrates well with several popular editors and IDEs and can also be run from the command line.
To install, run pip install pylint.
To use Pylint from the command line, run pylint [options] path/to/dir or pylint [options] path/to/module.py. Pylint will output warnings about style violations and other errors to the console.
You can customize what errors Pylint checks for with a configuration file called pylintrc.
2. Flake8
Flake8 is a ""Python tool that glues together PEP8, Pyflakes (similar to Pylint), McCabe (code complexity checker), and third-party plugins to check the style and quality of some Python code.""
To use Flake8, run pip install flake8. Then run flake8 [options] path/to/dir or flake8 [options] path/to/module.py to see its errors and warnings.
Like Pylint, Flake8 permits some customization for what it checks for with a configuration file. It has very clear docs, including some on useful commit hooks to automatically check your code as part of your development workflow.
Flake8 integrates with popular editors and IDEs, but those instructions generally aren't found in the docs. To integrate Flake8 with your favorite editor or IDE, search online for plugins (for example, Flake8 plugin for Sublime Text).
3. Isort
Isort is a library that sorts your imports alphabetically and breaks them up into appropriate sections (e.g., standard library imports, third-party library imports, imports from your own project, etc.). This increases readability and makes it easier to locate imports if you have a lot of them in your module.
Install isort with pip install isort, and run it with isort path/to/module.py. More configuration options are in the documentation. For example, you can configure how isort handles multi-line imports from one library in an .isort.cfg file.
Like Flake8 and Pylint, isort also provides plugins that integrate it with popular editors and IDEs.
Outsource your code style
Remembering to run linters manually from the command line for each file you change is a pain, and you might not like how a particular plugin behaves with your IDE. Also, your colleagues might prefer different linters or might not have plugins for their favorite editors, or you might be less meticulous about always running the linter and correcting the warnings. Over time, the codebase you all share will get messy and harder to read.
A great solution is to use a library that automatically reformats your code into something that passes PEP 8 for you. The three libraries we recommend all have different levels of customization and different defaults for how they format code. Some of these are more opinionated than others, so like with Pylint and Flake8, you'll want to test these out to see which offers the customizations you can't live without? and the unchangeable defaults you can live with.
4. Autopep8
Autopep8 automatically formats the code in the module you specify. It will re-indent lines, fix indentation, remove extraneous whitespace, and refactor common comparison mistakes (like with booleans and None). See a full list of corrections in the docs.
To install, run pip install --upgrade autopep8. To reformat code in place, run autopep8 --in-place --aggressive --aggressive <filename>. The aggressive flags (and the number of them) indicate how much control you want to give autopep8 over your code style. Read more about aggressive options.
5. Yapf
Yapf is yet another option for reformatting code that comes with its own list of configuration options. It differs from autopep8 in that it doesn't just address PEP 8 violations. It also reformats code that doesn't violate PEP 8 specifically but isn't styled consistently or could be formatted better for readability.
To install, run pip install yapf. To reformat code, run, yapf [options] path/to/dir or yapf [options] path/to/module.py. There is also a full list of customization options.
6. Black
Black is the new kid on the block for linters that reformat code in place. It's similar to autopep8 and Yapf, but way more opinionated. It has very few options for customization, which is kind of the point. The idea is that you shouldn't have to make decisions about code style; the only decision to make is to let Black decide for you. You can read about limited customization options and instructions on storing them in a configuration file.
Black requires Python 3.6+ but can format Python 2 code. To use, run pip install black. To prettify your code, run: black path/to/dir or black path/to/module.py.
Check your test coverage
You're writing tests, right? Then you will want to make sure new code committed to your codebase is tested and doesn't drop your overall amount of test coverage. While percentage of test coverage is not the only metric you should use to measure the effectiveness and sufficiency of your tests, it is one way to ensure basic testing standards are being followed in your project. For measuring test coverage, we have one recommendation: Coverage.
7. Coverage
Coverage has several options for the way it reports your test coverage to you, including outputting results to the console or to an HTML page and indicating which line numbers are missing test coverage. You can set up a configuration file to customize what Coverage checks for and make it easier to run.
To install, run pip install coverage. To run a program and see its output, run coverage run [path/to/module.py] [args], and you will see your program's output. To see a report of which lines of code are missing coverage, run coverage report -m.
Continuous integration tools
Continuous integration (CI) is a series of processes you can run to automatically check for linter errors and test coverage minimums before you merge and deploy code. There are lots of free or paid tools to automate this process, and a thorough walkthrough is beyond the scope of this article. But because setting up a CI process is an important step in removing blocks to more readable and maintainable code, you should investigate continuous integration tools in general; check out Travis CI and Jenkins in particular."
Python$Functions$Library,"How to retrieve source code of Python functions
Learn to use the inspect and dill libraries to access Python functions' source code.


Sometimes we want to know what some functions' source codes look like or where they are, or we need to manipulate the source codes as character strings. In such cases, we need to have a convenient way to retrieve our Python functions' source codes.

There are two Python libraries that may help:

    inspect is a built-in standard library
    dill is a third-party library

inspect

inspect is a built-in library. It's already there after you install Python on your computer. The inspect module provides several useful functions to help you get information about live objects, such as modules, classes, methods, functions, tracebacks, frame objects, and code objects. Among its many features, its capability to retrieve the source code of functions stands out.
In [1]:  

import pandas
import inspect

In [3]:  

source_DF = inspect.getsource(pandas.DataFrame)
print(type(source_DF))

<<class 'str'>>
In [4]:  

print(len(source_DF))

218432
In [5]:  

print(source_DF[:200])

class DataFrame(NDFrame):
    """""" Two-dimensional size-mutable, potentially heterogeneous tabular data
    structure with labeled axes (rows and columns). Arithmetic operations
    align on both row a
In [6]:  

source_file_DF = inspect.getsourcefile(pandas.DataFrame)
print(source_file_DF)

D:\Users\dengdong\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py
In [7]:  

sourcelines_DF = inspect.getsourcelines(pandas.DataFrame)
print(type(sourcelines_DF))
print(len(sourcelines_DF))
print(type(sourcelines_DF[0]))

<class 'tuple'>
2
<class 'list'>

In IPython or Jupyter, we can also use this method to retrieve the source code of the functions that we defined in the console.
In [9]:  

def test(x):

   return x*2

print(inspect.getsource(test))

def test(x): return x*2
In [10]:  

print(inspect.getsourcefile(test))

<ipython-input-9-70ac3e17460c>
In [11]:  

print(inspect.getsourcelines(test))

(['def test(x):\n', ' return x*2\n'], 1)

Note that retrieving source codes of self-defined functions only works in IPython or Jupyter. If we are using plain Python and define a function interactively, we will encounter error IOError: could not get source code and will not be able to retrieve the source code. This is because its setting only supports objects loaded from files, not interactive sessions.
dill

dill extends Python's pickle module for serializing and deserializing Python objects to the majority of the built-in Python types. At the same time, it can also retrieve the source code of your Python objects. Please note dill is not a standard library, so you must install it separately.

Its API is quite similar to inspect's.
In [6]:  

import dill

source_DF = dill.source.getsource(pandas.DataFrame)
print(type(source_DF))
print(len(source_DF))
print(source_DF[:200])

source_file_DF = dill.source.getsourcefile(pandas.DataFrame)
print(source_file_DF)

sourcelines_DF = dill.source.getsourcelines(pandas.DataFrame)
print(type(sourcelines_DF))
print(len(sourcelines_DF))
print(type(sourcelines_DF[0]))

<type 'str'>
195262
class DataFrame(NDFrame):
    """""" Two-dimensional size-mutable, potentially heterogeneous tabular data
    structure with labeled axes (rows and columns). Arithmetic operations
    align on both row a
/Users/XD/anaconda/lib/python2.7/site-packages/pandas/core/frame.py
<type 'tuple'>
2
<type 'list'>

 

However, a big difference between dill and inspect is that dill's retrieving feature supports self-defined objects in the plain Python console."
Python$Library$Mako$Jinja2$Genshi,"3 Python template libraries compared
Does your next Python project need a templating engine to automatically generate HTML? Here are a few options.
In my day job, I spend a lot of time wrangling data from various sources into human-readable information. While a lot of the time this just takes the form of a spreadsheet or some type of chart or other data visualization, there are other times when it makes sense to present the data instead in a written format.

But a pet peeve of mine is copying and pasting. If you?re moving data from its source to a standardized template, you shouldn?t be copying and pasting either. It?s error-prone, and honestly, it?s not a good use of your time.

So for any piece of information I send out regularly which follows a common pattern, I tend to find some way to automate at least a chunk of it. Maybe that involves creating a few formulas in a spreadsheet, a quick shell script, or some other solution to autofill a template with information pulled from an outside source.

But lately, I?ve been exploring Python templating to do much of the work of creating reports and graphs from other datasets.

Python templating engines are hugely powerful. My use case of simplifying report creation only scratches the surface of what they can be put to work for. Many developers are making use of these tools to build full-fledged web applications and content management systems. But you don?t have to have a grand vision of a complicated web app to make use of Python templating tools.
Why templating?

Each templating tool is a little different, and you should read the documentation to understand the exact usage. But let?s create a hypothetical example. Let?s say I?d like to create a short page listing all of the Python topics I've written about recently. Something like this:

<html>
  <head>
    <title>My Python articles</title>
  </head>
  <body>
    <p>These are some of the things I have written about Python:</p>
    <ul>
      <li>Python GUIs</li>
      <li>Python IDEs</li>
      <li>Python web scrapers</li>
    </ul>
  </body>
</html>

Simple enough to maintain when it?s just these three items. But what happens when I want to add a fourth, or fifth, or sixty-seventh? Rather than hand-coding this page, could I generate it from a CSV or other data file containing a list of all of my pages? Could I easily create duplicates of this for every topic I've written on? Could I programmatically change the text or title or heading on each one of those pages? That's where a templating engine can come into play.

There are many different options to choose from, and today I'll share with you three, in no particular order: Mako, Jinja2, and Genshi.
Mako

Mako is a Python templating tool released under the MIT license that is designed for fast performance (not unlike Jinja2). Mako has been used by Reddit to power their web pages, as well as being the default templating language for web frameworks like Pyramid and Pylons. It's also fairly simple and straightforward to use; you can design templates with just a couple of lines of code. Supporting both Python 2.x and 3.x, it's a powerful and feature-rich tool with good documentation, which I consider a must. Features include filters, inheritance, callable blocks, and a built-in caching system, which could be import for large or complex web projects.
Jinja2

Jinja2 is another speedy and full-featured option, available for both Python 2.x and 3.x under a BSD license. Jinja2 has a lot of overlap from a feature perspective with Mako, so for a newcomer, your choice between the two may come down to which formatting style you prefer. Jinja2 also compiles your templates to bytecode, and has features like HTML escaping, sandboxing, template inheritance, and the ability to sandbox portions of templates. Its users include Mozilla, SourceForge, NPR, Instagram, and others, and also features strong documentation. Unlike Mako, which uses Python inline for logic inside your templates, Jinja2 uses its own syntax.
Genshi

Genshi is the third option I'll mention. It's really an XML tool which has a strong templating component, so if the data you are working with is already in XML format, or you need to work with formatting beyond a web page, Genshi might be a good solution for you. HTML is basically a type of XML (well, not precisely, but that's beyond the scope of this article and a bit pedantic), so formatting them is quite similar. Since a lot of the data I work with commonly is in one flavor of XML or another, I appreciated working with a tool I could use for multiple things.

The release version currently only supports Python 2.x, although Python 3 support exists in trunk, I would caution you that it does not appear to be receiving active development. Genshi is made available under a BSD license.
Example

So in our hypothetical example above, rather than update the HTML file every time I write about a new topic, I can update it programmatically. I can create a template, which might look like this:

<html>
  <head>
    <title>My Python articles</title>
  </head>
  <body>
    <p>These are some of the things I have written about Python:</p>
    <ul>
      %for topic in topics:
      <li>${topic}</li>
      %endfor
    </ul>
  </body>
</html>

And then I can iterate across each topic with my templating library, in this case, Mako, like this:

from mako.template import Template

mytemplate = Template(filename='template.txt')
print(mytemplate.render(topics=(""Python GUIs"",""Python IDEs"",""Python web scrapers"")))

Of course, in a real-world usage, rather than listing the contents manually in a variable, I would likely pull them from an outside data source, like a database or an API."
Python$MachineLearning,"Using machine learning to color cartoons
Can we automate applying a simple color scheme without hand-drawing hundreds of examples as training data?

A big problem with supervised machine learning is the need for huge amounts of labeled data. It's a big problem especially if you don't have the labeled data?and even in a world awash with big data, most of us don't.

Although a few companies have access to enormous quantities of certain kinds of labeled data, for most organizations and many applications, creating sufficient quantities of the right kind of labeled data is cost prohibitive or impossible. Sometimes the domain is one in which there just isn't much data (for example, when diagnosing a rare disease or determining whether a signature matches a few known exemplars). Other times the volume of data needed multiplied by the cost of human labeling by Amazon Turkers or summer interns is just too high. Paying to label every frame of a movie-length video adds up fast, even at a penny a frame.
The big problem of big data requirements

The specific problem our group set out to solve was: Can we train a model to automate applying a simple color scheme to a black and white character without hand-drawing hundreds or thousands of examples as training data?

In this experiment (which we called DragonPaint), we confronted the problem of deep learning's enormous labeled-data requirements using:

    A rule-based strategy for extreme augmentation of small datasets
    A borrowed TensorFlow image-to-image translation model, Pix2Pix, to automate cartoon coloring with very limited training data

I had seen Pix2Pix, a machine learning image-to-image translation model described in a paper (""Image-to-Image Translation with Conditional Adversarial Networks,"" by Isola, et al.), that colorizes landscapes after training on AB pairs where A is the grayscale version of landscape B. My problem seemed similar. The only problem was training data.

I needed the training data to be very limited because I didn't want to draw and color a lifetime supply of cartoon characters just to train the model. The tens of thousands (or hundreds of thousands) of examples often required by deep-learning models were out of the question.

Based on Pix2Pix's examples, we would need at least 400 to 1,000 sketch/colored pairs. How many was I willing to draw? Maybe 30. I drew a few dozen cartoon flowers and dragons and asked whether I could somehow turn this into a training set.
The 80% solution: color by component

When faced with a shortage of training data, the first question to ask is whether there is a good non-machine-learning based approach to our problem. If there's not a complete solution, is there a partial solution, and would a partial solution do us any good? Do we even need machine learning to color flowers and dragons? Or can we specify geometric rules for coloring?

There is a non-machine-learning approach to solving my problem. I could tell a kid how I want my drawings colored: Make the flower's center orange and the petals yellow. Make the dragon's body orange and the spikes yellow.

At first, that doesn't seem helpful because our computer doesn't know what a center or a petal or a body or a spike is. But it turns out we can define the flower or dragon parts in terms of connected components and get a geometric solution for coloring about 80% of our drawings. Although 80% isn't enough, we can bootstrap from that partial-rule-based solution to 100% using strategic rule-breaking transformations, augmentations, and machine learning.

Connected components are what is colored when you use Windows Paint (or a similar application). For example, when coloring a binary black and white image, if you click on a white pixel, the white pixels that are be reached without crossing over black are colored the new color. In a ""rule-conforming"" cartoon dragon or flower sketch, the biggest white component is the background. The next biggest is the body (plus the arms and legs) or the flower's center. The rest are spikes or petals, except for the dragon's eye, which can be distinguished by its distance from the background.
Using strategic rule breaking and Pix2Pix to get to 100%

Some of my sketches aren't rule-conforming. A sloppily drawn line might leave a gap. A back limb will get colored like a spike. A small, centered daisy will switch a petal and the center's coloring rules.

For the 20% we couldn't color with the geometric rules, we needed something else. We turned to Pix2Pix, which requires a minimum training set of 400 to 1,000 sketch/colored pairs (i.e., the smallest training sets in the Pix2Pix paper) including rule-breaking pairs.

So, for each rule-breaking example, we finished the coloring by hand (e.g., back limbs) or took a few rule-abiding sketch/colored pairs and broke the rule. We erased a bit of a line in A or we transformed a fat, centered flower pair A and B with the same function (f) to create a new pair f(A) and f(B)?a small, centered flower. That got us to a training set.
Extreme augmentations with gaussian filters and homeomorphisms

It's common in computer vision to augment an image training set with geometric transformations, such as rotation, translation, and zoom.

But what if we need to turn sunflowers into daisies or make a dragon's nose bulbous or pointy?

Or what if we just need an enormous increase in data volume without overfitting? Here we need a dataset 10 to 30 times larger than what we started with.

Certain homeomorphisms of the unit disk make good daisies (e.g., r -> r cubed) and Gaussian filters change a dragon's nose. Both were extremely useful for creating augmentations for our dataset and produced the augmentation volume we needed, but they also started to change the style of the drawings in ways that an affine transformation could not.

This inspired questions beyond how to automate a simple coloring scheme: What defines an artist's style, either to an outside viewer or the artist? When does an artist adopt as their own a drawing they could not have made without the algorithm? When does the subject matter become unrecognizable? What's the difference between a tool, an assistant, and a collaborator?
How far can we go?

How little can we draw for input and how much variation and complexity can we create while staying within a subject and style recognizable as the artist's? What would we need to do to make an infinite parade of giraffes or dragons or flowers? And if we had one, what could we do with it?

Those are questions we'll continue to explore in future work.

But for now, the rules, augmentations, and Pix2Pix model worked. We can color flowers really well, and the dragons aren't bad."
Python$MachineLearning$bytecode,"An introduction to Python bytecode
Learn what Python bytecode is, how Python uses it to execute your code, and how knowing what it does can help you.
If you've ever written, or even just used, Python, you're probably used to seeing Python source code files; they have names ending in .py. And you may also have seen another type of file, with a name ending in .pyc, and you may have heard that they're Python ""bytecode"" files. (These are a bit harder to see on Python 3?instead of ending up in the same directory as your .py files, they go into a subdirectory called __pycache__.) And maybe you've heard that this is some kind of time-saver that prevents Python from having to re-parse your source code every time it runs.

But beyond ""oh, that's Python bytecode,"" do you really know what's in those files and how Python uses them?

If not, today's your lucky day! I'll take you through what Python bytecode is, how Python uses it to execute your code, and how knowing about it can help you.
How Python works

Python is often described as an interpreted language?one in which your source code is translated into native CPU instructions as the program runs?but this is only partially correct. Python, like many interpreted languages, actually compiles source code to a set of instructions for a virtual machine, and the Python interpreter is an implementation of that virtual machine. This intermediate format is called ""bytecode.""

So those .pyc files Python leaves lying around aren't just some ""faster"" or ""optimized"" version of your source code; they're the bytecode instructions that will be executed by Python's virtual machine as your program runs.

Let's look at an example. Here's a classic ""Hello, World!"" written in Python:

def hello()
    print(""Hello, World!"")

And here's the bytecode it turns into (translated into a human-readable form):

2           0 LOAD_GLOBAL              0 (print)
            2 LOAD_CONST               1 ('Hello, World!')
            4 CALL_FUNCTION            1

If you type up that hello() function and use the CPython interpreter to run it, the above listing is what Python will execute. It might look a little weird, though, so let's take a deeper look at what's going on.

Inside the Python virtual machine

CPython uses a stack-based virtual machine. That is, it's oriented entirely around stack data structures (where you can ""push"" an item onto the ""top"" of the structure, or ""pop"" an item off the ""top"").

CPython uses three types of stacks:

    The call stack. This is the main structure of a running Python program. It has one item?a ""frame""?for each currently active function call, with the bottom of the stack being the entry point of the program. Every function call pushes a new frame onto the call stack, and every time a function call returns, its frame is popped off.
    In each frame, there's an evaluation stack (also called the data stack). This stack is where execution of a Python function occurs, and executing Python code consists mostly of pushing things onto this stack, manipulating them, and popping them back off.
    Also in each frame, there's a block stack. This is used by Python to keep track of certain types of control structures: loops, try/except blocks, and with blocks all cause entries to be pushed onto the block stack, and the block stack gets popped whenever you exit one of those structures. This helps Python know which blocks are active at any given moment so that, for example, a continue or break statement can affect the correct block.

Most of Python's bytecode instructions manipulate the evaluation stack of the current call-stack frame, although there are some instructions that do other things (like jump to specific instructions or manipulate the block stack).

To get a feel for this, suppose we have some code that calls a function, like this: my_function(my_variable, 2). Python will translate this into a sequence of four bytecode instructions:

    A LOAD_NAME instruction that looks up the function object my_function and pushes it onto the top of the evaluation stack
    Another LOAD_NAME instruction to look up the variable my_variable and push it on top of the evaluation stack
    A LOAD_CONST instruction to push the literal integer value 2 on top of the evaluation stack
    A CALL_FUNCTION instruction

The CALL_FUNCTION instruction will have an argument of 2, which indicates that Python needs to pop two positional arguments off the top of the stack; then the function to call will be on top, and it can be popped as well (for functions involving keyword arguments, a different instruction?CALL_FUNCTION_KW?is used, but with a similar principle of operation, and a third instruction, CALL_FUNCTION_EX, is used for function calls that involve argument unpacking with the * or ** operators). Once Python has all that, it will allocate a new frame on the call stack, populate the local variables for the function call, and execute the bytecode of my_function inside that frame. Once that's done, the frame will be popped off the call stack, and in the original frame the return value of my_function will be pushed on top of the evaluation stack.
Accessing and understanding Python bytecode

If you want to play around with this, the dis module in the Python standard library is a huge help; the dis module provides a ""disassembler"" for Python bytecode, making it easy to get a human-readable version and look up the various bytecode instructions. The documentation for the dis module goes over its contents and provides a full list of bytecode instructions along with what they do and what arguments they take.

For example, to get the bytecode listing for the hello() function above, I typed it into a Python interpreter, then ran:

import dis
dis.dis(hello)

The function dis.dis() will disassemble a function, method, class, module, compiled Python code object, or string literal containing source code and print a human-readable version. Another handy function in the dis module is distb(). You can pass it a Python traceback object or call it after an exception has been raised, and it will disassemble the topmost function on the call stack at the time of the exception, print its bytecode, and insert a pointer to the instruction that raised the exception.

It's also useful to look at the compiled code objects Python builds for every function since executing a function makes use of attributes of those code objects. Here's an example looking at the hello() function:

>>> hello.__code__
<code object hello at 0x104e46930, file ""<stdin>"", line 1>
>>> hello.__code__.co_consts
(None, 'Hello, World!')
>>> hello.__code__.co_varnames
()
>>> hello.__code__.co_names
('print',)

The code object is accessible as the attribute __code__ on the function and carries a few important attributes:

    co_consts is a tuple of any literals that occur in the function body
    co_varnames is a tuple containing the names of any local variables used in the function body
    co_names is a tuple of any non-local names referenced in the function body

Many bytecode instructions?particularly those that load values to be pushed onto the stack or store values in variables and attributes?use indices in these tuples as their arguments.

So now we can understand the bytecode listing of the hello() function:

    LOAD_GLOBAL 0: tells Python to look up the global object referenced by the name at index 0 of co_names (which is the print function) and push it onto the evaluation stack
    LOAD_CONST 1: takes the literal value at index 1 of co_consts and pushes it (the value at index 0 is the literal None, which is present in co_consts because Python function calls have an implicit return value of None if no explicit return statement is reached)
    CALL_FUNCTION 1: tells Python to call a function; it will need to pop one positional argument off the stack, then the new top-of-stack will be the function to call.

The ""raw"" bytecode?as non-human-readable bytes?is also available on the code object as the attribute co_code. You can use the list dis.opname to look up the names of bytecode instructions from their decimal byte values if you'd like to try to manually disassemble a function.
Putting bytecode to use

Now that you've read this far, you might be thinking ""OK, I guess that's cool, but what's the practical value of knowing this?"" Setting aside curiosity for curiosity's sake, understanding Python bytecode is useful in a few ways.

First, understanding Python's execution model helps you reason about your code. People like to joke about C being a kind of ""portable assembler,"" where you can make good guesses about what machine instructions a particular chunk of C source code will turn into. Understanding bytecode will give you the same ability with Python?if you can anticipate what bytecode your Python source code turns into, you can make better decisions about how to write and optimize it.

Second, understanding bytecode is a useful way to answer questions about Python. For example, I often see newer Python programmers wondering why certain constructs are faster than others (like why {} is faster than dict()). Knowing how to access and read Python bytecode lets you work out the answers (try it: dis.dis(""{}"") versus dis.dis(""dict()"")).

Finally, understanding bytecode and how Python executes it gives a useful perspective on a particular kind of programming that Python programmers don't often engage in: stack-oriented programming. If you've ever used a stack-oriented language like FORTH or Factor, this may be old news, but if you're not familiar with this approach, learning about Python bytecode and understanding how its stack-oriented programming model works is a neat way to broaden your programming knowledge."
Python$Netflix$outage$failover,"How Netflix does failovers in 7 minutes flat
Netflix decreased the time it takes to respond to an outage from 45 minutes to seven with no additional cost.
During winter 2012, Netflix suffered an extended outage that lasted for seven hours due to problems in the AWS Elastic Load Balancer service in the US-East region. (Netflix runs on Amazon Web Services [AWS]?we don't have any data centers of our own. All of your interactions with Netflix are served from AWS, except the actual streaming of the video. Once you click ""play,"" the actual video files are served from our own CDN.) During the outage, none of the traffic going into US-East was reaching our services.

To prevent this from happening again, we decided to build a system of regional failovers that is resilient to failures of our underlying service providers. Failover is a method of protecting computer systems from failure in which standby equipment automatically takes over when the main system fails.
Regional failovers decreased the risk

We expanded to a total of three AWS regions: two in the United States (US-East and US-West) and one in the European Union (EU). We reserved enough capacity to perform a failover so that we can absorb an outage of a single region.

A typical failover looks like this:

    Realize that one of the regions is having trouble.
    Scale up the two savior regions.
    Proxy some traffic from the troubled region to the saviors.
    Change DNS away from the problem region to the savior regions.

We need metrics, and preferably a single metric, that can tell us the health of the system. At Netflix, we use a business metric called stream starts per second (SPS for short). This is a count of the number of clients that have successfully started streaming a show.

We have this data partitioned per region, and at any given time we can plot the SPS data for each region and compare it against the SPS value from the day before and the week before. When we notice a dip in the SPS graph, we know our customers are not able to start streaming shows, thus we're in trouble.

The trouble isn't necessarily a cloud infrastructure issue. It could be a bad code deploy in one of the hundreds of microservices that make up the Netflix ecosystem, a cut in an undersea cable, etc. We may not know the reason; we simply know that something is wrong.

If this dip in SPS is observed only in one region, it's a great candidate for regional failover. If the dip is observed in multiple regions, we're out of luck because we only have enough capacity to evacuate one region at a time. This is precisely why we stagger the deployment of our microservices to one region at a time. If there is a problem with a deployment, we can evacuate immediately and debug the issue later. Similarly, we want to avoid failing over when the problem would follow the traffic redirection (like would happen in a DDoS attack.)
2. Scale up the saviors

Once we have identified the sick region, we should prep the other regions (the ""saviors"") to receive the traffic from the sicko. Before we turn on the fire hose we need to scale the stack in the savior regions appropriately.

What does scaling appropriately mean in this context? Netflix's traffic pattern is not static throughout the day. We have peak viewing hours, usually around 6-9pm But 6pm arrives at different times in different parts of the world. The peak traffic in US-East is three hours ahead of US-West, which is eight hours behind the EU region.

When we failover US-East, we send traffic from the Eastern U.S. to the EU and traffic from South America to US-West. This is to reduce the latency and provide the best possible experience for our customers.

Taking this into consideration, we can use linear regression to predict the traffic that will be routed to the savior regions for that time of day (and day of week) using the historical scaling behavior of each microservice.

Once we have determined the appropriate size for each microservice, we trigger scaling for each of them by setting the desired size of each cluster and then let AWS do its magic.
3. Proxy traffic

Now that the microservice clusters have been scaled, we start proxying traffic from the sick region to the savior regions. Netflix has built a high-performance, cross-regional edge proxy called Zuul, which we have open sourced.

These proxy services are designed to authenticate requests, do load shedding, retry failed requests, etc. The Zuul proxy can also do cross-region proxying. We use this feature to route a trickle of traffic away from the suffering region, then progressively increase the amount of rerouted traffic until it reaches 100%.

This progressive proxying allows our services to use their scaling policies to do any reactive scaling necessary to handle the incoming traffic. This is to compensate for any change in traffic volume between the time when we did our scaling predictions and the time it took to scale each cluster.

Zuul does the heavy lifting at this point to route all incoming traffic from a sick region to the healthy regions. But the time has come to abandon the affected region completely. This is where the DNS switching comes into play.
4. Switch the DNS

The last step in the failover is to update the DNS records that point to the affected region and redirect them to the healthy regions. This will completely move all client traffic away from the sick region. Any clients that don't expire their DNS cache will still be routed by the Zuul layer in the affected region.

That's the background information of how failover used to work at Netflix. This process took a long time to complete?about 45 minutes (on a good day).

Speeding response with shiny, new processes

We noticed that majority of the time (approximately 35 minutes) was spent waiting for the savior regions to scale. Even though AWS could provision new instances for us in a matter of minutes, starting up the services, doing just-in-time warm-up, and handling other startup tasks before registering UP in discovery dominated the scaling process.

We decided this was too long. We wanted our failovers to complete in under 10 minutes. We wanted to do this without adding operational burden to the service owners. We also wanted to stay cost-neutral.

We reserve capacity in all three regions to absorb the failover traffic; if we're already paying for all that capacity, why not use it? Thus began Project Nimble.

Our idea was to maintain a pool of instances in hot standby for each microservice. When we are ready to do a failover, we can simply inject our hot standby into the clusters to take live traffic.

The unused reserved capacity is called trough. A few teams at Netflix use some of the trough capacity to run batch jobs, so we can't simply turn all the available trough into hot standby. Instead, we can maintain a shadow cluster for each microservice that we run and stock that shadow cluster with just enough instances to take the failover traffic for that time of day. The rest of the instances are available for batch jobs to use as they please.

At the time of failover, instead of the traditional scaling method that triggers AWS to provision instances for us, we inject the instances from the shadow cluster into the live cluster. This process takes about four minutes, as opposed to the 35 minutes it used to take.

Since our capacity injection is swift, we don't have to cautiously move the traffic by proxying to allow scaling policies to react. We can simply switch the DNS and open the floodgates, thus shaving even more precious minutes during an outage.

We added filters in the shadow cluster to prevent the dark instances from reporting metrics. Otherwise, they will pollute the metric space and confuse the normal operating behavior.

We also stopped the instances in the shadow clusters from registering themselves UP in discovery by modifying our discovery client. These instances will continue to remain in the dark (pun fully intended) until we trigger a failover.

Now we can do regional failovers in seven minutes. Since we utilized our existing reserved capacity, we didn't incur any additional infrastructure costs. The software that orchestrates the failover is written in Python by a team of three engineers."
Python$ChatOps$Opsdroid$Errbot,"Python ChatOps libraries: Opsdroid and Errbot
Learn about the most widely used ChatOps libraries in the Python world?what each does well and how to get started.


ChatOps is conversation-driven development. The idea is you can write code that is executed in response to something typed in a chat window. As a developer, you could use ChatOps to merge pull requests from Slack, automatically assign a support ticket to someone from a received Facebook message, or check the status of a deployment through IRC.

In the Python world, the most widely used ChatOps libraries are Opsdroid and Errbot. In this month's Python column, let's chat about what it's like to use them, what each does well, and how to get started with them.
Opsdroid

Opsdroid is a relatively young (since 2016) open source chatbot library written in Python. It has good documentation, a great tutorial, and includes plugins to help you connect to popular chat services.
What's built in

The library itself doesn't ship with everything you need to get started, but this is by design. The lightweight framework encourages you to enable its existing connectors (what Opsdroid calls the plugins that help you connect to chat services) or write your own, but it doesn't weigh itself down by shipping with connectors you may not need. You can easily enable existing Opsdroid connectors for:

    The command line
    Cisco Spark
    Facebook
    GitHub
    Matrix
    Slack
    Telegram
    Twitter
    Websockets

Opsdroid calls the functions the chatbot performs ""skills."" Skills are async Python functions and use Opsdroid's matching decorators, called ""matchers."" You can configure your Opsdroid project to use skills from the same codebase your configuration file is in or import skills from outside public or private repositories.

You can enable some existing Opsdroid skills as well, including seen, which tells you when a specific user was last seen by the bot, and weather, which will report the weather to the user.

Finally, Opdroid allows you to configure databases using its existing database modules. Current databases with Opsdroid support include:

    Mongo
    Redis
    SQLite

You configure databases, skills, and connectors in the configuration.yaml file in your Opsdroid project.
Opsdroid pros

Docker support: Opsdroid is meant to work well in Docker from the get-go. Docker instructions are part of its installation documentation. Using Opsdroid with Docker Compose is also simple: Set up Opsdroid as a service and when you run docker-compose up, your Opsdroid service will start and your chatbot will be ready to chat.

version: ""3""

services:
  opsdroid:
    container_name: opsdroid
    build:
      context: .
      dockerfile: Dockerfile

Lots of connectors: Opsdroid supports nine connectors to services like Slack and GitHub out of the box; all you need to do is enable those connectors in your configuration file and pass necessary tokens or API keys. For example, to enable Opsdroid to post in a Slack channel named #updates, add this to the connectors section of your configuration file:

- name: slack
    api-token: ""this-is-my-token""
    default-room: ""#updates""

You will have to add a bot user to your Slack workspace before configuring Opsdroid to connect to Slack.

If you need to connect to a service that Opsdroid does not support, there are instructions for adding your own connectors in the docs.

Pretty good docs. Especially for a young-ish library in active development, Opsdroid's docs are very helpful. The docs include a tutorial that leads you through creating a couple of different basic skills. The Opsdroid documentation on skills, connectors, databases, and matchers is also clear.

The repositories for its supported skills and connectors provide helpful example code for when you start writing your own custom skills and connectors.

Natural language processing: Opsdroid supports regular expressions for its skills, but also several NLP APIs, including Dialogflow, luis.ai, Recast.AI, and wit.ai.
Possible Opsdroid concern

Opsdroid doesn't yet enable the full features of some of its connectors. For example, the Slack API allows you to add color bars, images, and other ""attachments"" to your message. The Opsdroid Slack connector doesn't enable the ""attachments"" feature, so you would need to write a custom Slack connector if those features were important to you. If a connector is missing a feature you need, though, Opsdroid would welcome your contribution. The docs could use some more examples, especially of expected use cases.
Example usage

hello/__init__.py

from opsdroid.matchers import match_regex
import random


@match_regex(r'hi|hello|hey|hallo')
async def hello(opsdroid, config, message):
    text = random.choice([""Hi {}"", ""Hello {}"", ""Hey {}""]).format(message.user)
    await message.respond(text)

configuration.yaml

connectors:
  - name: websocket

skills:

  - name: hello
    repo: ""https://github.com/<user_id>/hello-skill""

Errbot

Errbot is a batteries-included open source chatbot. Errbot was released in 2012 and has everything anyone would expect from a mature project, including good documentation, a great tutorial, and plenty of plugins to help you connect to existing popular chat services.
What's built in

Unlike Opsdroid, which takes a more lightweight approach, Errbot ships with everything you need to build a customized bot safely.

Errbot includes support for XMPP, IRC, Slack, Hipchat, and Telegram services natively. It lists support for 10 other services through community-supplied backends.
Errbot pros

Good docs: Errbot's docs are mature and easy to use.

Dynamic plugin architecture: Errbot allow you to securely install, uninstall, update, enable, and disable plugins by chatting with the bot. This makes development and adding features easy. For the security conscious, this can all be locked down thanks to Errbot's granular permission system.

Errbot uses your plugin docstrings to generate documentation for available commands when someone types !help, which makes it easier to know what each command does.

Built-in administration and security: Errbot allows you to restrict lists of users who have administrative rights and even has fine-grained access controls. For example, you can restrict which commands may be called by specific users and/or specific rooms.

Extensive plugin framework: Errbot supports hooks, callbacks, subcommands, webhooks, polling, and many more features. If those aren't enough, you can even write Dynamic plugins. This feature is useful if you want to enable chat commands based on what commands are available on a remote server.

Ships with a testing framework: Errbot supports pytest and ships with some useful utilities that make testing your plugins easy and possible. Its ""testing your plugins"" docs are well thought out and provide enough to get started.
Possible Errbot concerns

Initial !: By default, Errbot commands are issued starting with an exclamation mark (!help and !hello). Some people may like this, but others may find it annoying. Thankfully, this is easy to turn off.

Plugin metadata: At first, Errbot's Hello World plugin example seems easy to use. However, I couldn't get my plugin to load until I read further into the tutorial and discovered that I also needed a .plug file, a file Errbot uses to load plugins. This is a pretty minor nitpick, but it wasn't obvious to me until I dug further into the docs.
Example usage

hello.py

import random
from errbot import BotPlugin, botcmd

class Hello(BotPlugin):

    @botcmd
    def hello(self, msg, args):
        text = random.choice([""Hi {}"", ""Hello {}"", ""Hey {}""]).format(message.user)
        return text

hello.plug

[Core]
Name = Hello
Module = hello

[Python]
Version = 2+

[Documentation]
Description = Example ""Hello"" plugin"
Python$Rcounterculture$SCALE,"A glimpse into R counterculture
The statistical computing languages R and Python offer similar features. The decision comes down to contrasting philosophies.
Back in 2009, Anne Milley of SAS dismissed the increasing significance of the R language (whose rivals include SAS, Python, and, more recently, Julia) in a New York Times article. She said:

    ""We have customers who build engines for aircraft. I am happy they are not using freeware when I get on a jet.""

After many readers expressed their indignation, Milley wrote a follow-up blog post on the SAS website, which took on a considerably more diplomatic tone. She defended SAS as software that can be valued for its ""support, reliability, and validation."" Recent history, however, has made it much more difficult to conflate proprietary software with reliability or functionality.

R certainly presents a powerful case study in how an open source language has rendered long-dominant proprietary software, such as SAS, largely irrelevant. Although it is difficult to quantify the size of R's user base, one interesting metric of popularity is its use in academic journal articles. In that court, R surpassed SAS in 2015. Additionally, although it is merely anecdotal, it is amusing to note a thread from 2017 on the Statistics subreddit, in which the original poster wonders why SAS is still around in substantial numbers. To paraphrase the prevailing response, companies still buy SAS because it's what they have always used in the past and change is hard! Or as Woodrow Wilson put it, ""If you want to make enemies, try to change something.""

In contrast, there are developers and data science professionals who don't want to make any concessions to functionality. They want the optimal tools for their analyses, even if it means having to dig through Stack Overflow every now and then. For them, there is R. It started as a statistical computing environment, but it's had so many additions that it can now be classified as a general-purpose language.
What about Python?

This begs the question: ""What about Python?"" Indeed, Python is also a popular open-source language used for data analytics. And if we have Python, why should we care about R? This can no longer be answered by appealing to functionality; Python and R have been copying each other's functionalities for years. For example, the R graphics library ggplot2 has been ported to Python; there are implementations of Jupyter notebooks with support for R; and the DataFrame class in Python's pandas library has an uncanny conceptual similarity to the data.frame class in base R. Accordingly, it is now far less common for a data scientist to make the choice between R and Python on account of differing functionality. There are exceptions to this rule, such as (in Python's favor) the full-stack capabilities of Python and (in R's favor) Shiny, an API to HTML and JavaScript that is implemented as an R library, allowing for seamless integration between web app development and R's capabilities.

Instead, the ""What about Python?"" question is best answered by clarifying the contrasting design philosophies between R and Python, then choosing which one most closely aligns with your personal style. The largest conceptual difference between the two languages is Python's preference of having only one obvious way to do something (a rule in the Python Philosophy), versus R's belief in providing limitless possibilities to programmers and allowing them to choose the approach they desire. There is certainly no analogue in the R community to the use of the word ""Pythonic"" in the Python community. R believes in giving choice to programmers rather than advocating regimented approaches. While this is certainly an issue of personal taste, I think it makes R more closely aligned than Python to the values upheld by the open source community.
Three reasons to choose R

At the end of the day, programmers should choose the language they feel is most comfortable, provided its utility meets their needs. I like that R syntax is very close to the way I think, which makes it very comfortable for me to use. Consider these three simple, but illustrative, examples.

    R indexes from 1, rather than the usual 0. I have been surprised by the severity of reactions to this; one of my colleagues even prefers Python over R for this very reason. But the point of a programming language is to be a middleman between our thoughts and 1s and 0s. If a language is a more effective ""middleman"" (for example, counting from 1, the same way we do), then what's wrong with that? I'm generally a fan of following convention, except when there's a good enough reason not to.

    One added benefit of R's approach to indexing is that you can remove elements from a vector by subsetting with negative indices (which requires the language to index from something greater than zero). For example: 

    > x = 1:5
    > print(x)
    [1]
    1 2 3 4 5
    > x = x[-3]
    > print(x)
    [1]
    1 2 4 5

     
    Base R has four different assignment operators, each with a different ranking in the order of operations. The following four statements all produce the same effect:

    assign('x', sqrt(pi))
    x = sqrt(pi)
    x <- sqrt(pi)
    sqrt(pi) -> x

    The third operator above (called ""leftward assignment"") is the most common, and I would not be surprised if most R programmers (out of habit) use it exclusively. I find it useful to have all of these available, as I think certain options are better suited to expressing how I form certain thoughts. Also, optional arguments to the first one, the assign() function, can explicitly specify in which environment/namespace to store the new variable. Moreover, R has the super-assignment operators <<- and ->> (which parallel leftward and rightward assignment, respectively) that allow a variable to be stored globally, even deep within nested functions or structures. (This can also be accomplished through the assign() function.)

    I think R beats every other language when it comes to ease of implementing list comprehension, even though this is typically touted as a Python selling point. One of several list comprehension methods in R is the ""apply"" family of functions, which provide a feature-rich way to apply functions across vectors or lists (i.e., R's equivalent of C structs). There is also a simpler approach based on R's convention of ""recycling"" which dictates that even when a function is declared to have only one element of input, an entire vector can be passed to the function anyway, and the function will be evaluated at each of the vector's elements. For example, the factorial() function is defined to take only one element of input, but you can nonetheless use it as: 

    > factorial(1:9)
    [1]
    1      2      6     24    120    720   5040  40320 362880


    Although the ""apply"" functions were originally considered a nuance in R, they inadvertently encouraged R programmers to set up their computations in embarrassingly parallel ways. Consequently, the R community naturally developed libraries for parallel and GPU computing.

In these and many other ways, R's embrace of the open source philosophy has made it a niche but growing language whose capabilities rival those of any other high-level interpreted language."
Python$Pipenv,"
Why Python devs should use Pipenv
Only a year old, Pipenv has become the official Python-recommended resource for managing package dependencies.
Pipenv, the ""Python Development Workflow for Humans"" created by Kenneth Reitz a little more than a year ago, has become the official Python-recommended resource for managing package dependencies. But there is still confusion about what problems it solves and how it's more useful than the standard workflow using pip and a requirements.txt file. In this month's Python column, we'll fill in the gaps.
A brief history of Python package installation

To understand the problems that Pipenv solves, it's useful to show how Python package management has evolved.

Take yourself back to the first Python iteration. We had Python, but there was no clean way to install packages.

Then came Easy Install, a package that installs other Python packages with relative ease. But it came with a catch: it wasn't easy to uninstall packages that were no longer needed.

Enter pip, which most Python users are familiar with. pip lets us install and uninstall packages. We could specify versions, run pip freeze > requirements.txt to output a list of installed packages to a text file, and use that same text file to install everything an app needed with pip install -r requirements.txt.

But pip didn't include a way to isolate packages from each other. We might work on apps that use different versions of the same libraries, so we needed a way to enable that. Along came virtual environments, which enabled us to create small, isolated environments for each app we worked on. We've seen many tools for managing virtual environments: virtualenv, venv, virtualenvwrapper, pyenv, pyenv-virtualenv, pyenv-virtualenvwrapper, and even more. They all play well with pip and requirements.txt files.
The new kid: Pipenv

Pipenv aims to solve several problems.

First, the problem of needing the pip library for package installation, plus a library for creating a virtual environment, plus a library for managing virtual environments, plus all the commands associated with those libraries. That's a lot to manage. Pipenv ships with package management and virtual environment support, so you can use one tool to install, uninstall, track, and document your dependencies and to create, use, and organize your virtual environments. When you start a project with it, Pipenv will automatically create a virtual environment for that project if you aren't already using one.

Pipenv accomplishes this dependency management by abandoning the requirements.txt norm and trading it for a new document called a Pipfile. When you install a library with Pipenv, a Pipfile for your project is automatically updated with the details of that installation, including version information and possibly the Git repository location, file path, and other information.

Second, Pipenv wants to make it easier to manage complex interdependencies. Your app might depend on a specific version of a library, and that library might depend on a specific version of another library, and it's just dependencies and turtles all the way down. When two libraries your app uses have conflicting dependencies, your life can become hard. Pipenv wants to ease that pain by keeping track of a tree of your app's interdependencies in a file called Pipfile.lock. Pipfile.lock also verifies that the right versions of dependencies are used in production.

Also, Pipenv is handy when multiple developers are working on a project. With a pip workflow, Casey might install a library and spend two days implementing a new feature using that library. When Casey commits the changes, they might forget to run pip freeze to update the requirements file. The next day, Jamie pulls down Casey's changes, and suddenly tests are failing. It takes time to realize that the problem is libraries missing from the requirements file that Jamie doesn't have installed in the virtual environment.

Because Pipenv auto-documents dependencies as you install them, if Jamie and Casey had been using Pipenv, the Pipfile would have been automatically updated and included in Casey's commit. Jamie and Casey would have saved time and shipped their product faster.

Finally, using Pipenv signals to other people who work on your project that it ships with a standardized way to install project dependencies and development and testing requirements. Using a workflow with pip and requirements files means that you may have one single requirements.txt file, or several requirements files for different environments. It might not be clear to your colleagues whether they should run dev.txt or local.txt when they're running the project on their laptops, for example. It can also create confusion when two similar requirements files get wildly out of sync with each other: Is local.txt out of date, or is it really supposed to be that different from dev.txt? Multiple requirements files require more context and documentation to enable others to install the dependencies properly and as expected. This workflow has the potential to confuse colleagues and increase your maintenance burden.

Using Pipenv, which gives you Pipfile, lets you avoid these problems by managing dependencies for different environments for you. This command will install the main project dependencies:

pipenv install

Adding the --dev tag will install the dev/testing requirements:

pipenv install --dev

There are other benefits to using Pipenv: It has better security features, graphs your dependencies in an easier-to-understand format, seamlessly handles .env files, and can automatically handle differing dependencies for development versus production environments in one file. You can read more in the documentation.
Pipenv in action

The basics of using Pipenv are detailed in the Managing Application Dependencies section of the official Python packaging tutorial. To install Pipenv, use pip:

pip install pipenv

To install packages to use in your project, change into the directory for your project. Then to install a package (we'll use Django as an example), run:

pipenv install django

You will see some output that indicates that Pipenv is creating a Pipfile for your project.

If you aren't already using a virtual environment, you will also see some output from Pipenv saying it is creating a virtual environment for you.

Then, you will see the output you are used to seeing when you install packages.

To generate a Pipfile.lock file, run:

pipenv lock

You can also run Python scripts with Pipenv. To run a top-level Python script called hello.py, run:

pipenv run python hello.py

And you will see your expected result in the console.

To start a shell, run:

pipenv shell

If you would like to convert a project that currently uses a requirements.txt file to use Pipenv, install Pipenv and run:

pipenv install requirements.txt

This will create a Pipfile and install the specified requirements. Consider your project upgraded!
"
Python$Django$WebDevelopment,"10 tips for making the Django Admin more secure
Don't take chances with app security. Here's how to protect your users.
Offloading the responsibility for making your app secure onto QA testers or an information security office is tempting, but security is everyone's responsibility. The Django Admin is one of our favorite features of Django, but unless it's locked down correctly, it presents opportunities for exploitation. To save your users from compromised data, here are 10 tips to make the Django Admin more secure.
1. Use SSL

Deploy your site behind HTTPS. If you aren?t using HTTPS, it?s possible for someone to snoop your (or your users') password while you are at a coffee shop, an airport, or in another public place. Read more about enabling SSL and extra steps you might need to take in the Django docs).
2. Change the URL

Change the default admin URL from /admin/ to something else. Instructions are in the Django documentation, but in short, replace admin/ in your URL conf to something else:

urlpatterns = [
    path('my-special-admin-login/', admin.site.urls),
]

For even more security, host the admin on a different domain entirely. If you need even more security, serve the admin behind a VPN or someplace that isn't public.
3. Use 'django-admin-honeypot'
Once you have moved your admin site to a new URL (or even decided to host it on its own domain), install the library django-admin-honeypot on your old /admin/ URL to capture attempts to hack your site. django-admin-honeypot generates a fake admin login screen and will email your site administrators whenever someone tries to log in to your old /admin/ URL.

The email generated by django-admin-honeypot will contain the attacker's IP address, so for added security if you notice repeated login attempts from the same IP address, you can block that address from using your site.
4. Require stronger passwords

Most of your users will choose poor passwords. Enabling password validation can ensure that your users select stronger passwords, which will in turn increase the security of their data and the data they have access to in the admin. Require strong passwords by enabling password validation. The Django documentation has a great introduction on how to enable the password validators that ship with Django. Check out third-party password validators like django-zxcvbn-password to make your users' passwords even more secure. Scot Hacker has a great post on what makes a strong password and implementing the python-zxcvbn library in a Python project.
5. Use two-factor authentication

Two-factor authentication (2FA) is when you require a password plus something else to authenticate a user for your site. You are probably familiar with apps that require a password and then text you a second login code before they allow you to log in; those apps are using 2FA.

There are three ways you can enable 2FA on your site:

    2FA with SMS, where you text a login code. This is better than requiring only a password, but SMS messages are surprisingly easy to intercept.
    2FA with an app like Google Authenticator, which generates unique login codes for any service you register to it. To set up these apps, users will need to scan a QR code on your site to register your site with their app. Then the app will generate the login code that they can use to log in to your site.
    2FA with YubiKey is the safest way to enable 2FA on your site. This method requires that your users have a physical device, a YubiKey, that they plug into a USB port when they try to log in.

The library django-two-factor-auth can help you enable any of the above 2FA methods.
6. Use the latest version of Django

Always use the latest Django minor version to keep up with security updates and bugfixes. As of this writing, that is Django 2.0.1. Upgrade to the newest long-term release (LTS) as soon as is feasible for you, but definitely make sure your project is upgraded before it falls out of support (see supported versions on the Download) page.
7. Never run `DEBUG` in production

When DEBUG is set to True in your settings file, errors will display with full tracebacks that are likely to contain information you don't want end users to see. You might also have other settings or methods that are only enabled when in Debug mode that could pose a risk to your users and their data.

To avoid this, use different settings files for local development and for production deployment. Check out Vitor Freitas's great introduction to using multiple settings files.
8. Remember your environment

The admin should explicitly state which environment you are in to keep users from accidentally deleting production data. You can accomplish this easily using the django-admin-env-notice library, which will place a color-coded banner at the top of your Admin site.
9. Check for errors

This isn't specific to the Django Admin, but it's still a great practice to secure your app. Find security errors using python manage.py check --deploy. If you run this command when you're running your project locally, you will likely see some warnings that won't be relevant in production. For example, your DEBUG setting is probably True, but you're already using a separate settings file to take care of that for production, right?

The output for this command will look something like this:

?: (security.W002) You do not have 'django.middleware.clickjacking.XFrameOptionsMiddleware' in your MIDDLEWARE, so your pages will not be served with an 'x-frame-options' header. Unless there is a good reason for your site to be served in a frame, you should consider enabling this header to help prevent clickjacking attacks.
?: (security.W012) SESSION_COOKIE_SECURE is not set to True. Using a secure-only session cookie makes it more difficult for network traffic sniffers to hijack user sessions.
?: (security.W016) You have 'django.middleware.csrf.CsrfViewMiddleware' in your MIDDLEWARE, but you have not set CSRF_COOKIE_SECURE to True. Using a secure-only CSRF cookie makes it more difficult for network traffic sniffers to steal the CSRF token.

System check identified 3 issues (0 silenced).

Notice that each warning contains an explanation of what your risk is and what you should change. More information about this check is in the Django documentation.
10. Get a checkup

This is another tip that isn't specific to the admin, but is still good practice. Once deployed to a staging site, run your website through Sasha's Pony Checkup. This site will give you a security score and a tidy list of things to do to improve that score. It will test your site for some of the things we've listed above, and also recommend other ways to protect your site from specific vulnerabilities and types of attacks."
Python$Pygame$JavaScript,"Why Python and Pygame are a great pair for beginning programmers
We look at three reasons Pygame is a good choice for learning to program.
Last month, Scott Nesbitt wrote about Mozilla awarding $500K to support open source projects. Phaser, a HTML/JavaScript game platform, was awarded $50,000. I?ve been teaching Phaser to my pre-teen daughter for a year, and it's one of the best and easiest HTML game development platforms to learn. Pygame, however, may be a better choice for beginners. Here's why.
1. One long block of code

Pygame is based on Python, the most popular language for introductory computer courses. Python is great for writing out ideas in one long block of code. Kids start off with a single file and with a single block of code. Before they can get to functions or classes, they start with code that will soon resemble spaghetti. It?s like finger-painting, as they throw thoughts onto the page.

This approach to learning works. Kids will naturally start to break things into functions and classes as their code gets more difficult to manage. By learning the syntax of a language like Python prior to learning about functions, the student will gain basic programming knowledge before using global and local scope.

Most HTML games separate the structure, style, and programming logic into HTML, CSS, and JavaScript to some degree and require knowledge of CSS and HTML. While the separation is better in the long term, it can be a barrier for beginners. Once kids realize that they can quickly build web pages with HTML and CSS, they may get distracted by the visual excitement of colors, fonts, and graphics. Even those who stay focused on JavaScript coding will still need to learn the basic document structure that the JavaScript code sits in.
2. Global variables are more obvious

Both Python and JavaScript use dynamically typed variables, meaning that a variable becomes a string, an integer, or float when it?s assigned; however, making mistakes is easier in JavaScript. Similar to typed variables, both JavaScript and Python have global and local variable scopes. In Python, global variables inside of a function are identified with the global keyword.

Let?s look at the basic Making your first Phaser game tutorial, by Alvin Ourrad and Richard Davey, to understand the challenge of using Phaser to teach programming to beginners. In JavaScript, global variables?variables that can be accessed anywhere in the program?are difficult to keep track of and often are the source of bugs that are challenging to solve. Richard and Alvin are expert programmers and use global variables intentionally to keep things concise.

var game = new Phaser.Game(800, 600, Phaser.AUTO, '', { preload: preload, create: create, update: update });

function preload() {

    game.load.image('sky', 'assets/sky.png');

}

var player;
var platforms;

function create() {
    game.physics.startSystem(Phaser.Physics.ARCADE);
?

In their Phaser programming book Interphase, Richard Davey and Ilija Melentijevic explain that global variables are commonly used in many Phaser projects because they make it easier to get things done quickly.

    ?If you?ve ever worked on a game of any significant size then this approach is probably already making you cringe slightly... So why do we do it? The reason is simply because it?s the most concise and least complicated way to demonstrate what Phaser can do.?

Although structuring a Phaser application to use local variables and split things up nicely into separation of concerns is possible, that?s tough for kids to understand when they?re first learning to program.

If you?re set on teaching your kids to code with JavaScript, or if they already know how to code in another language like Python, a good Phaser course is The Complete Mobile Game Development Course, by Pablo Farias Navarro. Although the title focuses on mobile games, the actual course focuses on JavaScript and Phaser. The JavaScript and Phaser apps are moved to a mobile phone with PhoneGap.
3. Pygame comes with less assembly required

Thanks to Python Wheels, Pygame is now super easy to install. You can also install it on Fedora/Red Hat with the yum package manager:

sudo yum install python3-pygame

See the official Pygame installation documentation for more information.

Although Phaser itself is even easier to install, it does require more knowledge to use. As mentioned previously, the student will need to assemble their JavaScript code within an HTML document with some CSS. In addition to the three languages?HTML, CSS, and JavaScript?Phaser also requires the use of Firefox or Chrome development tools and an editor. The most common editors for JavaScript are Sublime, Atom, VS Code (probably in that order).

Phaser applications will not run if you open the HTML file in a browser directly, due to same-origin policy. You must run a web server and access the files by connecting to the web server. Fortunately, you don?t need to run Apache on your local computer; you can run something lightweight like httpster for most projects.
Advantages of Phaser and JavaScript

With all the challenges of JavaScript and Phaser, why am I teaching them? Honestly, I held off for a long time. I worried about students learning variable hoisting and scope. I developed my own curriculum based on Pygame and Python, then I developed one based on Phaser. Eventually, I decided to use Pablo?s pre-made curriculum as a starting point. 

There are really two reasons that I moved to JavaScript. First, JavaScript has emerged as a serious language used in serious applications. In addition to web applications, it?s used for mobile and server applications. JavaScript is everywhere, and it?s used widely in applications kids see every day. If their friends code in JavaScript, they'll likely want to as well. As I saw the momentum behind JavaScript, I looked into alternatives that could compile into JavaScript, primarily Dart and TypeScript. I didn?t mind the extra conversion step, but I still looked at JavaScript.

In the end, I chose to use Phaser and JavaScript because I realized that the problems could be solved with JavaScript and a bit of work. High-quality debugging tools and the work of some exceptionally smart people have made JavaScript a language that is both accessible and useful for teaching kids to code.
Final word: Python vs. JavaScript

When people ask me what language to start their kids with, I immediately suggest Python and Pygame. There are tons of great curriculum options, many of which are free. I used ""Making Games with Python & Pygame"" by Al Sweigart with my son. I also used Think Python: How to Think Like a Computer Scientist by Allen B. Downey. You can get Pygame on your Android phone with RAPT Pygame by Tom Rothamel."
Python$MachineLearning$TensowFlow,"Resources for getting started with Python and machine learning
Advance from building a first program to building a neural network.
Are you interested in machine learning and want to learn how to program? That's why I started learning to code. In this article, I'll share a few of the best resources that helped me advance from building my first program to building my first neural network.
Picking up Python

Python is one of the most highly recommended programming languages for beginners learning to code. Python helped me understand programming concepts clearly and I like to use multiple resources to reinforce the fundamentals. Also, Python is a great choice because it powers machine learning libraries such as TensorFlow and Keras.

Here are the resources that helped me get started learning to code in Python (listed in chronological order):

    Learn to Program: The Fundamentals is an online course from Coursera. This was my first introduction to programming and Python. The course provides a thorough overview of programming concepts and is well-paced by gradually introducing new concepts and building on the foundations of Python.
    Automate the Boring Stuff with Python is a book supplemented by YouTube tutorials. Automate the Boring Stuff with Python is a fun, helpful read. Learn to write helpful Pythonic scripts as you learn the concepts and syntax.

    Think Python, 2nd edition is a book that builds on core concepts in more detail and introduces advanced features of Python without being overwhelming. Have a go at completing a few of the exercises and see what you pick up.

(I also wanted to thank the instructors and authors for making these resources freely available!)

Learning machine learning

Within computer science is the field of Artificial Intelligence, and machine learning is a sub-field of AI. Machine learning is all about computers that learn tasks from experience (i.e., from lots of data) instead of being programmed like conventional software. Deep Learning is a technique using neural networks for machine learning. Here are my top three resources to get started with machine  learning and deep learning for beginner programmers (all except the last resource on the list are available free to access):

    Machine Learning is Fun! is a series of articles introducing machine learning. The series provides a high-level overview, covering topics such as different types of neural networks, how they work, and what they're used for.
    Machine Learning Recipes is a YouTube series from Google developers. Short videos take viewers through setting up TensorFlow, using scikit-learn and TFLearn, the machine learning pipeline, and training a neural network.

    Grokking Deep Learning is a book that introduces deep learning. The chapters are released every few months, with the entire release scheduled for 2017. It helped me understand how neural networks work and to build a simple neural network from scratch in Python.

I also recommend an article by Rachel Thomas, a data scientist and co-founder of fast.ai. Providing a Good Education in Deep Learning emphasizes how inclusiveness should be a key responsibility in education pertaining to transformative technologies such as AI.
Additional thoughts

You can always search online to resolve errors or get answers to your questions. The Stack Overflow community, for example, is a good starting point because someone probably had the same problem and you'll find solutions to try. Python Tutor is an excellent tool for seeing what code does line by line.

I'm still on the learning path, too, but I've realized that two of the most important factors leading to success in programming?or learning anything?is time and the willingness to work on problems that are beyond your current skill level.

I started learning to program two years ago because I wanted to learn how to use machine learning and deep learning. Ideally, it would be great to have a programming resource that taught Python and machine learning concurrently, but I haven't found one yet. In the meantime, I hope the resources are useful for you in getting started with programming and machine learning."
Python$Scribus$Replacetext,"Python scripts to automatically replace text in Scribus
Need to swap out text in Scribus? These scripts may help.


In my last article, I described Autoquote, a script that converts typewriter (or ""straight"") quotes to typographic (or ""curly"") quotes, which was prompted by a question on the Scribus open source desktop publishing software's mail list. Most publications adhere to certain style conventions, including the type of quotation marks they use, and a script that automatically corrects deviations from house style is a big time-saver.

At the heart of the script was a method for parsing the contents of a text frame character-by-character, which works like this: First, various kinds of control characters, e.g., for carriage returns and those that denote styles, are left intact. Second, there is a progressive march along the text that keeps track of the current character being analyzed and also the preceding and following characters to determine whether a typewriter quote should be replaced with a left or right quotation mark.

After I created this Autoquote script, someone asked me if I could write one that scrambles the text in a document. The person wanted to post a Scribus document to show a layout, yet hide the text contents of the frames. The idea was not to encrypt the document, but simply turn the text into gibberish. It seemed to me that the basic parsing section from Autoquote would serve this purpose well.
replacetext.py

I called the end result replacetext.py, and I eventually ended up with four different versions, as you can see on the wiki page. The original one operated only on a selected text frame, but then there came a version that converted all text frames on regular pages, another for all text frames including those on master pages, and another version that worked only on the current page of the document.

I chose to do this scrambling as follows:

    alpha = random.randint(1,26)
    letter = chr(alpha + 96)
    LETTER = chr(alpha + 64)
    if ((ord(char)>96)and(ord(char)<123)):
        scribus.deleteText(textbox)
        scribus.insertText(letter, c, textbox)
    if ((ord(char)>64)and(ord(char)<91)):
        scribus.deleteText(textbox)
        scribus.insertText(LETTER, c, textbox)

For each newly parsed character, a random integer between 1 and 26 is generated. This random integer simultaneously creates a random lower case and upper case letter. Then the script tests the original text character to determine whether it's a lower or upper case letter so it can make the appropriate substitution (i.e., to preserve the original character's case). Only a-z and A-Z characters are affected, not numbers and not those outside of ASCII territory (although it wouldn't be hard to extend this).

Because of the randomization function, there is no way to reverse the process, but I still wanted to retain the rough appearance of text with capitalizations and word spacing intact. In practical use, one side effect is that text tends to take up more space, which I presume relates to an increase in the number of wider glyphs than is usual in English. In practice, a user could delete characters when necessary for the layout appearance.
en+emdash.py

Many publications specify the use of en (?) and em (?) dashes in their style guide. These are different from hyphens (-), but are sometimes denoted by two or three hyphens typed together. Many Scribus users compose their text in a text editor outside of Scribus, then import it into a text frame in the Scribus document. Like with typographic quotes, a utility to automatically convert hyphens to en and em dashes would be useful.

You can find the en+emdash.py script on its wiki page. Here is the pertinent assignment strategy:

    if (char == '-'):

      if (prevchar == '-'):
        if (nextchar == '-'):
          scribus.selectText(c-1, 3, textbox)
          scribus.deleteText(textbox)
          scribus.insertText(mdash, c-1, textbox)
          char = mdash
        else:
          scribus.selectText(c-1, 2, textbox)
          scribus.deleteText(textbox)
          scribus.insertText(ndash, c-1, textbox)
          char = ndash

In this case, the variables mdash and ndash have been previously assigned the appropriate Unicode characters. If the en+emdash.py script encounters a hyphen, it checks to see if a previous character was also a hyphen. If that is true, then it checks the following character, and if this is also a hyphen (i.e., ---), it assigns an em dash, but if not (i.e., --), it assigns an en dash. Single hyphens are left as single hyphens.

This isn't a powerful or frequently used script, but it functions as a simple utility to accomplish one task, much like a number of Unix/Linux command line functions.

This also shows that, once you have taken the time to work through the logic of a complex basic operation like text parsing, you can go on to adapt it to a variety of uses."
Python$BeeWare,"Cross-platform development with Python and BeeWare
Learn how the BeeWare suite of libraries and bridges will help Python developers deploy code across platforms.
If you want to develop for Android, you have to use Java. If you want to develop for iOS, you have to use Objective C. And if you want to develop for the web, you have to use JavaScript. Right?

These may be the preferred languages for these platforms, but at the end of the day, mobile phones and web browsers are computing platforms, and with a little work, you can use any language you want. With the BeeWare suite of libraries and bridges, you can use just Python. And, you can use the same code to deploy on all these platforms.

This article offers a preview of our upcoming PyCon US 2017 talk, Snek in the Browser, which is a deep dive into how the BeeWare project tackles using Python for front-end development using Batavia and Toga.
Why Python?

Because we don't have to use HTML and JavaScript, we can start looking at the browser as a platform for which we can deliver applications, rather than a specific set of technologies you have to write code to suit. Once we've adopted that mindset, it frees us up to look at all sorts of new ways of constructing web apps.

Python is one of the easier languages to pick up, and it doesn't have the learning curve of Java or Objective C. And recently, many scientific communities, including astronomy and data science, have picked up Python as their go-to language. To be able to use a language they already know to create their own applications, such as mobile-based data recording systems, without having to learn entire new languages would be a great asset to all these scientists.
How does it work?

The only language that works natively in the browser is JavaScript. Sure, there are workarounds with Flash, Silverlight, etc., but they are prone to security and development issues alike. Batavia, however, uses JavaScript to run Python. How? By implementing the Python virtual machine in JavaScript. Python itself is just a language specification. CPython, on the other hand, is the Python implementation most people use, with PyPy being another. CPython once compiled generates those .pyc files that you might see in your filesystem. These are bytecode representation of a combination of around 100 different base-level operations. Implement a way for JavaScript to understand how these operations work, and you have an interpreter.

Given this, if we create a JavaScript-based application that can take Python bytecode and return the same results as the CPython implementation, we can run Python in the browser. In fact, you can do this in less than 500 lines of code, as Allison Kaptur explains in her article ""500 Lines or Less, A Python Interpreter Written in Python"".

Emerging technologies such as ASM.js and WebAssembly make the prospect of in-browser language interpreters even more promising, given the performance improvements these projects provide.
Native rendering

To be able to render websites with the same code as for a mobile deployment, we need a universal wrapper that allows us to target these platforms. This is where Toga, an OS-native Graphical User Interface (GUI) toolkit, comes in. Toga abstracts out the interface options of different systems. If you want to create a Quit button, it will be in a different place on macOS than on Windows or Linux. It's not just a wrapper around native system calls?it's abstraction over the native layer, capturing the high-level user interaction use cases.

At the end of the day, a web page and a mobile screen are just bitmap devices rendering font, shapes, and lines on a screen. HTML has brought in a new way of thinking about the building blocks of graphic user interfaces, but by consolidating how we think about putting stuff on a screen, abstracting this to all platforms is made simpler."
Python$Ruby$Webdevelopment,"
Python vs. Ruby: Which is best for web development?
These two are similar in some ways but are worlds apart in their approach to solving problems.
Python and Ruby are among some of the most popular programming languages for developing websites, web-based apps, and web services.

In many ways, the two languages have a lot in common. Visually they are quite similar, and both provide programmers with high-level, object-oriented coding, an interactive shell, standard libraries, and persistence support. However, Python and Ruby are worlds apart in their approach to solving problems because their syntax and philosophies vary greatly, primarily because of their respective histories.

Which one to implement for web development requires some thought because all languages have strengths and weaknesses and your decision will have consequences.

The basics

Python was developed organically in the scientific space as a prototyping language that easily could be translated into C++ if a prototype worked. This happened long before it was first used for web development. Ruby, on the other hand, became a major player specifically because of web development; the Rails framework extended Ruby's popularity with people developing complex websites.

Which programming language best suits your needs? Here is a quick overview of each language to help you choose:
Approach: one best way vs. human-language
Python

Python takes a direct approach to programming. Its main goal is to make everything obvious to the programmer. In Python, there is only one ""best"" way to do something. This philosophy has led to a language strict in layout.

Python's core philosophy consists of three key hierarchical principles:

    Explicit is better than implicit
    Simple is better than complex
    Complex is better than complicated

This regimented philosophy results in Python being eminently readable and easy to learn?and why Python is great for beginning coders. Python has a big foothold in introductory programming courses. Its syntax is very simple, with little to remember. Because its code structure is explicit, the developer can easily tell where everything comes from, making it relatively easy to debug.

Python's hierarchy of principles is evident in many aspects of the language. Its use of whitespace to do flow control as a core part of the language syntax differs from most other languages, including Ruby. The way you indent code determines the meaning of its action. This use of whitespace is a prime example of Python's ""explicit"" philosophy, the shape a Python app takes spells out its logic and how the app will act.
Ruby

In contrast to Python, Ruby focuses on ""human-language"" programming, and its code reads like a verbal language rather than a machine-based one, which many programmers, both beginners and experts, like. Ruby follows the principle of ""least astonishment,"" and offers myriad ways to do the same thing. These similar methods can have multiple names, which many developers find confusing and frustrating.

Unlike Python, Ruby makes use of ""blocks,"" a first-class object that is treated as a unit within a program. In fact, Ruby takes the concept of OOP (Object-Oriented Programming) to its limit. Everything is an object?even global variables are actually represented within the ObjectSpace object. Classes and modules are themselves objects, and functions and operators are methods of objects. This ability makes Ruby especially powerful, especially when combined with its other primary strength: functional programming and the use of lambdas.

In addition to blocks and functional programming, Ruby provides programmers with many other features, including fragmentation, hashable and unhashable types, and mutable strings.

Ruby's fans find its elegance to be one of its top selling points. At the same time, Ruby's ""magical"" features and flexibility can make it very hard to track down bugs.
Communities: stability vs. innovation

Although features and coding philosophy are the primary drivers for choosing a given language, the strength of a developer community also plays an important role. Fortunately, both Python and Ruby boast strong communities.
Python

Python's community already includes a large Linux and academic community and therefore offers many academic use cases in both math and science. That support gives the community a stability and diversity that only grows as Python increasingly is used for web development.
Ruby

However, Ruby's community has focused primarily on web development from the get-go. It tends to innovate more quickly than the Python community, but this innovation also causes more things to break. In addition, while it has gotten more diverse, it has yet to reach the level of diversity that Python has.
Final thoughts

For web development, Ruby has Rails and Python has Django. Both are powerful frameworks, so when it comes to web development, you can't go wrong with either language. Your decision will ultimately come down to your level of experience and your philosophical preferences.

If you plan to focus on building web applications, Ruby is popular and flexible. There is a very strong community built upon it and they are always on the bleeding edge of development.

If you are interested in building web applications and would like to learn a language that's used more generally, try Python. You'll get a diverse community and lots of influence and support from the various industries in which it is used
"
Python$MachineLearning$Library$TensorFlow$Theano,"Top 3 machine learning libraries for Python
Learn about three of the most popular machine learning libraries for Python.
You don't have to be a data scientist to be fascinated by the world of machine learning, but a few travel guides might help you navigate the vast universe that also includes big data, artificial intelligence, and deep learning, along with a large dose of statistics and analytics. (""Deep learning"" and ""machine learning"" are often used interchangeably, so for a quick terminology primer that might help you understand the difference, read Nvidia's blog post, What's the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?)

In this article, I'll look at three of the most popular machine learning libraries for Python.

Theano

Released nearly a decade ago and primarily developed by a machine learning group at Universit? de Montr?al, Theano is one of the most-used CPU and GPU mathematical compilers in the machine learning community. A 2016 paper, Theano: A Python framework for fast computation of mathematical expressions, provides a thorough overview of the library. ""Several software packages have been developed to build on the strengths of Theano, with a higher-level user interface, more suitable for certain goals,"" the paper explains. ""Lasagne and Keras have been developed with the goal of making it easier to express the architecture of deep learning models and training algorithms as mathematical expressions to be evaluated by Theano. Another example is PyMC3, a probabilistic programming framework that uses Theano to derive expressions for gradients automatically, and to generate C code for fast execution."" (Keras and Lasagne run on top of both TensorFlow and Theano.)

Theano has more than 25,000 commits and almost 300 contributors on GitHub, and has been forked nearly 2,000 times.

TensorFlow

TensorFlow, an open source library for numerical computing using data flow graphs, is a newcomer to the world of open source, but this Google-led project already has almost 15,000 commits and more than 600 contributors on GitHub, and nearly 12,000 stars on its models repository.

In the first Open Source Yearbook, TensorFlow was picked as a project to fork in 2016. In the most recent Open Source Yearbook, TensorFlow made several appearances. We included the project on our list of top open source projects to watch in 2017. We also learned about TensorFlow-based project Magenta in an article by Josh Simmons, A tour of Google's 2016 open source releases. Simmons says Magenta is an effort to advance the state of the art in machine intelligence for music and art generation, and to build a collaborative community of artists, coders, and machine-learning researchers. Rachel Roumeliotis also refers to TensorFlow in a list of languages powering AI as part of her Hot programming trends of 2016 roundup.

TensorFlow 1.0 rolled out in mid-February. ""In just its first year, TensorFlow has helped researchers, engineers, artists, students, and many others make progress with everything from language translation to early detection of skin cancer and preventing blindness in diabetics,"" the Google Developers Blog announcement says.

scikit-learn

Built on NumPy, SciPy, and Matplotlib, scikit-learn (pronounced sy-kit learn) is used by Spotify engineers for music recommendations, at OkCupid to help evaluate and improve their matchmaking system, and during the exploration phase of new product development at Birchbox.

Scikit-learn has almost 22,000 commits and 800 contributors on GitHub."
Python$Design$Images,"Using Python to find corrupted images
If you're working with images on a computer, you're bound to eventually run into corrupted files that ruin your day. I run into this with animation renders (remember, the best practice here is to render to a sequence of image files and not a single video file). However, animation and visual effects are not the only places where you see image corruption. You can just as easily run into this in other fields. Perhaps you're a photographer and you've shot a bunch of brackets HDRI (High Dynamic Range Imaging) tone mapping and something glitches when transferring files from your camera.

The problem isn't so much the amount of effort to repair or replace a corrupted image, which is usually just a matter of re-rendering the image or re-copying the good image to your computer, rather the trick is to find those bad images as early in the process as possible. The longer you don't know, the greater the hassle you'll face when you do encounter a corrupt image.

So, what do you do? Well, you could go through and open each file?one at a time?in your image editor or viewer of choice, and let that program tell you there's a problem. However, photograph images are large and it can be annoying and time-consuming to go through a whole set just to find one or two baddies. And although animation renders are typically smaller files, you often have a lot more of them to go through. In my case, I regularly produce renders that have over 44,000 frames in a render. (No, that's not a typo?forty-four thousand frames.)

The solution? You guessed it. Write a script.

As with previous articles in this series, you'll do your scripting in Python. Step one: get a listing of your files. Fortunately, if you've gone through the last article in this series, you know that's a matter of using the os module. Assume that all of the image files you want to inspect are in a single directory on your hard drive. Furthermore, assume that you're going to run this script from within that directory. Using Python, you can get a list of those files with the following code:

import os
   
for filename in os.listdir('./'):
  print(filename)

If you'd like, you can narrow down that list of images (or at least more clearly specify it; for instance, you don't want to include this script as one of those files) by looking just for files that end with the PNG extension:

import os
   
for filename in os.listdir('./'):
  if filename.endswith('.png'):
    print(filename)

You now have a list of PNG image files in your current working directory. Now what? Well, now you need to figure out which, if any, of those images are corrupt. In the previous articles of this series, we exclusively used modules that ship with Python by default. Unfortunately, discovering if an image is corrupt without any image processing capability is difficult, and neither Python 2 nor Python 3 ship with any way to handle that out of the box. You'll need to get yourself an image processing module to view these files. Happily, the Python development community has made that easier for you.

In fact, you have an entire library of packages available to you to install. You just need to know how to get them. Let me introduce you to pip, the recommended tool for installing Python packages. It's installed by default on most platforms when you install Python.

Note: I'm using Python 3, but if you're using Python 2, nearly everything I've written in this series is transferable between both variations of the language. Also, many Linux distributions prefer that you use their own package management system over using pip to install Python packages. Feel free to stick to that if you prefer. The suggestion to use pip here is mostly in the interest of being consistent across all of the platforms you can use Python on.

The specific package that I'm going to recommend that you install is called Pillow. It's a ""friendly fork"" of the original PIL (Python Imaging Library) that works in current releases of both Python 3 and Python 2. All you need to install Pillow is to fire up a terminal window and type pip install Pillow. The Python package tool should handle the rest for you from there.

Once you have Pillow installed you need to actually have a way of using it in your script. Because it's installed, you can treat it just like any module that comes with Python. You use import?in this case, you could use import PIL. However, to look for corrupt images, you don't really need to import the entirety of the Pillow library into our script. In Python, you can import just a single subcomponent of a module. This is good practice because it reduces the memory footprint of your script and, just as importantly, it makes it more clear what things your script is going to do right from the start. Plus, when you import subcomponents, you end up needing to type less once you get into the meat of your script. Which is always a nice bonus.

To import a subcomponent of a module, you precede your import with a from directive. In the case of Pillow, your script really only needs to use the Image class. So, your import line would look like from PIL import Image. In fact, you can do the same thing with the os module. If you look back at the previous code, you might notice that you're only using the listdir function in the os module. So instead of import os, you could use from os import listdir. This means that when you get into your script, you no longer have to type os.listdir. Instead, you only need to type listdir, because that's all you've imported.

Pulling all that together, your script should now look something like this:

from os import listdir
from PIL import Image
   
for filename in listdir('./'):
  if filename.endswith('.png'):
    print(filename)

You've got the Image class in Pillow loaded, but your script still isn't doing anything with it yet. It's now time to get to the functional section of your script. What you're going to do is the scripted equivalent of opening each image file and checking to see if it's readable. If there's an error, then you've found a bad file. To do that, you're going to use a try/except block. In short, your script is going to try to run a function that opens a file. If that function returns an error, otherwise known as an exception, then you know that image has a problem. In particular, if the exception is of types IOError or SyntaxError, then you know you've got yourself a bad image.

The syntax for doing a try/except is pretty straightforward. I've described it in code comments below:

try: # These next functions may produce an exception
  # <some function>
except (IOError, SyntaxError) as e: # These are the exceptions we're looking for
  # <do something... like print an intelligent error message>

In the case of looking for corrupt image files, you'll want to test two functions: Image.open() and verify(). If you wrap those in a try/except block, your corrupt image-finding script should look like this:

from os import listdir
from PIL import Image
   
for filename in listdir('./'):
  if filename.endswith('.png'):
    try:
      img = Image.open('./'+filename) # open the image file
      img.verify() # verify that it is, in fact an image
    except (IOError, SyntaxError) as e:
      print('Bad file:', filename) # print out the names of corrupt files

And there you go. Save this script in your directory of images. When you run it from the command line, you should get a list of all the corrupt image files in there. If nothing prints out, then you can assume all of those image files are good, valid images.

Of course, being able to use this script on any arbitrary directory would be nice. And having the script prompt you to instruct it to go ahead and delete those corrupt files for you would be even nicer. Good news! You can make the script do exactly that. We'll cover that in the next articles in this series.

In the meantime, have fun rooting out corruption in your image folders."
Python$R$MachineLearning$DataAnalysis,"Python versus R for machine learning and data analysis


Machine learning and data analysis are two areas where open source has become almost the de facto license for innovative new tools. Both the Python and R languages have developed robust ecosystems of open source tools and libraries that help data scientists of any skill level more easily perform analytical work.

The distinction between machine learning and data analysis is a bit fluid, but the main idea is that machine learning prioritizes predictive accuracy over model interpretability, while data analysis emphasizes interpretability and statistical inference. Python, being more concerned with predictive accuracy, has developed a positive reputation in machine learning. R, as a language for statistical inference, has made its name in data analysis.

That isn't to pigeonhole either language into one category?Python can be used effectively as a data analysis tool, and R has enough flexibility to do some good work in machine learning. There is a multitude of packages for both languages that seek to replicate the functionality of the other. Python has libraries to boost its capacity for statistical inference and R has packages to improve its predictive accuracy.
Python's machine learning and data analysis packages

Even though Python is naturally disposed toward machine learning, it has packages that further optimize this attribute. PyBrain is a modular machine learning library that offers powerful algorithms for machine learning tasks. The algorithms are intuitive and flexible, but the library also has a variety of environments to test and compare your machine learning algorithms.

Scikit-learn is the most popular machine learning library for Python. Built on NumPy and SciPy, scikit-learn offers tools for data mining and analysis that bolster Python's already-superlative machine learning usability. NumPy and SciPy impress on their own. They are the core of data analysis in Python and any serious data analyst is likely using them raw, without higher-level packages on top, but scikit-learn pulls them together in a machine learning library with a lower barrier to entry.

When it comes to data analysis, Python receives a welcome boost from several different packages. Pandas, one of its most well-known data analysis packages, gives Python high-performance structures and data analysis tools. As is the case with many of Python's packages, it shortens the time between starting a project and doing meaningful work within that project. If you really want to stick with Python and get as much R functionality as you can, RPy2 offers all of R's major functionality. This gives you the best of R in Python natively.
R's machine learning and data analysis packages

R, like Python, has plenty of packages to boost its performance. When it comes to approaching parity with Python in machine learning, Nnet improves R by supplying the ability to easily model neural networks. Caret is another package that bolsters R's machine learning capabilities, in this case by offering a set of functions that increase the efficiency of predictive model creation.

But data analysis is R's domain, and there are packages to improve it beyond its already-stellar capabilities. Packages for the pre-modeling, modeling, and post-modeling stages of data analysis are available. These packages are directed at specific tasks like data visualization, continuous regression, and model validation. With all of these cross-functional libraries and packages, which language should you drag into the data battlefield with you?
Python for machine learning and data analysis

If you have some programming experience, Python might be the language for you. Python's syntax is more similar to other languages than R's syntax is. Python's readability is also nearly unmatched, as it reads much like a verbal language. This readability emphasizes development productivity, while R's non-standard code could lead to stutters in the programming process.

Python is well known as a flexible language, so if you plan to move on to projects in other fields when your machine learning or data analysis project is done, it might be a good idea to stick with Python so you aren't required to learn a new language.

Python's flexibility makes it a great choice for production use because, when the data analysis tasks need to be integrated with Web applications, for example, you can continue to use Python instead of integrating with another language. R is a great data analysis tool, but is fairly limited in terms of what it can accomplish beyond data analysis.

If you're completely new to programming, and therefore unfamiliar with ?standard? syntax, the learning curve for both languages is roughly the same. However, if the goal is to push past the basics of machine learning and data analysis, Python is probably a better choice. This is especially true considering the addition of scikit-learn to Python's arsenal of packages. The package is well maintained and actively in development. R might have a greater diversity of packages, but it also has more fragmentation and less consistency across those packages.
R for machine learning and data analysis

To date, R has primarily been used in academics and research. This is beginning to change, though, as R usage expands into the enterprise market. R was written by statisticians and it shows?basic data management tasks are very easy. Labeling data, filling missing values, and filtering are all simple and intuitive in R, which emphasizes user-friendly data analysis, statistics, and graphical models.

Since R was built as a statistical language, it has great statistical support overall. It represents the way statisticians think pretty well, so for anyone with a formal stats background it feels natural. Packages like statsmodels provide solid coverage for statistical models in Python, but the ecosystem of statistical model packages for R is much more robust. As far as beginner programmers are concerned, R makes exploratory work easier than Python because statistical models can be written with just a few lines of code.

R's closest answer to pandas is probably dplyr, but it is more limited than pandas. That might sound negative, but dplyr has the benefit of being more focused, which makes discovering how to perform a task much easier. Dplyr is also more readable than pandas.
Choosing your language

The main issue with R is its consistency. Algorithms are provided by third parties, which makes them comparatively inconsistent. The resulting decrease in development speed comes from having to learn new ways to model data and make predictions with each new algorithm you use. Every package requires a new understanding. Inconsistency is true of the documentation as well, as R's documentation is almost always incomplete.

However, if you find yourself in an academic setting and need a tool for data analysis, it's hard to argue with choosing R for the task. For professional use, Python makes more sense. Python is widely used throughout the industry and, while R is becoming more popular, Python is the language more likely to enable easy collaboration. Python's reach makes it easy to recommend not only as a general purpose and machine learning language, but with its substantial R-like packages, as a data analysis tool, as well.

If you don't already know R, learn Python and use RPy2 to access R's functionality. You'll be getting the power of two languages in one, and Python is production-ready because most companies have production systems ready for Python. This isn't true for R. Once you learn RPy2, the jump to pure R isn't very daunting, but moving in the opposite direction is considerably more difficult.

Both Python and R have great packages to maintain some kind of parity with the other, regardless of the problem you're trying to solve. There are so many distributions, modules, IDEs, and algorithms for each that you really can't go wrong with either. But if you're looking for a flexible, extensible, multi-purpose programming language that also excels in both machine learning and data analysis, Python is the clear choice."
MachineLearning$MatLab$Octave,"Machine Learning in MatLab/Octave
Recently I’ve created Machine Learning in Octave repository that contains MatLab/Octave examples of popular machine learning algorithms with code examples and mathematics behind them being explained.
The purpose of this repository was not to implement machine learning algorithms using 3rd party libraries or Octave/MatLab “one-liners” but rather to practice and to better understand the mathematics behind each algorithm. In most cases the explanations are based on this great machine learning course.
Supervised Learning
In supervised learning we have a set of training data as an input and a set of labels or “correct answers” for each training set as an output. Then we’re training our model (machine learning algorithm parameters) to map the input to the output correctly (to do correct prediction). The ultimate purpose is to find such model parameters that will successfully continue correct input→output mapping (predictions) even for new input examples.
Regression
In regression problems we do real value predictions. Basically we try to draw a line/plane/n-dimensional plane along the training examples.
Usage examples: stock price forecast, sales analysis, dependency of any number, etc.
Classification
In classification problems we split input examples by certain characteristic.
Usage examples: spam-filters, language detection, finding similar documents, handwritten letters recognition, etc.
Unsupervised Learning
Unsupervised learning is a branch of machine learning that learns from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data.
Clustering
In clustering problems we split the training examples by unknown characteristics. The algorithm itself decides what characteristic to use for splitting.
Usage examples: market segmentation, social networks analysis, organize computing clusters, astronomical data analysis, image compression, etc.
Anomaly Detection
Anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.
Usage examples: intrusion detection, fraud detection, system health monitoring, removing anomalous data from the dataset etc.
Neural Network (NN)
The neural network itself isn’t an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.
Usage examples: as a substitute of all other algorithms in general, image recognition, voice recognition, image processing (applying specific style), language translation, etc.
Demos
Each machine learning algorithm folder described above contains demo.m file that users might launch from Octave console or from MatLab. The demo scripts will output some useful information to the console and visualize the results of work of related algorithm.

"
JavaScript$web$CSS,"Web animation using CSS and JavaScript
Basics of animation
History
The first hints of 'animation' come from a pottery bowl in Iran, around 5000 years ago. Skip ahead to the 1500s, Leonardo Da Vinci had a few drawings depicting animations. And today, you might think of Walt Disney as the modern animation master.
Definition
To make sure we're on the same page: Animation is ""the technique of photographing successive drawings or positions of puppets or models to create an illusion of movement."" Our eyes cannot view individual objects moving faster than 60 frames per second, so web animation is a series of computer graphics shown in succession at a rate faster than 60 frames per second.
Principles
There are 12 basic principles in animations: Squash and Stretch, Anticipation, Staging, Straight Ahead Action and Pose-to-Pose, Follow Through and Overlapping Action, Slow In and Slow Out, Arc, Secondary Action, Timing, Exaggeration, Solid Drawing, and Appeal. Here's a bit more on four of them.
Anticipation: The action predicted from the before action is known as Anticipation. For example, hitting a pile of wood with an axe requires an action of taking the axe back of our head and then hitting on the wood with tremendous force. Taking the axe back of our head is considered as Anticipation. Another example can be, body movements of a pitcher in a baseball game, just before the pitcher actually throws the ball. Based on the pitcher's body movements it is anticipated that the ball will be thrown.
Secondary Action: The reaction of the action is known as secondary action. For example, when a ball touches the ground and kicks up the dust. In this case, ball touching the ground is a Primary action and dust is getting kicked up is Secondary action. Another example could be, spilling out of water while a person is diving in the swimming pool, is a Secondary action.
Staging: When we need to show or highlight only one animated objects from the group of all, we use the staging method. For example, if we are creating a modal popup window which is stagedto show in the middle of the browser screen and disabling all other controls in the background.
Slow-in Slow-out: In animation every object start slow and then reached its appropriate speed and then should slow down to finish the animation. Or it can be that it start fast but slow down at the end to finish. In CSS this is known as easing.
Using Flash
Animation on the web started in 1987 with the invention of the animated GIF, or Graphic Interface Format. GIFs were used mostly for advertisements on websites, but had some problems with the pixelation. Then, in the 1990s Adobe introduced Flash, a tool for animating with audio. This created a revolution and was the best way to do animation on websites for a very long time. But Flash has some issues.
Closed source: Users must purchase Flash from Adobe and cannot make modifications to the software.
Security: Flash allows writing and running complex scripts on websites and scripts can be written that directly access the memory of a computer.
Performance: Flash websites can take a long time to load.
Resource hog: Flash uses a high amount of computing resources and can actually hang or crash your system if multiple applications or flash sites are opened at the same time.
Plugin dependency: You need to have flash plugin installed in your browser. And every month or more, you need to update it.
Using CSS and JavaScript
To achieve animation on a website, we can use open source technologies like CSS and JavaScript.
With CSS, we use animation, transition, and transform properties as parameters. When using animation property, the keyframe property must be used to divide the animation into small pieces. Then we apply necessary transition or transform properties to each piece.
With JavaScript and jQuery, we use the animate() function and apply transformation and transition properties. Other functions are available in jQuery, which we can use directly: they are fadein(), fadeout(), slidedown(), slideup(), as well as the show() and hide() function for adding little delay, which will help to achieve animation.
A key to web animation is to hit the property which has less impact on the browser render capability.
Performance of the website depends completely on the technology we choose for implementation. While implementing robust logic, CSS animations will always be lighter than JavaScript animations. Most of the browsers use position, scale, rotation and opacity very lightly. If we are maintaining 60 fps (frames per second), it will not impact the performance as much. The higher we start on the timeline waterfall, the more work the browser has to do to get pixels onto the screen.
From a business perspective, we can use animation almost anywhere, but mostly it is used to promote the features of a product, like online tutorials and courses, online games, e-commerce websites, interactive portfolios, and much more.

"
JavaScript$Vert.x,"import { Router } from '@vertx/web';

// route all request based on the request path
const app = Router.router(vertx);

app.get('/greetings').handler(function (ctx) {
    // will invoke our existing drools engine here...
});

vertx
// create a HTTP server
.createHttpServer()
// on each request pass it to our APP
.requestHandler(function (req) {
    app.accept(req);
})
// listen on port 8080
.listen(8080);
The code is not complicated and should be self-explanatory, so let's focus on the integration with existing JVM code and libraries in the form of a Drools rule. Since Drools is a Java-based tool, we should build our application with a java build tool. Fortunately, because, behind the scenes, vertx-scripts delegates the JVM bits to Apache Maven, our work is easy.

mkdir -p src/main/java/drools
mkdir -p src/main/resources/drools
Next, we add the file src/main/resources/drools/rules.drl with the following content:

package drools

//list any import classes here.

//declare any global variables here

rule ""Greetings""
    when
        greetingsReferenceObject: Greeting( message == ""Hello World!"" )
    then
        greetingsReferenceObject.greet();
    end
Then we'll add the file src/main/java/drools/Greeting.java with the following content:

package drools;

public interface Greeting {

  String getMessage();

  void greet();
}
Finally, we'll add the helper utility class src/main/java/drools/DroolsHelper.java:

package drools;

import org.drools.compiler.compiler.*;
import org.drools.core.*;
import java.io.*;

public final class DroolsHelper {

  /**
   * Simple factory to create a Drools WorkingMemory from the given `drl` file.
   */
  public static WorkingMemory load(String drl) throws IOException, DroolsParserException {
    PackageBuilder packageBuilder = new PackageBuilder();
    packageBuilder.addPackageFromDrl(new StringReader(drl));
    RuleBase ruleBase = RuleBaseFactory.newRuleBase();
    ruleBase.addPackage(packageBuilder.getPackage());
    return ruleBase.newStatefulSession();
  }

  /**
   * Simple factory to create a Greeting objects.
   */
  public static Greeting createGreeting(String message, Runnable andThen) {
    return new Greeting() {
      @Override
      public String getMessage() {
        return message;
      }

      @Override
      public void greet() {
        andThen.run();
      }
    };
  }
}
We cannot use the file directly; we need to have drools. To do this, we add a custom property to our package.json named mvnDependencies (following the usual pattern):

{
    ""mvnDependencies"": {
        ""org.drools:drools-compiler"": ""6.0.1.Final""
    }
}
Of course, since we updated the project file, we should update npm:

npm install
We are now entering the final step of this project, where we mix Java and JavaScript. We had a placeholder before, so let's fill in the gaps. We first use the helper Java class to create an engine (you can now see the power of Vert.x, a truly polyglot runtime), then invoke our engine whenever an HTTP request arrives.

// get a reference from Java to the JavaScript runtime
const DroolsHelper = Java.type('drools.DroolsHelper');
// get a drools engine instance
const engine = DroolsHelper.load(vertx.fileSystem().readFileBlocking(""drools/rules.drl""));

app.get('/greetings').handler(function (ctx) {
  // create a greetings message
  var greeting = DroolsHelper.createGreeting('Hello World!', function () {
    // when a match happens you should see this message
    console.log('Greetings from Drools!');
  });

  // run the engine
  engine.insert(greeting);
  engine.fireAllRules();

  // complete the HTTP response
  ctx.response().end();
});
Conclusion

As this simple example shows, Vert.x allows you to be truly polyglot. The reason to choose Vert.x is not because it's another JavaScript runtime, rather it's a runtime that allows you to reuse what you already have and quickly build new code using the tools and language that run the internet. We didn't touch on performance here (as it is a topic on its own), but I encourage you to look at independent benchmarks such as TechEmpower to explore that topic.

"
Linux$containers,"Behind the scenes with Linux containers
Become a better container troubleshooter by using LXC to understand how they work.
Can you have Linux containers without Docker? Without OpenShift? Without Kubernetes?
Yes, you can. Years before Docker made containers a household term (if you live in a data center, that is), the LXC project developed the concept of running a kind of virtual operating system, sharing the same kernel, but contained within defined groups of processes.
Docker built on LXC, and today there are plenty of platforms that leverage the work of LXC both directly and indirectly. Most of these platforms make creating and maintaining containers sublimely simple, and for large deployments, it makes sense to use such specialized services. However, not everyone's managing a large deployment or has access to big services to learn about containerization. The good news is that you can create, use, and learn containers with nothing more than a PC running Linux and this article. This article will help you understand containers by looking at LXC, how it works, why it works, and how to troubleshoot when something goes wrong.
Sidestepping the simplicity
If you're looking for a quick-start guide to LXC, refer to the excellent Linux Containers website.
Installing LXC
If it's not already installed, you can install LXC with your package manager.
On Fedora or similar, enter:
$ sudo dnf install lxc lxc-templates lxc-doc
On Debian, Ubuntu, and similar, enter:
$ sudo apt install lxc
Creating a network bridge
Most containers assume a network will be available, and most container tools expect the user to be able to create virtual network devices. The most basic unit required for containers is the network bridge, which is more or less the software equivalent of a network switch. A network switch is a little like a smart Y-adapter used to split a headphone jack so two people can hear the same thing with separate headsets, except instead of an audio signal, a network switch bridges network data.
You can create your own software network bridge so your host computer and your container OS can both send and receive different network data over a single network device (either your Ethernet port or your wireless card). This is an important concept that often gets lost once you graduate from manually generating containers, because no matter the size of your deployment, it's highly unlikely you have a dedicated physical network card for each container you run. It's vital to understand that containers talk to virtual network devices, so you know where to start troubleshooting if a container loses its network connection.
To create a network bridge on your machine, you must have the appropriate permissions. For this article, use the sudo command to operate with root privileges. (However, LXC docs provide a configuration to grant users permission to do this without using sudo.)
$ sudo ip link add br0 type bridge
Verify that the imaginary network interface has been created:
$ sudo ip addr show br0
7: br0: <BROADCAST,MULTICAST> mtu 1500 qdisc
   noop state DOWN group default qlen 1000
   link/ether 26:fa:21:5f:cf:99 brd ff:ff:ff:ff:ff:ff
Since br0 is seen as a network interface, it requires its own IP address. Choose a valid local IP address that doesn't conflict with any existing IP address on your network and assign it to the br0 device:
$ sudo ip addr add 192.168.168.168 dev br0
And finally, ensure that br0 is up and running:
$ sudo ip link set br0 up
Setting the container config
The config file for an LXC container can be as complex as it needs to be to define a container's place in your network and the host system, but for this example the config is simple. Create a file in your favorite text editor and define a name for the container and the network's required settings:
lxc.utsname = opensourcedotcom
lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:bd
lxc.network.ipv4 = 192.168.168.1/24
lxc.network.ipv6 = 2003:db8:1:0:214:1234:fe0b:3596
Save this file in your home directory as mycontainer.conf.
The lxc.utsname is arbitrary. You can call your container whatever you like; it's the name you'll use when starting and stopping it.
The network type is set to veth, which is a kind of virtual Ethernet patch cable. The idea is that the veth connection goes from the container to the bridge device, which is defined by the lxc.network.link property, set to br0. The IP address for the container is in the same network as the bridge device but unique to avoid collisions.
With the exception of the veth network type and the up network flag, you invent all the values in the config file. The list of properties is available from man lxc.container.conf. (If it's missing on your system, check your package manager for separate LXC documentation packages.) There are several example config files in /usr/share/doc/lxc/examples, which you should review later.
Launching a container shell
At this point, you're two-thirds of the way to an operable container: you have the network infrastructure, and you've installed the imaginary network cards in an imaginary PC. All you need now is to install an operating system.
However, even at this stage, you can see LXC at work by launching a shell within a container space.
$ sudo lxc-execute --name basic \
--rcfile ~/mycontainer.conf /bin/bash \
--logfile mycontainer.log
#
In this very bare container, look at your network configuration. It should look familiar, yet unique, to you.
# /usr/sbin/ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state [...]
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
[...]
22: eth0@if23: <BROADCAST,MULTICAST,UP,LOWER_UP> [...] qlen 1000
link/ether 4a:49:43:49:79:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 192.168.168.167/24 brd 192.168.168.255 scope global eth0
   valid_lft forever preferred_lft forever
inet6 2003:db8:1:0:214:1234:fe0b:3596/64 scope global 
   valid_lft forever preferred_lft forever
[...]
Your container is aware of its fake network infrastructure and of a familiar-yet-unique kernel.
# uname -av
Linux opensourcedotcom 4.18.13-100.fc27.x86_64 #1 SMP Wed Oct 10 18:34:01 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
Use the exit command to leave the container:
# exit
Installing the container operating system
Building out a fully containerized environment is a lot more complex than the networking and config steps, so you can borrow a container template from LXC. If you don't have any templates, look for a separate LXC template package in your software repository.
The default LXC templates are available in /usr/share/lxc/templates.
$ ls -m /usr/share/lxc/templates/
lxc-alpine, lxc-altlinux, lxc-archlinux, lxc-busybox, lxc-centos, lxc-cirros, lxc-debian, lxc-download, lxc-fedora, lxc-gentoo, lxc-openmandriva, lxc-opensuse, lxc-oracle, lxc-plamo, lxc-slackware, lxc-sparclinux, lxc-sshd, lxc-ubuntu, lxc-ubuntu-cloud
Pick your favorite, then create the container. This example uses Slackware.
$ sudo lxc-create --name slackware --template slackware
Watching a template being executed is almost as educational as building one from scratch; it's very verbose, and you can see that lxc-create sets the ""root"" of the container to /var/lib/lxc/slackware/rootfs and several packages are being downloaded and installed to that directory.
Reading through the template files gives you an even better idea of what's involved: LXC sets up a minimal device tree, common spool files, a file systems table (fstab), init files, and so on. It also prevents some services that make no sense in a container (like udev for hardware detection) from starting. Since the templates cover a wide spectrum of typical Linux configurations, if you intend to design your own, it's wise to base your work on a template closest to what you want to set up; otherwise, you're sure to make errors of omission (if nothing else) that the LXC project has already stumbled over and accounted for.
Once you've installed the minimal operating system environment, you can start your container.
$ sudo lxc-start --name slackware \
--rcfile ~/mycontainer.conf
You have started the container, but you have not attached to it. (Unlike the previous basic example, you're not just running a shell this time, but a containerized operating system.) Attach to it by name.
$ sudo lxc-attach --name slackware
#
Check that the IP address of your environment matches the one in your config file.
# /usr/sbin/ip addr SHOW | grep eth
34: eth0@if35: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 [...] 1000
link/ether 4a:49:43:49:79:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 192.168.168.167/24 brd 192.168.168.255 scope global eth0
Exit the container, and shut it down.
# exit
$ sudo lxc-stop slackware
Running real-world containers with LXC
In real life, LXC makes it easy to create and run safe and secure containers. Containers have come a long way since the introduction of LXC in 2008, so use its developers' expertise to your advantage.
While the LXC instructions on linuxcontainers.org make the process simple, this tour of the manual side of things should help you understand what's going on behind the scenes.

"
Linux$OKD,"Getting started with OKD on your Linux desktop
Try out OKD, the community edition of the OpenShift container platform, with this tutorial.
OKD is the open source upstream community edition of Red Hat's OpenShift container platform. OKD is a container management and orchestration platform based on Docker and Kubernetes.
OKD is a complete solution to manage, deploy, and operate containerized applications that (in addition to the features provided by Kubernetes) includes an easy-to-use web interface, automated build tools, routing capabilities, and monitoring and logging aggregation features.
OKD provides several deployment options aimed at different requirements with single or multiple master nodes, high-availability capabilities, logging, monitoring, and more. You can create OKD clusters as small or as large as you need.
In addition to these deployment options, OKD provides a way to create a local, all-in-one cluster on your own machine using the oc command-line tool. This is a great option if you want to try OKD locally without committing the resources to create a larger multi-node cluster, or if you want to have a local cluster on your machine as part of your workflow or development process. In this case, you can create and deploy the applications locally using the same APIs and interfaces required to deploy the application on a larger scale. This process ensures a seamless integration that prevents issues with applications that work in the developer's environment but not in production.

This tutorial will show you how to create an OKD cluster using oc cluster up in a Linux box.
1. Install Docker
The oc cluster up command creates a local OKD cluster on your machine using Docker containers. In order to use this command, you need Docker installed on your machine. For OKD version 3.9 and later, Docker 1.13 is the minimum recommended version. If Docker is not installed on your system, install it by using your distribution package manager. For example, on CentOS or RHEL, install Docker with this command:
$ sudo yum install -y docker
On Fedora, use dnf:
$ sudo dnf install -y docker
This installs Docker and all required dependencies.
2. Configure Docker insecure registry
Once you have Docker installed, you need to configure it to allow the communication with an insecure registry on address 172.30.0.0/16. This insecure registry will be deployed with your local OKD cluster later.
On CentOS or RHEL, edit the file /etc/docker/daemon.json by adding these lines:
{
        ""insecure-registries"": [""172.30.0.0/16""]
}
On Fedora, edit the file /etc/containers/registries.conf by adding these lines:
[registries.insecure]
registries = ['172.30.0.0/16']
3. Start Docker
Before starting Docker, create a system group named docker and assign this group to your user so you can run Docker commands with your own user, without requiring root or sudo access. This allows you to create your OKD cluster using your own user.
For example, these are the commands to create the group and assign it to my local user, ricardo:
$ sudo groupadd docker
$ sudo usermod -a -G docker ricardo
You need to log out and log back in to see the new group association. After logging back in, run the id command and ensure you're a member of the docker group:
$ id
uid=1000(ricardo) gid=1000(ricardo) groups=1000(ricardo),10(wheel),1001(docker) 
context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
Now, start and enable the Docker daemon like this:
$ sudo systemctl start docker
$ sudo systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
Verify that Docker is running:
$ docker version
Client:
 Version:         1.13.1
 API version:     1.26
 Package version: docker-1.13.1-75.git8633870.el7.centos.x86_64
 Go version:      go1.9.4
 Git commit:      8633870/1.13.1
 Built:           Fri Sep 28 19:45:08 2018
 OS/Arch:         linux/amd64

Server:
 Version:         1.13.1
 API version:     1.26 (minimum version 1.12)
 Package version: docker-1.13.1-75.git8633870.el7.centos.x86_64
 Go version:      go1.9.4
 Git commit:      8633870/1.13.1
 Built:           Fri Sep 28 19:45:08 2018
 OS/Arch:         linux/amd64
 Experimental:    false
Ensure that the insecure registry option has been enabled by running docker info and looking for these lines:
$ docker info
... Skipping long output ...
Insecure Registries:
 172.30.0.0/16
 127.0.0.0/8
4. Open firewall ports
Next, open firewall ports to ensure your OKD containers can communicate with the master API. By default, some distributions have the firewall enabled, which blocks required connectivity from the OKD containers to the master API. If your system has the firewall enabled, you need to add rules to allow communication on ports 8443/tcp for the master API and 53/udp for DNS resolution on the Docker bridge subnet.
For CentOS, RHEL, and Fedora, you can use the firewall-cmd command-line tool to add the rules. For other distributions, you can use the provided firewall manager, such as UFW or iptables.
Before adding the firewall rules, obtain the Docker bridge network subnet's address, like this:
$ docker network inspect bridge | grep Subnet
                    ""Subnet"": ""172.17.0.0/16"",
Enable the firewall rules using this subnet. For CentOS, RHEL, and Fedora, use firewall-cmd to add a new zone:
$ sudo firewall-cmd --permanent --new-zone okdlocal
success
Include the subnet address you obtained before as a source to the new zone:
$ sudo firewall-cmd --permanent --zone okdlocal --add-source 172.17.0.0/16
success
Next, add the required rules to the okdlocal zone:
$ sudo firewall-cmd --permanent --zone okdlocal --add-port 8443/tcp
success
$ sudo firewall-cmd --permanent --zone okdlocal --add-port 53/udp
success
$ sudo firewall-cmd --permanent --zone okdlocal --add-port 8053/udp
success
Finally, reload the firewall to enable the new rules:
$ sudo firewall-cmd --reload
success
Ensure that the new zone and rules are in place:
$ sudo firewall-cmd --zone okdlocal --list-sources
172.17.0.0/16
$ sudo firewall-cmd --zone okdlocal --list-ports
8443/tcp 53/udp 8053/udp
Your system is ready to start the cluster. It's time to download the OKD client tools.
5. Download the OKD client tools
To deploy a local OKD cluster using oc, you need to download the OKD client tools package. For some distributions, like CentOS and Fedora, this package can be downloaded as an RPM from the official repositories. Please note that these packages may follow the distribution update cycle and usually are not the most recent version available.
For this tutorial, download the OKD client package directly from the official GitHub repository so you can get the most recent version available. At the time of writing, this was OKD v3.11.
Go to the OKD downloads page to get the link to the OKD tools for Linux, then download it with wget:
$ cd ~/Downloads/
$ wget https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz
Uncompress the downloaded package:
$ tar -xzvf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz
Finally, to make it easier to use the oc command systemwide, move it to a directory included in your $PATH variable. A good location is /usr/local/bin:
$ sudo cp openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit/oc /usr/local/bin/
One of the nicest features of the oc command is that it's a static single binary. You don't need to install it to use it.
Check that the oc command is working:
$ oc version
oc v3.11.0+0cbc58b
kubernetes v1.11.0+d4cacc0
features: Basic-Auth GSSAPI Kerberos SPNEGO
6. Start your OKD cluster
Once you have all the prerequisites in place, start your local OKD cluster by running this command:
$ oc cluster up
This command connects to your local Docker daemon, downloads all required images from Docker Hub, and starts the containers. The first time you run it, it takes a few minutes to complete. When it's finished, you will see this message:
... Skipping long output ...

OpenShift server started.

The server is accessible via web console at:
    https://127.0.0.1:8443

You are logged in as:
    User:     developer
    Password: <any value>

To login as administrator:
    oc login -u system:admin
From the command line, you can check if the cluster is running by entering this command:
$ oc cluster status
Web console URL: https://127.0.0.1:8443/console/

Config is at host directory 
Volumes are at host directory 
Persistent volumes are at host directory /home/ricardo/openshift.local.clusterup/openshift.local.pv
Data will be discarded when cluster is destroyed
You can also verify your cluster is working by logging in as the system:admin user and checking available nodes using the oc command-line tool:
$ oc login -u system:admin
Logged into ""https://127.0.0.1:8443"" as ""system:admin"" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    kube-dns
    kube-proxy
    kube-public
    kube-system
  * myproject
    openshift
    openshift-apiserver
    openshift-controller-manager
    openshift-core-operators
    openshift-infra
    openshift-node
    openshift-service-cert-signer
    openshift-web-console

Using project ""myproject"".

$ oc get nodes
NAME        STATUS    ROLES     AGE       VERSION
localhost   Ready     <none>    52m       v1.11.0+d4cacc0
Since this is a local, all-in-one cluster, you see only localhost in the nodes list.
7. Smoke-test your cluster
Now that your local OKD cluster is running, create a test app to smoke-test it. Use OKD to build and start the sample application so you can ensure the different components are working.
Start by logging in as the developer user:
$ oc login -u developer
Logged into ""https://127.0.0.1:8443"" as ""developer"" using existing credentials.

You have one project on this server: ""myproject""

Using project ""myproject"".
You're automatically assigned to a new, empty project named myproject. Create a sample PHP application based on an existing GitHub repository, like this:
$ oc new-app php:5.6~https://github.com/rgerardi/ocp-smoke-test.git 
--> Found image 92ed8b3 (5 months old) in image stream ""openshift/php"" under tag ""5.6"" for ""php:5.6""

    Apache 2.4 with PHP 5.6 
    ----------------------- 
    PHP 5.6 available as container is a base platform for building and running various PHP 5.6 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php56, rh-php56

    * A source build using source code from https://github.com/rgerardi/ocp-smoke-test.git will be created
      * The resulting image will be pushed to image stream tag ""ocp-smoke-test:latest""
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config ""ocp-smoke-test""
    * Ports 8080/tcp, 8443/tcp will be load balanced by service ""ocp-smoke-test""
      * Other containers can access this service through the hostname ""ocp-smoke-test""

--> Creating resources ...
    imagestream.image.openshift.io ""ocp-smoke-test"" created
    buildconfig.build.openshift.io ""ocp-smoke-test"" created
    deploymentconfig.apps.openshift.io ""ocp-smoke-test"" created
    service ""ocp-smoke-test"" created
--> Success
    Build scheduled, use 'oc logs -f bc/ocp-smoke-test' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/ocp-smoke-test' 
    Run 'oc status' to view your app.
OKD starts the build process, which clones the provided GitHub repository, compiles the application (if required), and creates the necessary images. You can follow the build process by tailing its log with this command:
$ oc logs -f bc/ocp-smoke-test
Cloning ""https://github.com/rgerardi/ocp-smoke-test.git"" ...
        Commit: 391a475713d01ab0afab700bab8a3d7549c5cc27 (Create index.php)
        Author: Ricardo Gerardi <ricardo.gerardi@gmail.com>
        Date:   Tue Oct 2 13:47:25 2018 -0400
Using 172.30.1.1:5000/openshift/php@sha256:f3c95020fa870fcefa7d1440d07a2b947834b87bdaf000588e84ef4a599c7546 as the s2i builder image
---> Installing application source...
=> sourcing 20-copy-config.sh ...
---> 04:53:28     Processing additional arbitrary httpd configuration provided by s2i ...
=> sourcing 00-documentroot.conf ...
=> sourcing 50-mpm-tuning.conf ...
=> sourcing 40-ssl-certs.sh ...
Pushing image 172.30.1.1:5000/myproject/ocp-smoke-test:latest ...
Pushed 1/10 layers, 10% complete
Push successful
After the build process completes, OKD starts the application automatically by running a new pod based on the created image. You can see this new pod with this command:
$ oc get pods
NAME                     READY     STATUS      RESTARTS   AGE
ocp-smoke-test-1-build   0/1       Completed   0          1m
ocp-smoke-test-1-d8h76   1/1       Running     0          7s
You can see two pods are created; the first one (with the status Completed) is the pod used to build the application. The second one (with the status Running) is the application itself.
In addition, OKD creates a service for this application. Verify it by using this command:
$ oc get service
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
ocp-smoke-test   ClusterIP   172.30.232.241   <none>        8080/TCP,8443/TCP   1m
Finally, expose this service externally using OKD routes so you can access the application from a local browser:
$ oc expose svc ocp-smoke-test
route.route.openshift.io/ocp-smoke-test exposed

$ oc get route
NAME             HOST/PORT                                   PATH      SERVICES         PORT       TERMINATION   WILDCARD
ocp-smoke-test   ocp-smoke-test-myproject.127.0.0.1.nip.io             ocp-smoke-test   8080-tcp                 None

"
,
Linux$containers$Podman,"Podman: A more secure way to run containers
Podman uses a traditional fork/exec model (vs. a client/server model) for running containers.
Before I get into the main topic of this article, Podman and containers, I need to get a little technical about the Linux audit feature.
What is audit?
The Linux kernel has an interesting security feature called audit. It allows administrators to watch for security events on a system and have them logged to the audit.log, which can be stored locally or remotely on another machine to prevent a hacker from trying to cover his tracks.
The /etc/shadow file is a common security file to watch, since adding a record to it could allow an attacker to get return access to the system. Administrators want to know if any process modified the file. You can do this by executing the command:
# auditctl -w /etc/shadow
Now let's see what happens if I modify the /etc/shadow file:
# touch /etc/shadow
# ausearch -f /etc/shadow -i -ts recent
type=PROCTITLE msg=audit(10/10/2018 09:46:03.042:4108) : proctitle=touch /etc/shadow
type=SYSCALL msg=audit(10/10/2018 09:46:03.042:4108) : arch=x86_64 syscall=openat
success=yes exit=3 a0=0xffffff9c a1=0x7ffdb17f6704 a2=O_WRONLY|O_CREAT|O_NOCTTY|
O_NONBLOCK a3=0x1b6 items=2 ppid=2712 pid=3727 auid=dwalsh uid=root gid=root
euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=pts1 ses=3 comm=touch
exe=/usr/bin/touch subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=(null)
There's a lot of information in the audit record, but I highlighted that it recorded that root modified the /etc/shadow file and the owner of the process' audit UID (auid) was dwalsh.
Did the kernel do that?
Tracking the login UID
There is a field called loginuid, stored in /proc/self/loginuid, that is part of the proc struct of every process on the system. This field can be set only once; after it is set, the kernel will not allow any process to reset it.

When I log into the system, the login program sets the loginuid field for my login process.
My UID, dwalsh, is 3267.
$ cat /proc/self/loginuid
3267
Now, even if I become root, my login UID stays the same.
$ sudo cat /proc/self/loginuid
3267
Note that every process that's forked and executed from the initial login process automatically inherits the loginuid. This is how the kernel knew that the person who logged was dwalsh.
Containers
Now let's look at containers.
sudo podman run fedora cat /proc/self/loginuid
3267
Even the container process retains my loginuid. Now let's try with Docker.
sudo docker run fedora cat /proc/self/loginuid
4294967295
Why the difference?
Podman uses a traditional fork/exec model for the container, so the container process is an offspring of the Podman process. Docker uses a client/server model. The docker command I executed is the Docker client tool, and it communicates with the Docker daemon via a client/server operation. Then the Docker daemon creates the container and handles communications of stdin/stdout back to the Docker client tool.
The default loginuid of processes (before their loginuid is set) is 4294967295. Since the container is an offspring of the Docker daemon and the Docker daemon is a child of the init system, we see that systemd, Docker daemon, and the container processes all have the same loginuid, 4294967295, which audit refers to as the unset audit UID.
cat /proc/1/loginuid
4294967295
How can this be abused?
Let's look at what would happen if a container process launched by Docker modifies the /etc/shadow file.
$ sudo docker run --privileged -v /:/host fedora touch /host/etc/shadow
$ sudo ausearch -f /etc/shadow -i
type=PROCTITLE msg=audit(10/10/2018 10:27:20.055:4569) : proctitle=/usr/bin/coreutils
--coreutils-prog-shebang=touch /usr/bin/touch /host/etc/shadow
type=SYSCALL msg=audit(10/10/2018 10:27:20.055:4569) : arch=x86_64 syscall=openat
success=yes exit=3 a0=0xffffff9c a1=0x7ffdb6973f50 a2=O_WRONLY|O_CREAT|O_NOCTTY|
O_NONBLOCK a3=0x1b6 items=2 ppid=11863 pid=11882 auid=unset uid=root gid=root
euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset
comm=touch exe=/usr/bin/coreutils subj=system_u:system_r:spc_t:s0 key=(null)
In the Docker case, the auid is unset (4294967295); this means the security officer might know that a process modified the /etc/shadow file but the identity was lost.
If that attacker then removed the Docker container, there would be no trace on the system of who modified the /etc/shadow file.
Now let's look at the exact same scenario with Podman.
$ sudo podman run --privileged -v /:/host fedora touch /host/etc/shadow
$ sudo ausearch -f /etc/shadow -i
type=PROCTITLE msg=audit(10/10/2018 10:23:41.659:4530) : proctitle=/usr/bin/coreutils
--coreutils-prog-shebang=touch /usr/bin/touch /host/etc/shadow
type=SYSCALL msg=audit(10/10/2018 10:23:41.659:4530) : arch=x86_64 syscall=openat
success=yes exit=3 a0=0xffffff9c a1=0x7fffdffd0f34 a2=O_WRONLY|O_CREAT|O_NOCTTY|
O_NONBLOCK a3=0x1b6 items=2 ppid=11671 pid=11683 auid=dwalsh uid=root gid=root
euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=3 comm=touch
exe=/usr/bin/coreutils subj=unconfined_u:system_r:spc_t:s0 key=(null)
Everything is recorded correctly with Podman since it uses traditional fork/exec.
This was just a simple example of watching the /etc/shadow file, but the auditing system is very powerful for watching what processes do on a system. Using a fork/exec container runtime for launching containers (instead of a client/server container runtime) allows you to maintain better security through audit logging.
Final thoughts
There are many other nice features about the fork/exec model versus the client/server model when launching containers. For example, systemd features include:
SD_NOTIFY: If you put a Podman command into a systemd unit file, the container process can return notice up the stack through Podman that the service is ready to receive tasks. This is something that can't be done in client/server mode.
Socket activation: You can pass down connected sockets from systemd to Podman and onto the container process to use them. This is impossible in the client/server model.
The nicest feature, in my opinion, is running Podman and containers as a non-root user. This means you never have give a user root privileges on the host, while in the client/server model (like Docker employs), you must open a socket to a privileged daemon running as root to launch the containers. There you are at the mercy of the security mechanisms implemented in the daemon versus the security mechanisms implemented in the host operating systems—a dangerous proposition.
"
Kuryr-Kubernetes$CNI,"How to 'Kubernetize' an OpenStack service
Kuryr-Kubernetes provides networking for Kubernetes pods by using OpenStack Neutron and Octavia.
Kuryr-Kubernetes is an OpenStack project, written in Python, that serves as a container network interface (CNI) plugin that provides networking for Kubernetes pods by using OpenStack Neutron and Octavia. The project stepped out of its experimental phase and became a fully supported OpenStack ecosystem citizen in OpenStack's Queens release (the 17th version of the cloud infrastructure software).
One of Kuryr-Kubernetes' main advantages is you don't need to use multiple software development networks (SDNs) for network management in OpenStack and Kubernetes. It also solves the issue of using double encapsulation of network packets when running a Kubernetes cluster on an OpenStack cloud. Imagine using Calico for Kubernetes networking and Neutron for networking the Kubernetes cluster's virtual machines (VMs). With Kuryr-Kubernetes, you use just one SDN—Neutron—to provide connectivity for the pods and the VMs where those pods are running.
You can also run Kuryr-Kubernetes on a bare-metal node as a normal OpenStack service. This way, you can provide interconnectivity between Kubernetes pods and OpenStack VMs—even if those clusters are separate—by just putting Neutron-agent and Kuryr-Kubernetes on your Kubernetes nodes.
Kuryr-Kubernetes consists of three parts:
kuryr-controller observes Kubernetes resources, decides how to translate them into OpenStack resources, and creates those resources. Information about OpenStack resources is saved into annotations of corresponding Kubernetes resources.
kuryr-cni is an executable run by the CNI that passes the calls to kuryr-daemon.
kuryr-daemon should be running on every Kubernetes node. It watches the pods created on the host and, when a CNI request comes in, wires the pods according to the Neutron ports included in the pod annotations.
In general, the control part of a CNI plugin (like Calico or Nuage) runs as a pod on the Kubernetes cluster where it provides networking, so, naturally, the Kuryr team decided to follow that model. But converting an OpenStack service into a Kubernetes app wasn't exactly a trivial task.
Kuryr-Kubernetes requirements
Kuryr-Kubernetes is just an application, and applications have requirements. Here is what each component needs from the environment and how it translates to Kubernetes' primitives.
kuryr-controller
There should be exactly one instance of kuryr-controller (although that number may be higher with the A/P high-availability feature implemented in OpenStack Rocky). This is easy to achieve using Kubernetes' Deployment primitive.
Kubernetes ServiceAccounts can provide access to the Kubernetes API with a granular set of permissions.
Different SDNs provide access to the OpenStack API differently. API SSL certificates should also be provided, for example by mounting a Secret in the pod.
To avoid a chicken-and-egg problem, kuryr-controller should run with hostNetworking to bypass using Kuryr to get the IP.
Provide a kuryr.conf file, preferably by mounting it as a ConfigMap.
In the end, we get a Deployment manifest similar to this:
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    name: kuryr-controller
  name: kuryr-controller
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: kuryr-controller
      name: kuryr-controller
    spec:
      serviceAccountName: kuryr-controller
      automountServiceAccountToken: true
      hostNetwork: true
      containers:
      - image: kuryr/controller:latest
        name: controller
        volumeMounts:
        - name: config-volume
          mountPath: ""/etc/kuryr/kuryr.conf""
          subPath: kuryr.conf
        - name: certificates-volume
          mountPath: ""/etc/ssl/certs""
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: kuryr-config
      - name: certificates-volume
        secret:
          secretName: kuryr-certificates
      restartPolicy: Always
kuryr-daemon and kuryr-cni
Both of these components should be present on every Kubernetes node. When the kuryr-daemon container starts on the Kubernetes nodes, it injects the kuryr-cni executable and reconfigures the CNI to use it. Let's break that down into requirements.
kuryr-daemon should run on every Kubernetes node. This means it can be represented as a DaemonSet.
It should be able to access the Kubernetes API. This can be implemented with ServiceAccounts.
It also needs a kuryr.conf file. Again, the best way is to use a ConfigMap.
To perform networking operations on the node, it must run with hostNetworking and as a privileged container.
As it needs to inject the kuryr-cni executable and the CNI configuration, the Kubernetes nodes' /opt/cni/bin and /etc/cni/net.d directories must be mounted on the pod.
It also needs access to the Kubernetes nodes' netns, so /proc must be mounted on the pod. (Note that you cannot use /proc as a mount destination, so it must be named differently and Kuryr needs to be configured to know that.)
If it's running with the Open vSwitch Neutron plugin, it must mount /var/run/openvswitch.
To identify pods running on its node, nodeName should be passed into the pod. This can be done using environment variables. (This is also true with the pod name, which will be explained below.)
This produces a more complicated manifest:
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kuryr-cni
  namespace: kube-system
  labels:
    name: kuryr-cni
spec:
  template:
    metadata:
      labels:
        Name: kuryr-cni
    spec:
      hostNetwork: true
      serviceAccountName: kuryr-controller
      containers:
      - name: kuryr-cni
        image: kuryr/cni:latest
        command: [ ""cni_ds_init"" ]
        env:
        - name: KUBERNETES_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: KURYR_CNI_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        securityContext:
          privileged: true
        volumeMounts:
        - name: bin
          mountPath: /opt/cni/bin
        - name: net-conf
          mountPath: /etc/cni/net.d
        - name: config-volume
          mountPath: /etc/kuryr/kuryr.conf
          subPath: kuryr-cni.conf
        - name: proc
          mountPath: /host_proc
        - name: openvswitch
          mountPath: /var/run/openvswitch
      volumes:
        - name: bin
          hostPath:
            path: /opt/cni/bin
        - name: net-conf
          hostPath:
            path: /etc/cni/net.d
        - name: config-volume
          configMap:
            name: kuryr-config
        - name: proc
          hostPath:
            path: /proc
        - name: openvswitch
          hostPath:
            path: /var/run/openvswitch
Injecting the kuryr-cni executable
This part took us the longest time. We went through four different approaches until everything worked. Our solution was to inject a Python application from the container into the container's host and to inject the CNI configuration files (but the latter is trivial). Most of the issues were related to the fact that Python applications aren't binaries, but scripts.
We first tried making our kuryr-cni script a binary using PyInstaller. Although this worked fairly well, it had serious disadvantages. For one thing, the build process was complicated—we had to create a container with PyInstaller and Kuryr-Kubernetes that generated the binary, then build the kuryr-daemon container image with that binary. Also, due to PyInstaller quirks, we ended up with a lot of misleading tracebacks in kubelet logs, i.e., in exceptions, we could get the wrong traceback on the logs. The deciding factor was that PyInstaller changed paths to the included Python modules. This meant that some checks in the os.vif library failed and broke our continuous integration (CI).
We also tried injecting a Python virtual environment (venv) containing a CPython binary, the kuryr-kubernetes package, and all its requirements. The problem is Python venvs aren't designed to be portable. Even though there is a --relocatable option in the virtualenv command-line tool, it doesn't always work. We abandoned that approach.
Then we tried what we think is the ""correct"" way: injecting the host with an executable script that does docker exec -i on a kuryr-daemon container. Because the kuryr-kubernetes package is installed in that container, it can easily execute the kuryr-cni binary. All the CNI environment variables must be passed through the docker exec command, which has been possible since Docker API v1.24. Then, we only needed to identify the Docker container where it should be executed.
At first, we tried calling the Kubernetes API from the kuryr-daemon container's entry point to get its own container ID. We quickly discovered that this causes a race condition, and sometimes the entry point runs before the Kubernetes API is updated with its container ID. So, instead of calling the Kubernetes API, we made the injected CNI script call the Docker API on the host. Then it's easy to identify the kuryr-daemon container using labels added by Kubernetes.
Lessons learned
In the end, we've got a working system that is easy to deploy and manage because it's running on Kubernetes. We've proved that Kuryr-Kubernetes is just an application. While it took a lot of time and effort, the results are worth it. A ""Kubernetized"" application is much easier to manage and distribute.
"
DevOps$containers,"What containers can teach us about DevOps
The use of containers supports the three pillars of DevOps practices: flow, feedback, and continual experimentation and learning.
One can argue that containers and DevOps were made for one another. Certainly, the container ecosystem benefits from the skyrocketing popularity of DevOps practices, both in design choices and in DevOps’ use by teams developing container technologies. Because of this parallel evolution, the use of containers in production can teach teams the fundamentals of DevOps and its three pillars: The Three Ways.
Principles of flow
Container flow
A container can be seen as a silo, and from inside, it is easy to forget the rest of the system: the host node, the cluster, the underlying infrastructure. Inside the container, it might appear that everything is functioning in an acceptable manner. From the outside perspective, though, the application inside the container is a part of a larger ecosystem of applications that make up a service: the web API, the web app user interface, the database, the workers, and caching services and garbage collectors. Teams put constraints on the container to limit performance impact on infrastructure, and much has been done to provide metrics for measuring container performance because overloaded or slow container workloads have downstream impact on other services or customers.

Real-world flow
This lesson can be applied to teams functioning in a silo as well. Every process (be it code release, infrastructure creation or even, say, manufacturing of Spacely’s Sprockets), follows a linear path from conception to realization. In technology, this progress flows from development to testing to operations and release. If a team working alone becomes a bottleneck or introduces a problem, the impact is felt all along the entire pipeline. A defect passed down the line destroys productivity downstream. While the broken process within the scope of the team itself may seem perfectly correct, it has a negative impact on the environment as a whole.
DevOps and flow
The first way of DevOps, principles of flow, is about approaching the process as a whole, striving to comprehend how the system works together and understanding the impact of issues on the entire process. To increase the efficiency of the process, pain points and waste are identified and removed. This is an ongoing process; teams must continually strive to increase visibility into the process and find and fix trouble spots and waste.
“The outcomes of putting the First Way into practice include never passing a known defect to downstream work centers, never allowing local optimization to create global degradation, always seeking to increase flow, and always seeking to achieve a profound understanding of the system (as per Deming).”
–Gene Kim, The Three Ways: The Principles Underpinning DevOps, IT Revolution, 25 Apr. 2017
Principles of feedback
Container feedback
In addition to limiting containers to prevent impact elsewhere, many products have been created to monitor and trend container metrics in an effort to understand what they are doing and notify when they are misbehaving. Prometheus, for example, is all the rage for collecting metrics from containers and clusters. Containers are excellent at separating applications and providing a way to ship an environment together with the code, sometimes at the cost of opacity, so much is done to try to provide rapid feedback so issues can be addressed promptly within the silo.
Real-world feedback
The same is necessary for the flow of the system. From inception to realization, an efficient process quickly provides relevant feedback to identify when there is an issue. The key words here are “quick” and “relevant.” Burying teams in thousands of irrelevant notifications make it difficult or even impossible to notice important events that need immediate action, and receiving even relevant information too late may allow small, easily solved issues to move downstream and become bigger problems. Imagine if Lucy and Ethel had provided immediate feedback that the conveyor belt was too fast—there would have been no problem with the chocolate production (though that would not have been nearly as funny).
DevOps and feedback
The Second Way of DevOps, principles of feedback, is all about getting relevant information quickly. With immediate, useful feedback, problems can be identified as they happen and addressed before impact is felt elsewhere in the development process. DevOps teams strive to “optimize for downstream” and immediately move to fix problems that might impact other teams that come after them. As with flow, feedback is a continual process to identify ways to quickly get important data and act on problems as they occur.
“Creating fast feedback is critical to achieving quality, reliability, and safety in the technology value stream.”
–Gene Kim, et al., The DevOps Handbook: How to Create World-Class Agility, Reliability, and Security in Technology Organizations, IT Revolution Press, 2016
Principles of continual experimentation and learning
Container continual experimentation and learning
It is a bit more challenging applying operational learning to the Third Way of DevOps:continual experimentation and learning. Trying to salvage what we can grasp of the very edges of the metaphor, containers make development easy, allowing developers and operations teams to test new code or configurations locally and safely outside of production and incorporate discovered benefits into production in a way that was difficult in the past. Changes can be radical and still version-controlled, documented, and shared quickly and easily.
Real-world continual experimentation and learning
For example, consider this anecdote from my own experience: Years ago, as a young, inexperienced sysadmin (just three weeks into the job), I was asked to make changes to an Apache virtual host running the website of the central IT department for a university. Without an easy-to-use test environment, I made a configuration change to the production site that I thought would accomplish the task and pushed it out. Within a few minutes, I overheard coworkers in the next cube:
“Wait, is the website down?”
“Hrm, yeah, it looks like it. What the heck?”
There was much eye-rolling involved.
Mortified (the shame is real, folks), I sunk down as far as I could into my seat and furiously tried to back out the changes I’d introduced. Later that same afternoon, the director of the department—the boss of my boss’s boss—appeared in my cube to talk about what had happened. “Don’t worry,” she told me. “We’re not mad at you. It was a mistake and now you have learned.”
In the world of containers, this could have been easily changed and tested on my own laptop and the broken configuration identified by more skilled team members long before it ever made it into production.
DevOps continual experimentation and learning
A real culture of experimentation promotes the individual’s ability to find where a change in the process may be beneficial, and to test that assumption without the fear of retaliation if they fail. For DevOps teams, failure becomes an educational tool that adds to the knowledge of the individual and organization, rather than something to be feared or punished. Individuals in the DevOps team dedicate themselves to continuous learning, which in turn benefits the team and wider organization as that knowledge is shared.
As the metaphor completely falls apart, focus needs to be given to a specific point: The other two principles may appear at first glance to focus entirely on process, but continual learning is a human task—important for the future of the project, the person, the team, and the organization. It has an impact on the process, but it also has an impact on the individual and other people.
“Experimentation and risk-taking are what enable us to relentlessly improve our system of work, which often requires us to do things very differently than how we’ve done it for decades.”
–Gene Kim, et al., The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win, IT Revolution Press, 2013
Containers can teach us DevOps
Learning to work effectively with containers can help teach DevOps and the Three Ways: principles of flow, principles of feedback, and principles of continuous experimentation and learning. Looking holistically at the application and infrastructure rather than putting on blinders to everything outside the container teaches us to take all parts of the system and understand their upstream and downstream impacts, break out of silos, and work as a team to increase global performance and deep understanding of the entire system. Working to provide timely and accurate feedback teaches us to create effective feedback patterns within our organizations to identify problems before their impact grows. Finally, providing a safe environment to try new ideas and learn from them teaches us to create a culture where failure represents a positive addition to our knowledge and the ability to take big chances with educated guesses can result in new, elegant solutions to complex problems.
"
Perl6$containers,"Containers in Perl 6
In the third article in this series comparing Perl 5 to Perl 6, learn how to handle references, bindings, and containers in Perl 6.
In the first article in this series comparing Perl 5 to Perl 6, we looked into some of the issues you might encounter when migrating code into Perl 6. In the second article, we examined how garbage collection works in Perl 6. Here, in the third article, we'll focus on Perl 5's references and how they're handled in Perl 6, and introduce the concepts of binding and containers.
References
There are no references in Perl 6, which is surprising to many people used to Perl 5's semantics. But worry not: because there are no references, you don't have to worry about whether something should be de-referenced or not.
# Perl 5
my $foo = \@bar;   # must add reference \ to make $foo a reference to @bar
say @bar[1];       # no dereference needed
say $foo->[1];     # must add dereference ->

# Perl 6
my $foo = @bar;    # $foo now contains @bar
say @bar[1];       # no dereference needed, note: sigil does not change
say $foo[1];       # no dereference needed either
One could argue that everything in Perl 6 is a reference. Coming from Perl 5 (where an object is a blessed reference), this would be a logical conclusion about Perl 6 where everything is an object (or can be considered one). But that wouldn't do justice to the situation in Perl 6 and would hinder you in understanding how things work in Perl 6. Beware of false friends!
Binding
Before we get to assignment, it is important to understand the concept of binding in Perl 6. You can bind something explicitly to something else using the := operator. When you define a lexical variable, you can bind a value to it:
my $foo := 42;  # note: := instead of =
Simply put, this creates a key with the name ""$foo"" in the lexical pad (lexpad) (which you could consider a compile-time hash that contains information about things that are visible in that lexical scope) and makes 42 its literal value. Because this is a literal constant, you can't change it. Trying to do so will cause an exception. So don't do that!
This binding operation is used under the hood in many situations, for instance when iterating:
my @a = 0..9;    # can also be written as ^10
say @a;          # [0 1 2 3 4 5 6 7 8 9]
for @a { $_++ }  # $_ is bound to each array element and incremented
say @a;          # [1 2 3 4 5 6 7 8 9 10]
If you try to iterate over a constant list, then $_ is bound to the literal values, which you can not increment:
for 0..9 { $_++ }  # error: requires mutable arguments
Assignment
If you compare ""create a lexical variable and assign to it"" in Perl 5 and Perl 6, it looks the same on the outside:
my $bar = 56;  # both Perl 5 and Perl 6
In Perl 6, this also creates a key with the name ""$bar"" in the lexpad. But instead of directly binding the value to that lexpad entry, a container (a Scalar object) is created for you and that is bound to the lexpad entry of ""$bar"". Then, 56 is stored as the value in that container. In pseudo-code, you can think of this as:
my $bar := Scalar.new( value => 56 );
Notice that the Scalar object is bound, not assigned. The closest thing to this in Perl 5 is a tied scalar. But of course ""= 56"" is much less to type!
Data structures such as Array and Hash also automatically put values in containers bound to the structure.
my @a;       # empty Array
@a[5] = 42;  # bind a Scalar container to 6th element and put 42 in it
Containers
The Scalar container object is invisible for most operations in Perl 6, so most of the time you don't have to think about it. For instance, whenever you call a subroutine (or a method) with a variable as an argument, it will bind to the value in the container. And because you cannot assign to a value, you get:
sub frobnicate($this) {
    $this = 42;
}
my $foo = 666;
frobnicate($foo); # Cannot assign to a readonly variable or a value
If you want to allow assigning to the outer value, you can add the is rw trait to the variable in the signature. This will bind the variable in the signature to the container of the variable specified, thus allowing assignment:
sub oknicate($this is rw) {
    $this = 42;
}
my $foo = 666;
oknicate($foo); # no problem
say $foo;       # 42
Proxy
Conceptually, the Scalar object in Perl 6 has a FETCH method (for producing the value in the object) and a STORE method (for changing the value in the object), just like a tied scalar in Perl 5.
Suppose you later assign the value 768 to the $bar variable:
$bar = 768;
What happens is conceptually the equivalent of:
$bar.STORE(768);
Suppose you want to add 20 to the value in $bar:
$bar = $bar + 20;
What happens conceptually is:
$bar.STORE( $bar.FETCH + 20 );
If you like to specify your own FETCH and STORE methods on a container, you can do that by binding to a Proxy object. For example, to create a variable that will always report twice the value that was assigned to it:
my $double := do {  # $double now a Proxy, rather than a Scalar container
    my $value;
    Proxy.new(
      FETCH => method ()     { $value + $value },
      STORE => method ($new) { $value = $new }
    )
}
Note that you will need an extra variable to keep the value stored in such a container.
Constraints and default
Apart from the value, a Scalar also contains extra information such as the type constraint and default value. Take this definition:
my Int $baz is default(42) = 666;
It creates a Scalar bound with the name ""$baz"" to the lexpad, constrains the values in that container to types that successfully smartmatch with Int, sets the default value of the container to 42, and puts the value 666 in the container.
Assigning a string to that variable will fail because of the type constraint:
$baz = ""foo"";
# Type check failed in assignment to $baz; expected Int but got Str (""foo"")
If you do not give a type constraint when you define a variable, then the Any type will be assumed. If you do not specify a default value, then the type constraint will be assumed.
Assigning Nil (the Perl 6 equivalent of Perl 5's undef) to that variable will reset it to the default value:
say $baz;   # 666
$baz = Nil;
say $baz;   # 42
Summary
Perl 5 has values and references to values. Perl 6 has no references, but it has values and containers. There are two types of containers in Perl 6: Proxy (which is much like a tied scalar in Perl 5) and Scalar. Simply stated, a variable, as well as an element of a List, Array, or Hash, is either a value (if it is bound), or a container (if it is assigned). Whenever a subroutine (or method) is called, the given arguments are de-containerized and bound to the parameters of the subroutine (unless told to do otherwise). A container also keeps information such as type constraints and a default value. Assigning Nil to a variable will return it to its default value, which is Any if you do not specify a type constraint.
"
sysadmin$containers,"A sysadmin's guide to containers
What you need to know to understand how containers work.
The term ""containers"" is heavily overused. Also, depending on the context, it can mean different things to different people.
Traditional Linux containers are really just ordinary processes on a Linux system. These groups of processes are isolated from other groups of processes using resource constraints (control groups [cgroups]), Linux security constraints (Unix permissions, capabilities, SELinux, AppArmor, seccomp, etc.), and namespaces (PID, network, mount, etc.).
If you boot a modern Linux system and took a look at any process with cat /proc/PID/cgroup, you see that the process is in a cgroup. If you look at /proc/PID/status, you see capabilities. If you look at /proc/self/attr/current, you see SELinux labels. If you look at /proc/PID/ns, you see the list of namespaces the process is in. So, if you define a container as a process with resource constraints, Linux security constraints, and namespaces, by definition every process on a Linux system is in a container. This is why we often say Linux is containers, containers are Linux. Container runtimes are tools that modify these resource constraints, security, and namespaces and launch the container.
Docker introduced the concept of a container image, which is a standard TAR file that combines:
Rootfs (container root filesystem): A directory on the system that looks like the standard root (/) of the operating system. For example, a directory with /usr, /var, /home, etc.
JSON file (container configuration): Specifies how to run the rootfs; for example, what command or entrypoint to run in the rootfs when the container starts; environment variables to set for the container; the container's working directory; and a few other settings.
Docker ""tar's up"" the rootfs and the JSON file to create the base image. This enables you to install additional content on the rootfs, create a new JSON file, and tar the difference between the original image and the new image with the updated JSON file. This creates a layered image.
The definition of a container image was eventually standardized by the Open Container Initiative (OCI) standards body as the OCI Image Specification.
Tools used to create container images are called container image builders. Sometimes container engines perform this task, but several standalone tools are available that can build container images.
Docker took these container images (tarballs) and moved them to a web service from which they could be pulled, developed a protocol to pull them, and called the web service a container registry.
Container engines are programs that can pull container images from container registries and reassemble them onto container storage. Container engines also launch container runtimes (see below).
Container storage is usually a copy-on-write (COW) layered filesystem. When you pull down a container image from a container registry, you first need to untar the rootfs and place it on disk. If you have multiple layers that make up your image, each layer is downloaded and stored on a different layer on the COW filesystem. The COW filesystem allows each layer to be stored separately, which maximizes sharing for layered images. Container engines often support multiple types of container storage, including overlay, devicemapper, btrfs, aufs, and zfs.
After the container engine downloads the container image to container storage, it needs to create a container runtime configuration. The runtime configuration combines input from the caller/user along with the content of the container image specification. For example, the caller might want to specify modifications to a running container's security, add additional environment variables, or mount volumes to the container.

The layout of the container runtime configuration and the exploded rootfs have also been standardized by the OCI standards body as the OCI Runtime Specification.
Finally, the container engine launches a container runtime that reads the container runtime specification; modifies the Linux cgroups, Linux security constraints, and namespaces; and launches the container command to create the container's PID 1. At this point, the container engine can relay stdin/stdout back to the caller and control the container (e.g., stop, start, attach).
Note that many new container runtimes are being introduced to use different parts of Linux to isolate containers. People can now run containers using KVM separation (think mini virtual machines) or they can use other hypervisor strategies (like intercepting all system calls from processes in containers). Since we have a standard runtime specification, these tools can all be launched by the same container engines. Even Windows can use the OCI Runtime Specification for launching Windows containers.
At a much higher level are container orchestrators. Container orchestrators are tools used to coordinate the execution of containers on multiple different nodes. Container orchestrators talk to container engines to manage containers. Orchestrators tell the container engines to start containers and wire their networks together. Orchestrators can monitor the containers and launch additional containers as the load increases.

"
containers,"What's in a container image: Meeting the legal challenges
The implications of distributing software through container images are quite different from those of the package managers many people are familiar with.
Container technology has, for many years, been transforming how workloads in data centers are managed and speeding the cycle of application development and deployment.
In addition, container images are increasingly used as a distribution format, with container registries a mechanism for software distribution. Isn't this just like packages distributed using package management tools? Not quite. While container image distribution is similar to RPMs, DEBs, and other package management systems (for example, storing and distributing archives of files), the implications of container image distribution are more complicated. It is not the fault of container technology itself; rather, it's because container distribution is used differently than package management systems.
Talking about the challenges of license compliance for container images, Dirk Hohndel, chief open source officer at VMware, pointed out that the content of a container image is more complex than most people expect, and many readily available images have been built in surprisingly cavalier ways. (See the LWN.net article by Jake Edge about a talk Dirk gave in April.)
Why is it hard to understand the licensing of container images? Shouldn't there just be a label for the image (""the license is X"")? In the Open Container Image Format Specification, one of the pre-defined annotation keys is ""org.opencontainers.image.licenses,"" which is described as ""License(s) under which contained software is distributed as an SPDX License Expression."" But that doesn't contemplate the complexity of a container image–while very simple images are built from tens of components, images are often built from hundreds of components. An SPDX License Expression is most frequently used to convey the licensing for a single source file. Such expressions can handle more than one license, such as ""GPL-2.0 OR BSD-3-Clause"" (see, for example, Appendix IV of version 2.1 of the SPDX specification). But the licensing for a typical container image is, typically, much more complicated.

In talking about container-related technology, the term ""container"" can lead to confusion. A container does not refer to the containment of files for storing or transferring. Rather, it refers to using features built into the kernel (such as cgroups and namespaces) to present a sort of ""contained"" experience to code running on the kernel. In other words, the containment to which ""container"" refers is an execution experience, not a distribution experience. The set of files to be laid out in a file system as the basis for an executing container is typically distributed in what is known as a ""container image,"" sometimes confusingly referred to simply as a container, thereby awkwardly overloading the term ""container.""
In understanding software distribution via container images, I believe it is useful to consider two separate factors:
Diversity of content: The basic unit of software distribution (a container image) includes a larger quantity and diversity of content than in the basic unit of distribution in typical software distribution mechanisms.
Use model: The nature of widely used tooling fosters the use of a registry, which is often publicly available, in the typical workflow.
Diversity of content
When talking about a particular container image, the focus of attention is often on a particular software component (for example, a database or the code that implements one specific service). However, the container image includes a much larger collection of software. In fact, even the developer who created the image may have only a superficial understanding of and/or interest in most of the components in the image. With other distribution mechanisms, those other pieces of software would be identified as dependencies, and users of the software might be directed elsewhere for expertise on those components. In a container, the individual who acquires the container image isn't aware of those additional components that play supporting roles to the featured component.
The unit of distribution: user-driven vs. factory-driven
For container images, the distribution unit is user-driven, not factory-driven. Container images are a great tool for reducing the burden on software consumers. With a container image, the image's consumer can focus on the application of interest; the image's builder can take care of the dependencies and configuration. This simplification can be a huge benefit.
When the unit of software is driven by the ""factory,"" the user bears a greater responsibility for building a platform on which to run the software of interest, assembling the correct versions of the dependencies, and getting all the configuration details right. The unit of distribution in a package management system is a modular unit, rather than a complete solution. This unit facilitates building and maintaining a flow of components that are flexible enough to be assembled into myriad solutions. Note that because of this unit, a package maintainer will typically be far more familiar with the content of the packages than someone who builds containers. A person building a container may have a detailed understanding of the container's featured components, but limited familiarity with the image's supporting components.
Packages, package management system tools, package maintenance processes, and package maintainers are incredibly underappreciated. They have been central to delivery of a large variety of software over the last two decades. While container images are playing a growing role, I don't expect the importance of package management systems to fade anytime soon. In fact, the bulk of the content in container images benefits from being built from such packages.
In understanding container images, it is important to appreciate how distribution via such images has different properties than distribution of packages. Much of the content in images is built from packages, but the image's consumer may not know what packages are included or other package-level information. In the future, a variety of techniques may be used to build containers, e.g., directly from source without involvement of a package maintainer.
Use models
What about reports that so many container images are poorly built? In part, the volume of casually built images is because of container tools that facilitate a workflow to make images publicly available. When experimenting with container tools and moving to a workflow that extends beyond a laptop, the tools expect you to have a repository where multiple machines can pull container images (a container registry). You could spin up your own. Some widely used tools make it easy to use an existing registry that is available at no cost, provided the images are publicly available. This makes many casually built images visible, even those that were never intended to be maintained or updated.
By comparison, how often do you see developers publishing RPMs of their early explorations? RPMs resulting from experimentation by random developers are not ending up in the major package repositories.
Or consider someone experimenting with the latest machine learning frameworks. In the past, a researcher might have shared only analysis results. Now, they can share a full analytical software configuration by publishing a container image. This could be a great benefit to other researchers. However, those browsing a container registry could be confused by the ready-to-run nature of such images. It is important to distinguish between an image built for one individual's exploration and an image that was assembled and tested with broad use in mind.
Be aware that container images include supporting software, not just the featured software; a container image distributes a collection of software. If you are building upon or otherwise using images built by others, be aware of how that image was built and consider your level of confidence in the image's source.
"
JavaScript$framework$machinelearning,"5 trending open source machine learning JavaScript frameworks
Whether you're a JavaScript developer who wants to dive into machine learning or a machine learning expert who plans to use JavaScript, these open source frameworks may intrigue you.
The tremendous growth of the machine learning field has been driven by the availability of open source tools that allow developers to build applications easily. (For example, AndreyBu, who is from Germany and has more than five years of experience in machine learning, has been utilizing various open source frameworks to build captivating machine learning projects.)
Although the Python programming language powers most of the machine learning frameworks, JavaScript hasn’t been left behind. JavaScript developers have been using various frameworks for training and deploying machine learning models in the browser.
Here are the five trending open source machine learning frameworks in JavaScript.
1. TensorFlow.js
TensorFlow.js is an open source library that allows you to run machine learning programs completely in the browser. It is the successor of Deeplearn.js, which is no longer supported. TensorFlow.js improves on the functionalities of Deeplearn.js and empowers you to make the most of the browser for a deeper machine learning experience.
With the library, you can use versatile and intuitive APIs to define, train, and deploy models from scratch right in the browser. Furthermore, it automatically offers support for WebGL and Node.js.

If you have pre-existing trained models you want to import to the browser, TensorFlow.js will allow you do that. You can also retrain existing models without leaving the browser.
2. Machine learning tools
The machine learning tools library is a compilation of resourceful open source tools for supporting widespread machine learning functionalities in the browser. The tools provide support for several machine learning algorithms, including unsupervised learning, supervised learning, data processing, artificial neural networks (ANN), math, and regression.
If you are coming from a Python background and looking for something similar to Scikit-learn for JavaScript in-browser machine learning, this suite of tools could have you covered.
3. Keras.js
Keras.js is another trending open source framework that allows you to run machine learning models in the browser. It offers GPU mode support using WebGL. If you have models in Node.js, you’ll run them only in CPU mode. Keras.js also offers support for models trained using any backend framework, such as the Microsoft Cognitive Toolkit (CNTK).
Some of the Keras models that can be deployed on the client-side browser include Inception v3 (trained on ImageNet), 50-layer Residual Network (trained on ImageNet), and Convolutional variational auto-encoder (trained on MNIST).
4. Brain.js
Machine learning concepts are very math-heavy, which may discourage people from starting. The technicalities and jargons in this field may make beginners freak out. This is where Brain.js becomes important. It is an open source, JavaScript-powered framework that simplifies the process of defining, training, and running neural networks.
If you are a JavaScript developer who is completely new to machine learning, Brain.js could reduce your learning curve. It can be used with Node.js or in the client-side browser for training machine learning models. Some of the networks that Brain.js supports include feed-forward networks, Ellman networks, and Gated Recurrent Units networks.
5. STDLib
STDLib is an open source library for powering JavaScript and Node.js applications. If you are looking for a library that emphasizes in-browser support for scientific and numerical web-based machine learning applications, STDLib could suit your needs.
The library comes with comprehensive and advanced mathematical and statistical functions to assist you in building high-performing machine learning models. You can also use its expansive utilities for building applications and other libraries. Furthermore, if you want a framework for data visualization and exploratory data analysis, you’ll find STDLib worthwhile.
Conclusion
If you are a JavaScript developer who intends to delve into the exciting world of machine learning or a machine learning expert who intends to start using JavaScript, the above open source frameworks will intrigue you.
Do you know of another open source library that offers in-browser machine learning capabilities? Please let us know in the comment section below.

"
JavaScript$interview,"You don't know JavaScript, but you should
Thank you all for having me. I'm Kyle Simpson, known as ""getify"" online on Twitter, GitHub, and all the other places that matter. I was here in Rochester teaching a workshop for the Thought @ Work conference this past weekend, and figured I'd stick around to check out some JavaScript (JS) and Node classes here in the New Media Interactive Development program, so thank you for having me.
I have been writing a book series on JavaScript called You Don't Know JS. The entire series is being written in the open, up online on GitHub for free reading. They're also being professionally edited and published through O'Reilly. There are five titles planned for the series: two have already been published, the third is complete and in final editing, the fourth is almost complete, and the fifth one will commence soon.
Scope & Closures: Covers closure primarily, which is one of the most important foundational topics. All JS programs use closures, but most developers don't know that they're using it, or what to call it, or just how it works.
this & Object Prototypes: Covers the mystery of how the this keyword works, and then tackles the misconception that JS has classes—not true! Instead, JavaScript has prototype delegation, and we should embrace that rather than trying to fake class orientation.
Types & Grammar: Goes deep into coercion, the mechanism most people think is evil in JS. I encourage you to dig into it and learn it, because coercion not only isn't as bad or weird as you've been told, but it can actually help improve your code if you learn how to use it properly!
Async & Performance (in progress): Explains why callbacks for async programming are insufficient, then goes deep into promises and generators as much better async patterns. Also covers optimizing and benchmarking JS performance.
ES6 & Beyond (planned): Covering all the changes to JS coming in ES6, as well as forward looking to beyond-ES6 evolution on the horizon.
To understand the spirit of this series, compare it to JavaScript: The Good Parts by Douglas Crockford. His book was both good and bad for our community. It's almost single-handedly responsible for bringing lots of developers to (or back to!) the language and giving it serious attention. We owe a lot to him for that. But it also taught developers that there is only a small part of the language you need to learn. And because you only have to learn a little bit of it, that's all most developers ever learn. Even developers with 5 or 10 years JS experience know comparatively very little of the language.
My books are the opposite. They're the anti-""The Good Parts."" That doesn't mean they're the bad parts, it means they're all the parts. Rather than avoiding most of the language because one guy said to—rather than running away from the hard parts—I encourage you to run towards ""the tough parts"" and learn them. When you see something in JS that you don't understand or is confusing, instead of blaming the language as being poorly designed, turn your attention toward your own lack of understanding, and spend the effort to increase your understanding.
This is somewhat unique to JS developers, that they expect a language should be so simple and intuitive that merely glancing at it should be enough to understand it, and that if they can't, it's a failure of the language. Expecting perfectly self-explanatory syntax and rules wouldn't be reasonable of any other language, like Java or C++. If you were confused by code, you wouldn't blame the designers of those languages. You'd blame either your own understanding, or at least that of the person who wrote the code. Either way, learning the language better is the best solution to that lack of understanding. Many times, when developers hate something about JS, it turns out it's because they simply don't understand it enough. When I explain how it works, many times they go from hating it to appreciating it—and by the way, appreciating doesn't mean liking, it just means respecting.
I believe JavaScript takes time to learn properly and completely, and that if you're going to write it, then you should invest that effort. You should understand why the code that you write works the way that it works. Instead of saying ""it works, but I don't care how,"" the most important question you can always ask is: ""How does it work, and WHY?"" I'm an open web evangelist who teaches JavaScript for a living. I work with developers all the time who've learned JS incompletely and improperly, and they're having to fight hard against the grain to re-learn it. That's why I'm so encouraged to see you learning JS in university. By learning JS properly here in school, you can graduate and come right into the industry as a new generation of developers that already understand and appreciate the importance of JS as the standard for the entire web platform.
JS is going to be the foundation of the web platform for the rest of our careers. We might as well get to know it better!
I'll leave you with this: I believe strongly, that the most important thing you can learn at university—of course, you're being taught lots of great stuff—but the most important is how to learn, and how to love and enjoy learning. You'll never find ""just one thing"" you love and do that for the rest of your career. The industry reinvents itself every couple of years. If nothing else, it'll just be Apple doing that. You have to be adept at learning and remastering new things. That's the path to success in your career, whatever interests you dig into.

Q&A session
Q: Five books, should they be read in a specific order?
A: Scope and Closures is in most demand and chronological release order is certainly OK. The first three are about the core of JavaScript. Four and five will be build upon the first three, but mostly deal with new things coming to the langauge as of ES6.
Q: How important is free and open source software in your work?
A: Everything about my career is open source. I believe very strongly in the power of open source, and its position in the future success of our industry. If you study the history of technologies, they start closed/proprietary, are shepherded through adoption and evolution, and eventually end up open. Ultimately, open always wins. But increasingly, I believe open should be the default mode. Many people say, ""I don't feel like I wanna put my stuff out, they'll make fun of my crappy code..."" And when I write code, people say ""you just have more confidence 'cause you're good."" But if you look at my old code, there is some terrible stuff in there. When I say ""you"" in ""You Don't Know JS"", that's a collective term. I don't know it either.
Every time I start writing code for a project, I start with an empty file, publicly, on GitHub. I do the best I can and am constantly evolving. But instead of just using GitHub as a platform for marketing my own code and ideas, I assume that every line of code I write is the worst, and the only way to get better is with the help of others. Open source collectively makes the best software better than any one person can make.
It is a culture you should strive for individually, and professionally. I believe very strongly, ""open"" is the reason why all this exists, and why the stuff we're doing now will still exist 10 years from now.
Q: I'm in that camp where I am afraid to code publicly. Where do I start?
A: My perspective—and there are different answers—is to seek out others' projects. There is a lot of FOSS contribution that isn't about code. Docs are usually left to the end of a project, and are neglected, but it is critically important they are up to date. If you can read others' code, and add details, examples, or tests, that is a super important contribution you can make. Many of ""the rockstars"" in FOSS got there by just pitching in and started with docs/tests. Some projects go the extra mile, and identify ""low hanging fruit"" or bugs that are known to have simple solutions. It is a great place to start with, and you can learn about how the project works. Even providing bug reports is a way you can contribute without writing your first line of code. But even one line of code is important. Someone after you can learn from it.
Q: Where?
A: GitHub is the de facto standard. Any community is fine, sure, and I wouldn't say ""pick this project."" You should pick a project that is interesting to YOU. If you are into  data visualizations, get into D3. Find what you are passionate about. If you do, you'll quickly build your confidence, and that will create a virtuous cycle of making both the people and code, better.
Q: You said that you think JS will be the ""only language for the web"" for our careers? I'm not necessarily a supporter of Dart, or other similar languages, but do you not expect those to succeed?
A: Great question, and loaded, but... Dart isn't going to succeed in replacing JavaScript, not because it is bad or poorly designed, but because of how Google is going about it. Going beyond what they say on their site, they've positioned it to compete against JS, in hopes of replacing it, rather than being a language that experiments with things to intentionally inform and influence the future of JS. From the original ""leaked memo"" where the world learned about Dart in terms of ""fundamental flaws [in JavaScript] that cannot be fixed,"" to the Dartium VM they're building in Chrome to sit alongside JS, to the Dart2JS transpiler—the messaging is unclear and smells of not just being a ""better compiling JS lang,"" but more an attempt to hope JS declines if developers can just write Dart in the web natively. I can tell you this for sure: Mozilla will never implement Dart in Firefox. Unless there is a future where Firefox doesn't exist, which I cannot imagine, Dart will not replace JS.
In a bigger sense, there are hundreds of languages that you can compile into JS. You want to run your code on the web, so can ""transpile"" it into JS. I don't like most of those languages personally, but they are all super important! Source code, is not for a computer! There are an infinite number of ways to write code to produce the 1s and 0s. Source code is for the developer, and you need to find the language that works the best with your brain. Also, we need more experimentation, and more Compile-to-JS languages, like CoffeeScript, which influenced many great things being added to JS in ES6. The future, I think may be limited for CoffeeScript itself, but that's OK because it was very important to evolve JS forward. As far as Typescript, I don't like classes, but Eich is on record saying there may be something like the type annontations in the future of JS.
Learn JS first, but as you go about your career, you'll find other languages that work better for certain problems or teams. Many people do that because they don't wanna learn JS, but that is the wrong way of going about it. Once you really know JS, then it's totally OK and healthy for you to find other languages that you prefer that will use JS as their compliation target. That's great for the future of the web platform.
"
API$opensource,"3 open source alternatives to Google Maps API
Every year on the third Wednesday of November, map geeks around the world (myself included) celebrate GIS Day. Short for geographic information systems, or occasionally geospatial information science, GIS is all about using computer systems to collect, store, analyze, and display geographic data, or really any data that has a spatial component.

The past decade has changed GIS immensely. With the advent of data mining, machine learning, mobile applications, the Internet of Things, social media, and other recent additions to the computing landscape, there are orders of magnitude more data available across every discipline, and the race to do useful and interesting things is in full throttle. Once limited to more traditional realms like land records, agriculture, natural resources, and urban planning, GIS now permeates practically every field.


But how does one get started exploring the vast world of GIS? Before exploring advanced modeling, spatial analytics, and data management, the most logical place to begin learning about GIS is with the most basic representation of geographical data: a map. And if you come to GIS with any background in basic web programming and markup—JavaScript, HTML, and CSS—a web map is probably the path of least resistance to getting started.
Many people familiar with Google Maps immediately turn to the Google Maps API for getting started, but Google Maps are far from the only option. In fact, there are many open source alternatives which are better fit to specific needs, whether those needs are displaying something very basic with minimal overhead, or a complex application with many diverse components and integrations.
If you're looking to get started with web mapping, here are three libraries which are worth checking out.
Leaflet
Leaflet has in the past couple of years become one of the most popular options for creating interactive JavaScript maps. It's basic library is fairly small, making Leaflet a great option for mobile applications or other situations in which load time or size is at a premium. But it also has a ton of available plugins so that you can add on just about any functionality available with a heaftier mapping library.
Leaflet also has a strong documentation project behind it, making it a good choice for beginners, and there are a number of community-contributed examples out there on the Internet for when you get stuck, as well as many examples on the project homepage. Leaflet's source code is available on GitHub and it is licensed under a BSD license.
Modest Maps
Though Leaflet can be considered a minimalist library, Modest Maps may win an award for being even smaller. Also very modular in design, Modest Maps is a great choice for both simple maps and for people who wish to pick-and-choose components. Check out their example library for more ideas of what you might do with it.
Modest Maps is open source under a BSD license, and you can view its source on GitHub as well.
Polymaps
Polymaps is a JavaScript mapping library for creating interactive maps with both SVG-based vectors as well as tile-based maps for raster data. The nice thing about using SVG (scalable vector graphics) for creating maps is that it means that many of the same styling options which can be applied to web documents with CSS will also work with your map design. I also find that Polymaps handles zooming incredibly well compared to some other libraries
Like othe others above, Polymaps is open source under a BSD license, and you can view its source on GitHub.
Others
Of course, the three we looked at aren't the only options. There are many others, and depending on what your specific needs are, you should probably spend some time exploring all of the options before making a decison of where to get started. Some others I like include:
OpenLayers, which you might think of as the kitchen sink of web map programming. I often use OpenLayers when working with GeoServer; if you use the open source OpenGeo Suite for building a complete GIS solution, all of the components integrate nicely.
Mapael, which is a jQuery library for building attractive map visualizations with vector data.
D3.js, which is a more general-purpose JavaScript visualization library, but has some great features for creating simple maps easily. If you're more interested in the broader world of data visualization, D3 is a great place to start.
Cesium, which is very specifically meant for creating WebGL visualizations of data mapped onto 3D globes. If this interests you, some of the demos are amazing.
For others, the OpenGeo Foundation provides a fairly comprehensive list of web mapping tools, but be warned, they are in various states of scope and production-readiness.
The nice thing about using these libraries is that they encourage mixing and matching to style maps perfectly for your needs. Don't like the default map layer which sits underneath your map? Pick a different one. Prefer a different icon for representing points on your map? No problem.
Is there a favorite web mapping API we didn't mention here that you think would be a good fit for beginners? Let us know in the comments below. And have a happy GIS Day!

"
JavaScript,"9 resources to get started coding with JavaScript
If you're new to JavaScript, check out these resources to get started quickly.
JavaScript is the lingua franca of the Internet. Like English, it's a language that language snobs love to hate, and it's spoken everywhere. As Anders Hejlsberg said: ""JavaScript is the only true cross-platform game in town.""
Written in just 10 days by Brendan Eich back in 1993, JavaScript has evolved and expanded to fill every computing niche. You can run JavaScript on the server and on desktop computers. You can use JavaScript to build server applications using Node.js. You can use it to build mobile apps with Ionic and desktop apps with Electron. If you want to do something really old school, you can even use it to build web applications!
Ready to get started coding with JavaScript? Here are nine resources that can help.
1. Google Chrome
The current Cambrian explosion of JavaScript was triggered by Google's Chrome browser. In 2008, after years of stagnation in web browser technology, Google released a revolutionary browser called Chrome along with an awesome comic book that you can still access.
The Chrome browser increased the speed of JavaScript by up to 100x, and it had a modular engine—V8—that could be used separately from the browser. The V8 engine became the basis for Node.js and server-side JavaScript.
The other thing that Google Chrome introduced is a powerful suite of developer tools. To access them, follow these instructions:
For MacOS, open the Google Chrome browser and choose View > Developer > Developer Tools.
For Windows, hit Ctrl-Shift-I. Developer tools will open in a window to the right of the webpage you're on. After you get there (via either method), you have to click ""Console"" at the top of the Developer tools window to get to the Console.
This will open the Chrome Developer tools. In the Console, type the following:
alert('Hello World!');
Now hit the Enter key on your keyboard. You'll see a pop-up window that says: ""Hello World!""
Welcome to the JavaScript REPL—the Read-Eval-Print Loop. You can use this to explore and test JavaScript with immediate results. It's a powerful and useful tool that will become your staple for developing and debugging JavaScript.
2. repl.it
On the subject of REPL, repl.it is an online REPL that includes lessons in JavaScript. You can code in your browser without having to install any tools and immediately test your code.
3. @JavaScriptDaily
Follow @JavaScriptDaily on Twitter. Don't try to understand everything on this daily Twitter account, which focuses on JavaScript news and events; just let it flow over you and change your perspective over time.
4. Eric Elliott
Eric Elliott blogs prolifically about JavaScript on Medium.com. He is a longtime software developer with a well-considered perspective and a commitment to the JavaScript space.
5. FunFunFunction
Mattias Petter Johansson (mpj for short) has a YouTube channel about JavaScript called FunFunFunction. It's equal parts entertaining and educational. Start at the beginning of his episodes.
6. Visual Studio Code
Microsoft Visual Studio Code is a cross-platform code editor that is written in JavaScript and uses Electron to provide a cross-platform desktop app. It has first-class support for JavaScript and also supports Microsoft's TypeScript language, which adds static typing to JavaScript.
7. CoderDojo/FreeCodeCamp/Meetup
Hanging out with people who are coding in JavaScript IRL (in real life) is a great way to pick up enthusiasm and knowledge. There are free coding groups in most cities—from CoderDojos (you can learn while you teach kids), to FreeCodeCamp groups, to the local JavaScript Meetup.
8. GitHub
GitHub is the ""Facebook of coding."" People share code on GitHub like they share photos on Facebook. When you learn to use GitHub, you'll tap into a rich source of learning and support. The GitHub tutorial is an easy introduction to it.
9. Magikcraft
Magikcraft is a way to code JavaScript in Minecraft. It's a fun way to learn, and you can get immediate, dramatic results. It's especially good for people (of any age) who play Minecraft."
JavaScript$presentations,"3 tools to make creating presentations easy
In recent years, there has been a proliferation of JavaScript presentation frameworks. These frameworks use HTML5, CSS3, and JavaScript to create presentation slides that can be viewed in any modern web browser. Gone are the days of being tied to using PowerPoint, nowadays there are a plethora of tools to choose from when it comes to creating a presentation.
There are many different presentation frameworks you can choose from, each with its own strengths and weaknesses. Last year, Opensource.com Community Moderator Luis Ibáñez covered reveal.js, a very nice framework that creates wonderful, but rather traditional, presentations. A few months ago, I wrote about impress.js, which creates Prezi-like presentations that rotate, zoom, and scale in three dimensions. However, impress.js is a very complex tool to use. Today, I am going to share with you Bespoke.js, a presentation framework that sits at the opposite end of the complexity scale.
Bespoke.js
Mark Dalgleish's Bespoke.js is a super lightweight presentation framework (released under an MIT License) that can be expanded by plugins. The minimized version of the core library is only (approximately) one kilobyte. This core library provides enough features needed to create a basic presentation, while a large number of plugins provide a wide variety of extra features. Granted, you are almost guaranteed to need at least one plugin because navigating a presentation using keyboard input is a function provided a plugin. That said, Bespoke.js plus the keyboard input plugin is still far smaller than most JavaScript presentation frameworks.
The official Bespoke.js plugins provide a wide variety of features. In addition to the aforementioned plugin for keyboard interaction, there are plugins for touch input, responsive scaling, animated bullet lists, syntax highlighting for code examples, and several other advanced features. There are also two official Bespoke.js themes (which are basically plugins that change how presentations are displayed), Cube, and Voltaire. Both themes are polished and not too flashy, though you might want to change the colors to match your needs, but they are both perfectly usable as is.
Bespoke.js plugins and themes, as well as Bespoke.js itself, can be installed using node.js's npm or Twitter's Bower.
To install Bespoke.js or a plugin using npm:
npm install bespoke or npm install bespoke-[plugin]
To install Bespoke.js or a plugin using bower:
bower install bespoke.js or bower install bespoke-[plugin]
It is also possible to get Bespoke.js and its plugins by cloning the projects' GitHub repositories, or by downloading the files from the projects' GitHub pages. However, there is a much easier way to get started, the Bespoke.js Generator project.
Bespoke Generator
The quickest way to start making a Bespoke.js presentation is to use Bespoke.js Generator. The generator will allow you to set the title of your presentation and will walk you through a series of questions about which plugins you would like to use. It will then automatically download all the required Bower components and Node.js modules. After the generator is done, you will be able to start editing your presentation.
To install the Bespoke.js Generator:
1. If you don't have npm installed, download and install it from the node.js site, or by installing npm using your distribution's package manager.
2. Once npm is installed, run: npm install -g generator-bespoke using an account with sufficient privileges to install software packages (e.g., root.)
To create a presentation, make a new directory and then change to that directory:
mkdir presentation-directory
cd presentation-directory
Then start Bespoke Generator and create your presentation by running:
yo bespoke
All the files you need to edit will be located in the src sub-directory. The basic structure and content of a Bespoke.js presentation are created by editing the Jade markup in the index.jade file. Styles are edited by modifying the styles/main.styl file, which makes use of Stylus to make writing CSS rules easier. Finally, the scripts/main.js file is where you can edit the JavaScript for your presentation.
To build your presentation, or to create a local server with LiveReload, you will use gulp.js. The following commands will allow you to build a static copy of presentation, start a local server, or push your project to GitHub Pages (assuming the presentation is already set up as a Git repository with 'origin' pointing to GitHub.)
To build a static site (the files end up in the dist sub-directory): gulp
To push to GitHub Pages: gulp deploy
To start a local server for testing: gulp serve
For advanced users who want to create plugins and themes, the Bespoke.js Plugin Generator and Bespoke.js Theme Generator are also available. Like all of the other official Bespoke.js projects, they are released under an MIT License.
Strut
I previously covered Strut in my article about impress.js, but it can also create Bespoke.js presentations. Strut is a web-based presentation creation tool that behaves very much like traditional presentation software. While not as feature complete as standalone presentation software (e.g., PowerPoint), Strut has more than enough features to make creating a presentation a quick and easy process. The left side of the interface lets you create and sort slides. The toolbar at the top of the screen lets you add text, shapes, and images, as well as embed videos and websites. You can quickly and easily change the background color of a single slide or all the slides at once. It is also possible to change the background color for the canvas behind the slides in the same manner. For power users, Strut supports adding custom CSS classes to objects and lets you write new CSS rules from inside Strut. It is also possible to add text to a slide using Markdown syntax, for users that prefer writing out a presentation instead of using drag and drop tools.
The only difference between creating a impress.js presentation and a Bespoke.js presentation in Strut is what happens when you go into Strut's overview mode. For impress.js it allows you to position and rotate the slides on a 3D canvas, while the overview mode for Bespoke.js lets you select from several side transitions. There are eight different transitions ranging from a simple fade between slides to coverflow and rotating cube options. All the transitions are nice and rather understated. Using any of them would bring a little polish to a presentation without being so flashy that the transitions end up distracting from the presentation's content.
You can try out Strut using the example site, or you can clone or download the project from GitHub and build Strut to install on your own server (or run it locally) using Grunt. However, a new version of Strut—Strut2.0—is being developed in a private repository and should be publicly available in early October, so keep an eye out for that.
"
node.js$M,"Node.js integrates with M: Next big thing in healthcare IT
Join the M revolution and the next big thing in healthcare IT: the integration of the node.js programming language with the NoSQL hierarchical database, M.
M was developed to organize and access with high efficiency the type of data that is typically managed in healthcare, thus making it uniquely well-suited for the job.
One of the biggest reasons for the success of M is that it integrates the database into the language in a natural and seamless way. The growth and involvement of th community of M developers however, has been below the radar for educators and the larger IT community. As a consequece it has been facing challenges for recruiting young new developers, despite the critical importance of this technology for supporting the Health IT infrastructure of the US.
At the recent 26th VistA Community Meeting, an exciting alternative was presented by Rob Tweed. I summarize it as: Node.js meets the M Database.
In his work, Rob has created an intimate integration between the M database and the language features of node.js. The result is a new way of accessing the M database from Javascript code in such a way that the developer doesn't feel that is accessing a database.
It is now possible to access M from node.js, both when using theM implementation of Intersystems Cache and with the open source M implementation of GT.M. This second interface was implemented by David Wicksell, based on the API previously defined for Cache in the GlobalsDB project.
In a recent blog post, Rob describes some of the natural notation in node.js that provides access to the M hierarchical database by nicely following the language patterns of Javascript. Here are some of Rob's examples:
The M expression:
set town = ^patient(123456, ""address"", ""town"")
becomes the Javascript expression:
var town = patient.$('address').$('town')._value;
with some flavor of jQuery.
The following M expression of a healthcare typical example:
^patient(123456,""birthdate"")=-851884200 ^patient(123456,""conditions"",0,""causeOfDeath"")="""" ^patient(123456,""conditions"",0,""codes"",""ICD-10-CM"",0)=""I21.01"" ^patient(123456,""conditions"",0,""codes"",""ICD-9-CM"",0)=""410.00"" ^patient(123456,""conditions"",0,""description"")=""Diagnosis, Active: Hospital Measures - AMI (Code List: 2.16.840.1.113883.3.666.5.3011)"" ^patient(123456,""conditions"",0,""end_time"")=1273104000
becomes the following JSON datastructure that can be manipulated with Javascript:
var patient = new ewd.GlobalNode(""patient"", [123456]); patient._delete(); var document = { ""birthdate"": -851884200, ""conditions"": [ { ""causeOfDeath"": null, ""codes"": { ""ICD-9-CM"": [ ""410.00"" ], ""ICD-10-CM"": [ ""I21.01"" ] }, ""description"": ""Diagnosis, Active: Hospital Measures - AMI (Code List: 2.16.840.1.113883.3.666.5.3011)"", ""end_time"": 1273104000 } ] };
More detailed examples are provided in Rob's blog post. The M module for node.js is available here.
What this achieves is seamless integration between the powerful M hierarchical database and the language features of the very popular node.js implementation of Javascript. This integration becomes a great opportunity for hundreds of node.js developers to join the space of healthcare IT, and to do, as Tim O'Reilly advises: Work on Stuff that Matters!
M is currently being used in hundreds of hospitals in the public sector:
The Department of Veterans Affairs
The Department of Defense
The Indian Health Service
As well as hundreds of hospitals in the private sector:
Kaiser Permanente hospital system
Johns Hopkins
Beth Israel Deaconess Medical Center
Harvard Medical School
In particular at deploments of these EHR systems:
Epic
GE/Centricity
McKesson
Meditech
Given this, and the large popularity of Javascript and the high efficiency of node.js, this may be the most significant event happening in healthcare IT in recent years.
If you are an enthusiast of node.js, or you are looking for the best next language to learn, or you want to do some social good, this could be the thing for you.
"
JavaScript$framework,"When not to use a JavaScript framework
Helpful hints on where frameworks make sense, and where they don’t.
As the internet has evolved, web development has grown well beyond its intended abilities—for good and bad. To smooth over the rougher edges, web developers have invented a plethora of frameworks, both small and not so small. This has been good for developers, because browser fragmentation and standards problems abound, especially for those who want new features in APIs and more unified syntax for those features. Plus, for the most part the frameworks have been open source, which is great for everyone.
Now, seeing that those rough edges have been worn down by time and aren't as sharp as they once were, we should probably reduce our use of some of the frameworks we created. In other ways, we simply need to consider the cost of using a framework for the given task.
Things we can do ourselves
Consider the humble HTTP request, once a good 50-line function for a simple GET that worked both in Firefox and Internet Explorer. For example, here is a simple function that does a POST; we used it in production in Phone Janitor for more than a year as our main React data pump:
function postMe(name, data, callback, onError) {
    var request = new XMLHttpRequest();
    request.onreadystatechange = function() {
        if (request.readyState != 4 || request.status != 200) { return; }
        var body = JSON.parse(request.responseText);
        if (body.error) {
            onError(body.error);
        }
        else {
            callback.(body);
        }
    };
    request.open(""POST"", '/api/' + name, true);
    request.setRequestHeader(""Content-type"", ""application/json"");
    request.send(JSON.stringify(data));
}
This framework-free code could easily be rewritten to work with Promises, be adaptable for request types, or support any number of features that might be critical to your application. Is it well-engineered? Maybe not. Is it robust? It was for our needs at the time. The takeaway is less about what it is or isn't, and more about why I would use anything else.
If I don't want to write my own HTTP request engine, there is a cornucopia of options. They all come with a cost, though. How big are they? How can I include them in my code, and how does it affect my workflow? What other unnecessary things are they doing that waste time in execution? If I spent an hour (which is about what we spent on the code and testing) implementing this function to meet all my needs, would that save a lot of time compared to what it would take to integrate a library to do the same thing? Each of us will have different answers.
All things to all people
We consume services that try to be many things for a variety of use cases. This is really the crux of the issue. It is a good thing to unify APIs for the community's benefit, because some things are nuanced and hard to do alone. jQuery was invented because browsers did things wildly differently, and the JavaScript API didn't have much to it. There was a time, though, that every web dev included jQuery just so they could select document object module (DOM) elements for simple innerHTML, and it made a noticeable impact on page-load times. Consider that use case now:
// Author's note, this is mostly for example, don't manipulate DOM unless you know what that means for your app

var el1 = document.getElementById(id_Name);
var el2 = document.getElementsByClass(class_Name); // returns array
var el3 = document.querySelector(""div.user-panel.main input[name='login']"");

// Want it in a jquery style function name?

var $ = document.querySelector; // Or get fancier if you wish
var el4 = $(""div.user-panel.main input[name='login']"");
We still include jQuery for this in a lot of places  (e.g., the average WordPress theme). Don't take my word for it; feel free to look and see where it's included or not. There isn't much we can't do these days without it.
Even if we use frameworks
This is not just a matter of how and when we use frameworks, though; it's also about how we approach features and add-ons. For instance, consider Google Visualization integration into the Angular framework. At MobilSense, we rely heavily on charts for reporting to management teams—but we also use Angular 1.5. So how do we integrate updating functionality into our app's charts?
Our options were using angular-google-chart or developing our own solution. Although angular-google-chart is a fantastic library—I have used it elsewhere and I appreciate the author supporting his project for free—we decided to do our own for some now obvious reasons. Here's how they stacked up:

Our own solution does not handle all the cases the library does. We don't need those cases, and if we did, we could probably add them pretty easily and in a way that is portable to our workflows and other frameworks. This is the type of tradeoff we each need to decide based on our own specific needs. There is no shame in either choice.
When we should and shouldn't use frameworks
I strongly advocate for knowing the purpose in coding a given tool. If our goal is a quickly cobbled-together thing that exists temporarily, engineering it well probably doesn't matter. If we're looking for longevity, I think using framework tools is something we need to weigh more heavily. A framework is hard to transition away from, especially if we add in libraries that further tie us into that framework.
If it will take just a day or two to write your own solution, I would lean toward that. If it will take a week or more, the considerations may change.
Another good reason to write your own would be if it would be costly to couple yourself to a framework that may not be around for the life of your project. However, if it is a really complicated thing, such as integrating PDF support, you probably don't want to consider writing your own, as in that way lies madness.
As with any type of software engineering, consider your work like construction. If you're building a doghouse, whatever you do is probably fine. If you're building a skyscraper, you've got to do more planning. Where do we draw the line between them? The role of frameworks is the same as the role of the materials and styles of construction you are building. Does it fit the environment, and can we get replacement materials later on when we need them? Although your decisions are your own, I hope this information and these examples will help guide you.
"
Go$Golang,"4 tips for learning Golang
Arriving in Golang land: A senior developer's journey.
In the summer of 2014...
IBM: ""We need you to go figure out this Docker thing.""
Me: ""OK.""
IBM: ""Start contributing and just get involved.""
Me: ""OK."" (internal voice): ""This is written in Go. What's that?"" (Googles) ""Oh, a programming language. I've learned a few of those in my career. Can't be that hard.""
My university's freshman programming class was taught using VAX assembler. In data structures class, we used Pascal—loaded via diskette on tired, old PCs in the library's computer center. In one upper-level course, I had a professor that loved to show all examples in ADA. I learned a bit of C via playing with various Unix utilities' source code on our Sun workstations. At IBM we used C—and some x86 assembler—for the OS/2 source code, and we heavily used C++'s object-oriented features for a joint project with Apple. I learned shell scripting soon after, starting with csh, but moving to Bash after finding Linux in the mid-'90s. I was thrust into learning m4 (arguably more of a macro-processor than a programming language) while working on the just-in-time (JIT) compiler in IBM's custom JVM code when porting it to Linux in the late ‘90s.
Fast-forward 20 years... I'd never been nervous about learning a new programming language. But Go felt different. I was going to contribute publicly, upstream on GitHub, visible to anyone interested enough to look! I didn't want to be the laughingstock, the Go newbie as a 40-something-year-old senior developer! We all know that programmer pride that doesn't like to get bruised, no matter your experience level.

My early investigations revealed that Go seemed more committed to its ""idiomatic-ness"" than some languages. It wasn't just about getting the code to compile; I needed to be able to write code ""the Go way.""
Now that I'm four years and several hundred pull requests into my personal Go journey, I don't claim to be an expert, but I do feel a lot more comfortable contributing and writing Go code than I did in 2014. So, how do you teach an old guy new tricks—or at least a new programming language? Here are four steps that were valuable in my own journey to Golang land.
1. Don't skip the fundamentals
While you might be able to get by with copying code and hunting and pecking your way through early learnings (who has time to read the manual?!?), Go has a very readable language spec that was clearly written to be read and understood, even if you don't have a master's in language or compiler theory. Given that Go made some unique decisions about the order of the parameter:type constructs and has interesting language features like channels and goroutines, it is important to get grounded in these new concepts. Reading this document alongside Effective Go, another great resource from the Golang creators, will give you a huge boost in readiness to use the language effectively and properly.
2. Learn from the best
There are many valuable resources for digging in and taking your Go knowledge to the next level. All the talks from any recent GopherCon can be found online, like this exhaustive list from GopherCon US in 2018. Talks range in expertise and skill level, but you can easily find something you didn't know about Go by watching the talks. Francesc Campoy created a Go programming video series called JustForFunc that has an ever-increasing number of episodes to expand your Go knowledge and understanding. A quick search on ""Golang"" reveals many other video and online resources for those who want to learn more.
Want to look at code? Many of the most popular cloud-native projects on GitHub are written in Go: Docker/Moby, Kubernetes, Istio, containerd, CoreDNS, and many others. Language purists might rate some projects better than others regarding idiomatic-ness, but these are all good starting points to see how large codebases are using Go in highly active projects.
3. Use good language tools
You will learn quickly about the value of gofmt. One of the beautiful aspects of Go is that there is no arguing about code formatting guidelines per project—gofmt is built into the language runtime, and it formats Go code according to a set of stable, well-understood language rules. I don't know of any Golang-based project that doesn't insist on checking with gofmt for pull requests as part of continuous integration.
Beyond the wide, valuable array of useful tools built directly into the runtime/SDK, I strongly recommend using an editor or IDE with good Golang support features. Since I find myself much more often at a command line, I rely on Vim plus the great vim-go plugin. I also like what Microsoft has offered with VS Code, especially with its Go language plugins.
Looking for a debugger? The Delve project has been improving and maturing and is a strong contender for doing gdb-like debugging on Go binaries.
4. Jump in and write some Go!
You'll never get better at writing Go unless you start trying. Find a project that has some ""help needed"" issues flagged and make a contribution. If you are already using an open source project written in Go, find out if there are some bugs that have beginner-level solutions and make your first pull request. As with most things in life, the only real way to improve is through practice, so get going.
And, as it turns out, apparently you can teach an old senior developer new tricks—or languages at least."
Go$LocksSersusChannels,"Locks versus channels in concurrent Go
Compare two ways to share information with goroutines, one using synchronized shared memory and the other using channels.
Go has popularized the mantra don't communicate by sharing memory; share memory by communicating. The language does have the traditional mutex (mutual exclusion construct) to coordinate access to shared memory, but it favors the use of channels to share information among goroutines.
In this article, a short look at goroutines, threads, and race conditions sets the scene for a look at two Go programs. In the first program, goroutines communicate through synchronized shared memory, and the second uses channels for the same purpose. The code is available from my website in a .zip file with a README.
Threads and race conditions
A thread is a sequence of executable instructions, and threads within the same process share an address space: Every thread in a multi-threaded process has read/write access to the very same memory locations. A memory-based race condition occurs if two or more threads (at least one of which performs a write operation) have uncoordinated access to the same memory location.
Consider this depiction of integer variable n, whose value is 777, and two threads trying to alter its contents:
On a multiprocessor machine, the two threads could execute literally at the same time. The impact on variable n is then indeterminate. It's critical to note that each attempted update consists of two machine-level operations: an arithmetic operation on n's current value (either adding or subtracting 10), and a subsequent assignment operation that sets n to a new value (either 787 or 767).
The paired operations executed in the two threads could interleave in various inappropriate ways. Consider the following scenario, with each numbered item as a single operation at the machine level. For simplicity, assume that each operation takes one tick of the system clock:
Thread1 does the addition to compute 787, which is saved in a temporary location (on the stack or in a CPU register).
Thread2 does the subtraction to compute 767, also saved in a temporary location.
Thread2 performs the assignment; the value of n is now 767.
Thread1 performs the assignment; the value of n is now 787.
By coming in last, Thread1 has won the race against Thread2. It's clear that improper interleaving has occurred. Thread1 performs an addition operation, is delayed for two ticks, and then performs the assignment. By contrast, Thread2 performs the subtraction and subsequent assignment operations without interruption. The fix is clear: The arithmetic and assignment operations should occur as if they were a single, atomic operation. A construct such as a mutex provides the required fix, and Go has the mutex.
Go programs are typically multi-threaded, although the threading occurs beneath the surface. On the surface are goroutines. A goroutine is a green thread—a thread under the Go runtime control. By contrast, a native thread is directly under OS control. But goroutines multiplex onto native threads that the OS schedules, which means that memory-based race conditions are possible in Go. The first of two sample programs illustrates this.
MiserSpendthrift1
The MiserSpendthrift1 program simulates shared access to a bank account. In addition to main, there are two other goroutines:
The miser goroutine repeatedly adds to the balance, one currency unit at a time.
The spendthrift goroutine repeatedly subtracts from the balance, also one currency unit at a time.
The number of times each goroutine performs its operation depends on a command-line argument, which should be large enough to be interesting (e.g., 100,000 to a few million). The account balance is initialized to zero and should wind up as zero because the deposits and withdrawals are for the same amount and are the same in number.
Example 1. Using a mutex to coordinate access to shared memory
package main

import (
   ""os""
   ""fmt""
   ""runtime""
   ""strconv""
   ""sync""
)

var accountBalance = 0    // balance for shared bank account
var mutex = &sync.Mutex{} // mutual-exclusion lock

// critical-section code with explicit locking/unlocking
func updateBalance(amt int) {
   mutex.Lock()
   accountBalance += amt  // two operations: update and assignment
   mutex.Unlock()
}

func reportAndExit(msg string) {
   fmt.Println(msg)
   os.Exit(-1) // all 1s in binary
}

func main() {
   if len(os.Args) < 2 {
      reportAndExit(""\nUsage: go ms1.go <number of updates per thread>"")
   }
   iterations, err := strconv.Atoi(os.Args[1])
   if err != nil {
      reportAndExit(""Bad command-line argument: "" + os.Args[1]);
   }

   var wg sync.WaitGroup  // wait group to ensure goroutine coordination

   // miser increments the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()  // invoke Done on the WaitGroup when finished
      for i := 0; i < iterations ; i++ {
         updateBalance(1)
         runtime.Gosched()  // yield to another goroutine
      }
   }()

   // spendthrift decrements the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()
      for i := 0; i < iterations; i++ {
         updateBalance(-1)
         runtime.Gosched()  // be nice--yield
      }
   }()

   wg.Wait()  // await completion of miser and spendthrift
   fmt.Println(""Final balance: "", accountBalance)  // confirm final balance is zero
}
Flow-of-control in the MiserSpendthrift1 program (see above) can be described as follows:
The program begins by attempting to read and verify a command-line argument that specifies how many times (e.g., a million) the miser and the spendthrift should each update the account balance.
The main goroutine starts two others with the call: go func() { // either the miser or the spendthrift  The first of the two started goroutines represents the miser, and the second represents the spendthrift.
The program uses a sync.WaitGroup to ensure that the main goroutine does not print the final balance until the miser and the spendthrift goroutines have finished their work and terminated.
The MiserSpendthrift1 program declares two global variables, one an integer variable to represent the shared bank account and the other a mutex to ensure coordinated goroutine access to the account:
var accountBalance = 0    // balance for shared bank account
var mutex = &sync.Mutex{} // mutual-exclusion lock
The mutex code occurs in the updateBalance function to safeguard a critical section, which is a code segment that must be executed in single-threaded fashion for the program to behave correctly:
func updateBalance(amt int) {
   mutex.Lock()
   accountBalance += amt  // critical section
   mutex.Unlock()
}
The critical section is the statement between the Lock() and Unlock() calls. Although a single line in Go source code, this statement involves two distinct operations: an arithmetic operation followed by an assignment. These two operations must be executed together, one thread at a time, which the mutex code ensures. With the locking code in place, the accountBalance is zero at the end because the number of additions by 1 and subtractions by 1 is the same.
If the mutex code is removed, then the final value of the accountBalance is unpredictable. On two sample runs with the lock code removed, the final balance was 249 on the first run and -87 on the second, thereby confirming that a memory-based race condition occurred.
The mutex code's behavior deserves a closer look:
To execute the critical section code, a goroutine must first grab the lock by executing the mutex.Lock() call. If the lock is held already, then the goroutine blocks until the lock becomes available; otherwise, the goroutine executes the mutex-protected critical section.
The mutex guarantees mutual exclusion in that only one goroutine at a time can execute the locked code segment. The mutex ensures single-threaded execution of the critical section: the arithmetic operation followed by the assignment operation.
The call to Unlock() releases a held lock so that some goroutine (perhaps the one that just released the lock) can grab the lock anew.
In the MiserSpendthrift1 program, three goroutines (the miser, the spendthrift, and main) communicate through the shared memory location named accountBalance. A mutex coordinates access to this variable by the miser and the spendthrift, and main tries to access the variable only after both the miser and the spendthrift have terminated. Even with a relatively large command-line argument (e.g., five to 10 million), the program runs relatively fast and yields the expected final value of zero for the accountBalance.
The package sync/atomic has functions such as AddInt32 with synchronization baked in. For example, if the accountBalance type were changed from int to int32, then the updateBalance function could be simplified as follows:
func updateBalance(amt int32) {          // argument must be int32 as well
   atomic.AddInt32(&accountBalance, amt) // no explicit locking required
}
The MiserSpendthrift1 program uses explicit locking to highlight the critical-section code and to underscore the need for thread synchronization to prevent a race condition. In a production-grade example, a critical section might comprise several lines of source code. In any case, a critical section should be as short as possible to keep the program as concurrent as possible.
MiserSpendthrift2
The MiserSpendthrift2 program again has a global variable accountBalance initialized to zero, and again there are miser and spendthrift goroutines contending to update the balance. However, this program does not use a mutex to prevent a race condition. Instead, there is now a banker goroutine that accesses the accountBalance in response to requests from the miser and the spendthrift. These two goroutines no longer update the accountBalance directly. Here is a sketch of the architecture:
                  requests         updates
miser/spendthrift---------->banker--------->balance
This architecture, with support from a thread-safe Go channel to serialize requests from the miser and the spendthrift, prevents a race condition on the accountBalance.
Example 2. Using a thread-safe channel to coordinate access to shared memory
package main

import (
   ""os""
   ""fmt""
   ""runtime""
   ""strconv""
   ""sync""
)

type bankOp struct { // bank operation: deposit or withdraw
   howMuch int       // amount
   confirm chan int  // confirmation channel
}

var accountBalance = 0          // shared account
var bankRequests chan *bankOp   // channel to banker

func updateBalance(amt int) int {
   update := &bankOp{howMuch: amt, confirm: make(chan int)}
   bankRequests <- update
   newBalance := <-update.confirm
   return newBalance
}

// For now a no-op, but could save balance to a file with a timestamp.
func logBalance(current int) { }

func reportAndExit(msg string) {
   fmt.Println(msg)
   os.Exit(-1) // all 1s in binary
}

func main() {
   if len(os.Args) < 2 {
      reportAndExit(""\nUsage: go ms1.go <number of updates per thread>"")
   }
   iterations, err := strconv.Atoi(os.Args[1])
   if err != nil {
      reportAndExit(""Bad command-line argument: "" + os.Args[1]);
   }

   bankRequests = make(chan *bankOp, 8) // 8 is channel buffer size

   var wg sync.WaitGroup
   // The banker: handles all requests for deposits and withdrawals through a channel.
   go func() {
      for {
         /* The select construct is non-blocking:
            -- if there's something to read from a channel, do so
            -- otherwise, fall through to the next case, if any */
         select {
         case request := <-bankRequests:
            accountBalance += request.howMuch   // update account
            request.confirm <- accountBalance   // confirm with current balance
         }
      }
   }()

   // miser increments the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()  // invoke Done on the WaitGroup when finished
      for i := 0; i < iterations ; i++ {
         newBalance := updateBalance(1)
         logBalance(newBalance)
         runtime.Gosched()  // yield to another goroutine
      }
   }()

   // spendthrift decrements the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()
      for i := 0; i < iterations; i++ {
         newBalance := updateBalance(-1)
         logBalance(newBalance)
         runtime.Gosched()  // be nice--yield
      }
   }()

   wg.Wait()  // await completion of miser and spendthrift
   fmt.Println(""Final balance: "", accountBalance) // confirm the balance is zero
}
The changes in the MiserSpendthrift2 program can be summarized as follows. There is a BankOp structure:
type bankOp struct { // bank operation: deposit or withdraw
   howMuch int       // amount
   confirm chan int  // confirmation channel
}
that the miser and the spendthrift goroutines use to make update requests. The howMuch field is the update amount, either 1 (miser) or -1 (spendthrift). The confirm field is a channel that the banker goroutine uses in responding to a miser or a spendthrift request; this channel carries the new balance back to the requester as confirmation. For efficiency, the address of a bankOp structure, rather than a copy of it, is sent over the bankRequests channel, which is declared as follows:
var bankRequests chan *bankOp // channel of pointers to a bankOp
Channels are synchronized—that is, thread-safe—by default.
The miser and the spendthrift again call the updateBalance function in order to change the account balance. This function no longer has any explicit thread synchronization:
func updateBalance(amt int) int {   // request structure
   update := &bankOp{howMuch: amt,
                     confirm: make(chan int)}
   bankRequests <- update           // send request
   newBalance := <-update.confirm   // await confirmation
   return newBalance                // perhaps to be logged
}
The bankRequests channel has a buffer size of eight to minimize blocking. The channel can hold up to eight unread requests before further attempts to add another bankOp pointer are blocked. In the meantime, the banker goroutine should be processing the requests as they arrive; a request is removed automatically from the channel when the banker reads it. The confirm channel is not buffered, however. The requester blocks until the confirmation message—the updated balance stored locally in the newBalanace variable—arrives from the banker.
Local variables and parameters in the updateBalance function (update, newBalance, and amt) are thereby thread-safe because every goroutine gets its own copies of them. The channels, too, are thread-safe so that the body of the updateBalance function no longer requires explicit locking. What a relief for the programmer!
The banker goroutine loops indefinitely, awaiting requests from the miser and spendthrift goroutines:
for {
   select {
   case request := <-bankRequests:      // Is there a request?
      accountBalance += request.howMuch // If so, update balance and
      request.confirm <- accountBalance // confirm to requester
   }
   // other cases could be added (e.g., golf outings)
}
While the miser and spendthrift goroutines are still active, only the banker goroutine has access to the accountBalance, which means that a race condition on this memory location cannot arise. Only after the miser and spendthrift finish their work and terminate does the main goroutine print the final value of the accountBalance and exit. When main terminates, so does the banker goroutine.
Locks or channels?
The MiserSpendthrift2 program adheres to the Go mantra by favoring channels over synchronized shared memory. To be sure, locked memory can be tricky. The mutex API is low-level and thus prone to mistakes such as locking but forgetting to unlock—with deadlock as a possible result. More subtle mistakes include locking only part of a critical section (underlocking) and locking code that does not belong to a critical section (overlocking). Thread-safe functions such as atomic.AddInt32 reduce these risks because the locking and unlocking occur automatically. Yet the challenge remains of how to reason about low-level memory locking in complicated programs.
The Go mantra brings challenges of its own. If the two miser/spendthrift programs are run with a sufficiently large command-line argument, the contrast in performance is noteworthy. The mutex may be low-level, but it performs well. Go channels are appealing because they provide built-in thread safety and encourage single-threaded access to shared critical resources such as the accountBalance in the two sample programs. Channels, however, incur a performance penalty compared to mutexes.
It's rare in programming that one tool fits all tasks. Go accordingly comes with options for thread safety, ranging from low-level locking through high-level channels.

"
OpenStack$edgecomputing,"Project teams in the OpenStack community are working on addressing current pain points, such as improving federation capabilities in Keystone, the OpenStack identity service and image caching in Glance, the OpenStack image service based on a reference architecture with a minimalistic view that builds on the immediate needs of the edge ecosystem.
In parallel to design, development, and testing activities, we are iterating on the architecture to identify new requirements and gaps to address on the infrastructure layer while focusing on enhancements (versus reinventing the wheel).

As the demand for edge is extremely high, there are numerous groups in the open source and standardization ecosystem with different scopes—from the infrastructure to the applications to addressing different segments of the industry, such as telecom, automotive, or manufacturing. Collaboration between these groups is more crucial than ever.

The Edge Computing Group and the OpenStack community are actively working together with communities such as OPNFV, ONAP, and Kubernetes to ensure interoperability and smooth integration between the different components, as the success of edge strongly depends on combining the different technologies."
OpenStack$kubernetize,"You can also run Kuryr-Kubernetes on a bare-metal node as a normal OpenStack service. This way, you can provide interconnectivity between Kubernetes pods and OpenStack VMs—even if those clusters are separate—by just putting Neutron-agent and Kuryr-Kubernetes on your Kubernetes nodes.
Kuryr-Kubernetes consists of three parts:

kuryr-controller observes Kubernetes resources, decides how to translate them into OpenStack resources, and creates those resources. Information about OpenStack resources is saved into annotations of corresponding Kubernetes resources.
kuryr-cni is an executable run by the CNI that passes the calls to kuryr-daemon.
kuryr-daemon should be running on every Kubernetes node. It watches the pods created on the host and, when a CNI request comes in, wires the pods according to the Neutron ports included in the pod annotations.
In general, the control part of a CNI plugin (like Calico or Nuage) runs as a pod on the Kubernetes cluster where it provides networking, so, naturally, the Kuryr team decided to follow that model. But converting an OpenStack service into a Kubernetes app wasn't exactly a trivial task.

Kuryr-Kubernetes requirements

Kuryr-Kubernetes is just an application, and applications have requirements. Here is what each component needs from the environment and how it translates to Kubernetes' primitives.

kuryr-controller

There should be exactly one instance of kuryr-controller (although that number may be higher with the A/P high-availability feature implemented in OpenStack Rocky). This is easy to achieve using Kubernetes' Deployment primitive.
Kubernetes ServiceAccounts can provide access to the Kubernetes API with a granular set of permissions.
Different SDNs provide access to the OpenStack API differently. API SSL certificates should also be provided, for example by mounting a Secret in the pod.
To avoid a chicken-and-egg problem, kuryr-controller should run with hostNetworking to bypass using Kuryr to get the IP.
Provide a kuryr.conf file, preferably by mounting it as a ConfigMap.
In the end, we get a Deployment manifest similar to this:

apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    name: kuryr-controller
  name: kuryr-controller
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: kuryr-controller
      name: kuryr-controller
    spec:
      serviceAccountName: kuryr-controller
      automountServiceAccountToken: true
      hostNetwork: true
      containers:
      - image: kuryr/controller:latest
        name: controller
        volumeMounts:
        - name: config-volume
          mountPath: ""/etc/kuryr/kuryr.conf""
          subPath: kuryr.conf
        - name: certificates-volume
          mountPath: ""/etc/ssl/certs""
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: kuryr-config
      - name: certificates-volume
        secret:
          secretName: kuryr-certificates
      restartPolicy: Always
kuryr-daemon and kuryr-cni

Both of these components should be present on every Kubernetes node. When the kuryr-daemon container starts on the Kubernetes nodes, it injects the kuryr-cni executable and reconfigures the CNI to use it. Let's break that down into requirements.

kuryr-daemon should run on every Kubernetes node. This means it can be represented as a DaemonSet.
It should be able to access the Kubernetes API. This can be implemented with ServiceAccounts.
It also needs a kuryr.conf file. Again, the best way is to use a ConfigMap.
To perform networking operations on the node, it must run with hostNetworking and as a privileged container.
As it needs to inject the kuryr-cni executable and the CNI configuration, the Kubernetes nodes' /opt/cni/bin and /etc/cni/net.d directories must be mounted on the pod.
It also needs access to the Kubernetes nodes' netns, so /proc must be mounted on the pod. (Note that you cannot use /proc as a mount destination, so it must be named differently and Kuryr needs to be configured to know that.)
If it's running with the Open vSwitch Neutron plugin, it must mount /var/run/openvswitch.
To identify pods running on its node, nodeName should be passed into the pod. This can be done using environment variables. (This is also true with the pod name, which will be explained below.)
This produces a more complicated manifest:

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kuryr-cni
  namespace: kube-system
  labels:
    name: kuryr-cni
spec:
  template:
    metadata:
      labels:
        Name: kuryr-cni
    spec:
      hostNetwork: true
      serviceAccountName: kuryr-controller
      containers:
      - name: kuryr-cni
        image: kuryr/cni:latest
        command: [ ""cni_ds_init"" ]
        env:
        - name: KUBERNETES_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: KURYR_CNI_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        securityContext:
          privileged: true
        volumeMounts:
        - name: bin
          mountPath: /opt/cni/bin
        - name: net-conf
          mountPath: /etc/cni/net.d
        - name: config-volume
          mountPath: /etc/kuryr/kuryr.conf
          subPath: kuryr-cni.conf
        - name: proc
          mountPath: /host_proc
        - name: openvswitch
          mountPath: /var/run/openvswitch
      volumes:
        - name: bin
          hostPath:
            path: /opt/cni/bin
        - name: net-conf
          hostPath:
            path: /etc/cni/net.d
        - name: config-volume
          configMap:
            name: kuryr-config
        - name: proc
          hostPath:
            path: /proc
        - name: openvswitch
          hostPath:
            path: /var/run/openvswitch
Injecting the kuryr-cni executable

This part took us the longest time. We went through four different approaches until everything worked. Our solution was to inject a Python application from the container into the container's host and to inject the CNI configuration files (but the latter is trivial). Most of the issues were related to the fact that Python applications aren't binaries, but scripts.

We first tried making our kuryr-cni script a binary using PyInstaller. Although this worked fairly well, it had serious disadvantages. For one thing, the build process was complicated—we had to create a container with PyInstaller and Kuryr-Kubernetes that generated the binary, then build the kuryr-daemon container image with that binary. Also, due to PyInstaller quirks, we ended up with a lot of misleading tracebacks in kubelet logs, i.e., in exceptions, we could get the wrong traceback on the logs. The deciding factor was that PyInstaller changed paths to the included Python modules. This meant that some checks in the os.vif library failed and broke our continuous integration (CI).

We also tried injecting a Python virtual environment (venv) containing a CPython binary, the kuryr-kubernetes package, and all its requirements. The problem is Python venvs aren't designed to be portable. Even though there is a --relocatable option in the virtualenv command-line tool, it doesn't always work. We abandoned that approach.

Then we tried what we think is the ""correct"" way: injecting the host with an executable script that does docker exec -i on a kuryr-daemon container. Because the kuryr-kubernetes package is installed in that container, it can easily execute the kuryr-cni binary. All the CNI environment variables must be passed through the docker exec command, which has been possible since Docker API v1.24. Then, we only needed to identify the Docker container where it should be executed.

At first, we tried calling the Kubernetes API from the kuryr-daemon container's entry point to get its own container ID. We quickly discovered that this causes a race condition, and sometimes the entry point runs before the Kubernetes API is updated with its container ID. So, instead of calling the Kubernetes API, we made the injected CNI script call the Docker API on the host. Then it's easy to identify the kuryr-daemon container using labels added by Kubernetes.

Lessons learned

In the end, we've got a working system that is easy to deploy and manage because it's running on Kubernetes. We've proved that Kuryr-Kubernetes is just an application. While it took a lot of time and effort, the results are worth it. A ""Kubernetized"" application is much easier to manage and distribute. 

"
government$code,"Doing your civic duty one line of code at a time
Take an active role in shaping your country by contributing to open source code.
When it comes to doing our civic duty in today's technologically driven world, there is a perception that we don't care like older generations did. History teaches us that in the early 20th century's New Deal, Americans stepped up to the nation's challenges on a wide range of government-financed public works projects. Airport construction. Infrastructure improvements. Building dams, bridges, hospitals. This was more than just individuals ""pulling themselves up by their bootstraps"" but, by design, performing incredible civic duties. Quite an amazing feat when you think about it.
In our modern digital world, though, many believe we have lost that sense of civic duty. There is a debate brewing that as a society we are losing ourselves in technology instead of being inspired by it. This isn't just a single generation, but multiple generations indulging in innovative technology. But in that indulgence, are we missing opportunities to push technology forward?
Deep question, we know. Just track with us here.
Technology is more than just a service, more than just a convenience. Technology in our smartphone- and desktop-driven culture is a gateway to civic service. This is what the U.S. government offers at Code.gov. Through our website, we offer you an opportunity to work with open source software (OSS) that belongs to you—after all, you, the taxpayer, paid for the development of this code. This is one of many reasons we call what we offer on our website ""America's Code."" One option we offer at Code.gov to enable you to take on your digital civic duty is our Open Tasks section. Here you'll find tasks ranging from debugging code to developing a new capability or improvement to existing code. When you undertake a single task or an entire project (or something in between), you are not only honing your coding skills but also fulfilling—again, by design—a service to your government to help it improve and innovate.
In turn, the code you work with through Code.gov is yours. Do you need a base to build your app on? Do you or your business need the software a federal agency is implementing? We want to inspire your creativity and ingenuity. We want to hear your story concerning your experience with Code.gov and how we have inspired you, so share your stories with us through email, Twitter, LinkedIn, or the comments section below.
Perhaps we are far-removed from the ""pulling up by the bootstraps"" generation of the New Deal, but we are hardly lacking opportunities in taking an active role in shaping our country. Code.gov offers you a chance to fulfill a civic duty, this time on a digital platform, by answering challenges offered by the government's projects. We hope to see you online, to answer whatever questions you may have for us, and to stand alongside you as you fulfill your own civic duty, one line of code at a time.
Code on, and thank you.
"
government$agile,"How Agile helps non-technical teams get things done
What are the best ways for governments to improve effectiveness and efficiency? At San Jose City Hall, we’re getting traction with an unconventional approach: agile for non-technical teams. Public servants who do everything from emergency management to parks programs are finding that Agile methods help them with that most basic of challenges: Getting things done amid frequent interruptions and evolving priorities.
Last September, I proclaimed, ""Scrum is the best thing that’s happened to our government team."" Our innovation team of five had discovered that planning and delivering work in small increments enables us to stay focused, aligned, and continuously improving. We didn’t yet know if our experience would be replicable by other teams in our organization. We offered Agile training for 10 colleagues to see what would happen.
Nine months later, 12 teams and more than 100 staff members throughout our organization are using Agile methods to organize their work. Notably, the spread of Agile among city teams has been largely organic, not driven by top-down mandates.
What is Agile?
Agile is a way of organizing teams to work in an iterative, incremental, and highly collaborative manner. There are many different forms of Agile, but two of the most common are Kanban and Scrum. While Kanban is a lightweight framework that focuses on visualizing tasks on a board (physical or digital), Scrum is more prescriptive and complex, with specific roles, meetings, and time-boxed sprints. Both Kanban and Scrum place value in responding to change over following a plan, continuously improving process, and giving the team autonomy to determine how the work gets done.
While there are many variations on Agile methods, here are some aspects that are common among our teams in the City of San Jose:

The team maintains a list of all potential tasks, known as a ""backlog""
Tasks are not worked on until explicitly prioritized
Work is made visible by tracking tasks on public boards
Team conducts regular check-in meetings
Team conducts regular ""retrospective"" meetings to identify on what’s working well and what needs improvement
While Agile originally developed in the manufacturing industry and became well-known as a software development framework, it has been adapted to other sectors, including government.
Why city leaders are choosing Agile
Of the dozen teams using Agile in the City of San Jose, almost none are delivering software, and almost none had heard of Agile a year ago.
Why did these teams decide to go Agile? Here are the top reasons we hear from internal Agile champions.
Aligning as a team. When Ramses Madou began managing the City of San Jose’s transportation planning group, he saw a need to align the contributions of his talented team members into a coherent effort that advanced the department’s overall priorities. Before implementing Scrum, he says, ""Everyone was running on their own track.""
Focus on priorities. After the launch of a new grant program, San Jose’s Parks, Recreation and Neighborhood Services department suddenly found itself with 47 contracts to process in six weeks. To help the team get a handle on the work, program manager CJ Ryan enlisted our resident Agile coach Alvina Nishimoto to guide the team in setting up a Kanban board to visually track the contracts. After two weeks, CJ could see that the team was not progressing as quickly as needed, so she decided she needed a 100% focus. She blocked off her team’s calendar for two full days and gave them air cover to ignore other demands on their time. Without constant interruptions, her team got through 25 drafts contracts in just two days.
Manage interruptions. The web team for San Jose Public Library receives a constant flow of requests from other library divisions, which makes it difficult for the team to make progress on important longer-term projects. When the team implemented Scrum, the team’s manager and ""product owner"" Laurie Willis became the filter for all requests, ensuring that they would be worked on only after being prioritized for a given sprint. Team member Stacy Tomaszewski immediately started feeling the benefit of not being pressured to respond immediately: ""We feel like we can say no."" And in government, where everything feels like a priority, the ability to say ""wait until our next sprint"" can be very powerful indeed.
5 tips for getting started
Start with the basics: Make work visible. Transparency is one of the first benefits experienced by most teams in our organization. To get started, all you really need is to put prioritized tasks up in one place where you can progress from ""backlog"" to ""work in progress"" to ""done."" Team members benefit from having a visual reminder of their priorities and getting to share their progress with the team. Managers and stakeholders get more visibility into work underway. We recommend starting with a simple, lightweight system. If your team is co-located, it’s hard to beat a physical board in terms of a visual reminder. Most San Jose teams still use sticky notes to track tasks. A few teams use Trello or other digital tools and have successfully gotten team members in the habit of checking and updating Trello regularly.
Iterate and improve over time. Continuous, incremental improvement is the real secret to the power of Agile. All of San Jose’s teams do regular retrospectives to reflect on what’s working and what needs to be improved. This process empowers your team to make Agile work for them and overcomes resistance by allowing the team to drive small experiments that add up over time. CJ Ryan’s team in the Parks department initially resisted the idea of daily stand-ups, and she decided not to push them. Instead, the team started with a simple Kanban board. After a while, the team decided that daily stand-up meetings would be useful so they could share progress at a finer-grained level. Next, they decided to start regular planning meetings as a team to establish priorities. Effectively, the team has evolved to adopt many ""Scrum-like"" practices, but at a pace that makes sense to them.
Get some expert help to take it to the next level. While it’s easy enough to start adopting Agile practices you read about online, troubleshooting can be hard if your peers and team don’t have prior experience with Agile in a range of contexts. We’ve found it critical to have the wisdom of an experienced ""Agile coach"" to support teams, provide guidance, and offer spot training and coaching. We found both our coaches through the Encore fellow program, but you could also hire an Agile consultant or perhaps find pro bono volunteers from the private sector.
Be patient. Agile helps teams to establish productive habits like prioritizing, asking for help, and reflecting. But Agile is not a magic bullet—it won’t solve all your problems overnight. In fact, it may even make some of your problems more apparent. Give yourself time to figure out how to make the process work for your team.
And finally, have fun with it. A big benefit of Agile is that it builds connections among team members, enabling deeper collaboration and greater effectiveness as a team. So don’t be too serious about it. Instead, experiment with creative ways to make Agile an enjoyable and integral part of your team culture.
Let us know what you learn along the way."
Textricator$docs,"Textricator: Data extraction made simple
New open source tool extracts complex data from PDF docs, no programming skills required.
You probably know the feeling: You ask for data and get a positive response, only to open the email and find a whole bunch of PDFs attached. Data, interrupted.
We understand your frustration, and we’ve done something about it: Introducing Textricator, our first open source product.
We’re Measures for Justice, a criminal justice research and transparency organization. Our mission is to provide data transparency for the entire justice system, from arrest to post-conviction. We do this by producing a series of up to 32 performance measures covering the entire criminal justice system, county by county. We get our data in many ways—all legal, of course—and while many state and county agencies are data-savvy, giving us quality, formatted data in CSVs, the data is often bundled inside software with no simple way to get it out. PDF reports are the best they can offer.
Developers Joe Hale and Stephen Byrne have spent the past two years developing Textricator to extract tens of thousands of pages of data for our internal use. Textricator can process just about any text-based PDF format—not just tables, but complex reports with wrapping text and detail sections generated from tools like Crystal Reports. Simply tell Textricator the attributes of the fields you want to collect, and it chomps through the document, collecting and writing out your records.
Not a software engineer? Textricator doesn’t require programming skills; rather, the user describes the structure of the PDF and Textricator handles the rest. Most users run it via the command line; however, a browser-based GUI is available.
We evaluated other great open source solutions like Tabula, but they just couldn’t handle the structure of some of the PDFs we needed to scrape. “Textricator is both flexible and powerful and has cut the time we spend to process large datasets from days to hours,” says Andrew Branch, director of technology.
At MFJ, we’re committed to transparency and knowledge-sharing, which includes making our software available to anyone, especially those trying to free and share data publicly. Textricator is available on GitHub and released under GNU Affero General Public License Version 3.
You can see the results of our work, including data processed via Textricator, on our free online data portal. Textricator is an essential part of our process and we hope civic tech and government organizations alike can unlock more data with this new tool."
gaming$RaspberryPi,"Build a retro gaming console with RetroPie
Play your favorite classic Nintendo, Sega, and Sony console games on Linux.
The most common question I get on my YouTube channel and in person is what my favorite Linux distribution is. If I limit the answer to what I run on my desktops and laptops, my answer will typically be some form of an Ubuntu-based Linux distro. My honest answer to this question may surprise many. My favorite Linux distribution is actually RetroPie.
As passionate as I am about Linux and open source software, I'm equally passionate about classic gaming, specifically video games produced in the '90s and earlier. I spend most of my surplus income on older games, and I now have a collection of close to a thousand games for over 20 gaming consoles. In my spare time, I raid flea markets, yard sales, estate sales, and eBay buying games for various consoles, including almost every iteration made by Nintendo, Sega, and Sony. There's something about classic games that I adore, a charm that seems lost in games released nowadays.
Unfortunately, collecting retro games has its fair share of challenges. Cartridges with memory for save files will lose their charge over time, requiring the battery to be replaced. While it's not hard to replace save batteries (if you know how), it's still time-consuming. Games on CD-ROMs are subject to disc rot, which means that even if you take good care of them, they'll still lose data over time and become unplayable. Also, sometimes it's difficult to find replacement parts for some consoles. This wouldn't be so much of an issue if the majority of classic games were available digitally, but the vast majority are never re-released on a digital platform.

Gaming on RetroPie
RetroPie is a great project and an asset to retro gaming enthusiasts like me. RetroPie is a Raspbian-based distribution designed for use on the Raspberry Pi (though it is possible to get it working on other platforms, such as a PC). RetroPie boots into a graphical interface that is completely controllable via a gamepad or joystick and allows you to easily manage digital copies (ROMs) of your favorite games. You can scrape information from the internet to organize your collection better and manage lists of favorite games, and the entire interface is very user-friendly and efficient. From the interface, you can launch directly into a game, then exit the game by pressing a combination of buttons on your gamepad. You rarely need a keyboard, unless you have to enter your WiFi password or manually edit configuration files.
I use RetroPie to host a digital copy of every physical game I own in my collection. When I purchase a game from a local store or eBay, I also download the ROM. As a collector, this is very convenient. If I don't have a particular physical console within arms reach, I can boot up RetroPie and enjoy a game quickly without having to connect cables or clean cartridge contacts. There's still something to be said about playing a game on the original hardware, but if I'm pressed for time, RetroPie is very convenient. I also don't have to worry about dead save batteries, dirty cartridge contacts, disc rot, or any of the other issues collectors like me have to regularly deal with. I simply play the game.
Also, RetroPie allows me to be very clever and utilize my technical know-how to achieve additional functionality that's not normally available. For example, I have three RetroPies set up, each of them synchronizing their files between each other by leveraging Syncthing, a popular open source file synchronization tool. The synchronization happens automatically, and it means I can start a game on one television and continue in the same place on another unit since the save files are included in the synchronization. To take it a step further, I also back up my save and configuration files to Backblaze B2, so I'm protected if an SD card becomes defective.
Setting up RetroPie
Setting up RetroPie is very easy, and if you've ever set up a Raspberry Pi Linux distribution before (such as Raspbian) the process is essentially the same—you simply download the IMG file and flash it to your SD card by utilizing another tool, such as Etcher, and insert it into your RetroPie. Then plug in an AC adapter and gamepad and hook it up to your television via HDMI. Optionally, you can buy a case to protect your RetroPie from outside elements and add visual appeal. Here is a listing of things you'll need to get started:
Raspberry Pi board (Model 3B+ or higher recommended)
SD card (16GB or larger recommended)
A USB gamepad
UL-listed micro USB power adapter, at least 2.5 amp
If you choose to add the optional Raspberry Pi case, I recommend the Super NES and Super Famicom themed cases from RetroFlag. Not only do these cases look cool, but they also have fully functioning power and reset buttons. This means you can configure the reset and power buttons to directly trigger the operating system's halt process, rather than abruptly terminating power. This definitely makes for a more professional experience, but it does require the installation of a special script. The instructions are on RetroFlag's GitHub page. Be wary: there are many cases available on Amazon and eBay of varying quality. Some of them are cheap knock-offs of RetroFlag cases, and others are just a lower quality overall. In fact, even cases by RetroFlag vary in quality—I had some power-distribution issues with the NES-themed case that made for an unstable experience. If in doubt, I've found that RetroFlag's Super NES and Super Famicom themed cases work very well.
Adding games
When you boot RetroPie for the first time, it will resize the filesystem to ensure you have full access to the available space on your SD card and allow you to set up your gamepad. I can't give you links for game ROMs, so I'll leave that part up to you to figure out. When you've found them, simply add them to the RetroPie SD card in the designated folder, which would be located under /home/pi/RetroPie/roms/<console_name>. You can use your favorite tool for transferring the ROMs to the Pi, such as SCP in a terminal, WinSCP, Samba, etc. Once you've added the games, you can rescan them by pressing start and choosing the option to restart EmulationStation. When it restarts, it should automatically add menu entries for the ROMs you've added. That's basically all there is to it.
(The rescan updates EmulationStation’s game inventory. If you don’t do that, it won’t list any newly added games you copy over.)
Regarding the games' performance, your mileage will vary depending on which consoles you're emulating. For example, I've noticed that Sega Dreamcast games barely run at all, and most Nintendo 64 games will run sluggishly with a bad framerate. Many PlayStation Portable (PSP) games also perform inconsistently. However, all of the 8-bit and 16-bit consoles emulate seemingly perfectly—I haven't run into a single 8-bit or 16-bit game that doesn't run well. Surprisingly, games designed for the original PlayStation run great for me, which is a great feat considering the lower-performance potential of the Raspberry Pi.
Overall, RetroPie's performance is great, but the Raspberry Pi is not as powerful as a gaming PC, so adjust your expectations accordingly.
Conclusion
RetroPie is a fantastic open source project dedicated to preserving classic games and an asset to game collectors everywhere. Having a digital copy of my physical game collection is extremely convenient. If I were to tell my childhood self that one day I could have an entire game collection on one device, I probably wouldn't believe it. But RetroPie has become a staple in my household and provides hours of fun and enjoyment.


"
gaming$Linux,"Go on an adventure in your Linux terminal
Our final day of the Linux command-line toys advent calendar ends with the beginning of a grand adventure.
Today is the final day of our 24-day-long Linux command-line toys advent calendar. Hopefully, you've been following along, but if not, start back at the beginning and work your way through. You'll find plenty of games, diversions, and oddities for your Linux terminal.

And while you may have seen some toys from our calendar before, we hope there’s at least one new thing for everyone.
Today's toy was suggested by Opensource.com moderator Joshua Allen Holm:
""If the last day of your advent calendar is not ESR's [Eric S. Raymond's] open source release of Adventure, which retains use of the classic 'advent' command (Adventure in the BSD Games package uses 'adventure), I will be very, very, very disappointed. ;-)""
What a perfect way to end our series.
Colossal Cave Adventure (often just called Adventure), is a text-based game from the 1970s that gave rise to the entire adventure game genre. Despite its age, Adventure is still an easy way to lose hours as you explore a fantasy world, much like a Dungeons and Dragons dungeon master might lead you through an imaginary place.
Rather than take you through the history of Adventure here, I encourage you to go read Joshua's history of the game itself and why it was resurrected and re-ported a few years ago. Then, go clone the source and follow the installation instructions to launch the game with advent on your system. Or, as Joshua mentions, another version of the game can be obtained from the bsd-games package, which is probably available from your default repositories in your distribution of choice.
Do you have a favorite command-line toy that you we should have included? Our series concludes today, but we'd still love to feature some cool command-line toys in the new year. Let me know in the comments below, and I'll check it out. And let me know what you thought of today's amusement.
"
gaming$RaspberryPi,"5 ways to play old-school games on a Raspberry Pi
Relive the golden age of gaming with these open source platforms for Raspberry Pi.
They don't make 'em like they used to, do they? Video games, I mean.
Sure, there's a bit more grunt in the gear now. Princess Zelda used to be 16 pixels in each direction; there's now enough graphics power for every hair on her head. Today's processors could beat up 1988's processors in a cage-fight deathmatch without breaking a sweat.
But you know what's missing? The fun.
You've got a squillion and one buttons to learn just to get past the tutorial mission. There's probably a storyline, too. You shouldn't need a backstory to kill bad guys. All you need is jump and shoot. So, it's little wonder that one of the most enduring popular uses for a Raspberry Pi is to relive the 8- and 16-bit golden age of gaming in the '80s and early '90s. But where to start?
There are a few ways to play old-school games on the Pi. Each has its strengths and weaknesses, which I'll discuss here.
Retropie
Retropie is probably the most popular retro-gaming platform for the Raspberry Pi. It's a solid all-rounder and a great default option for emulating classic desktop and console gaming systems.
What is it?
Retropie is built to run on Raspbian. It can also be installed over an existing Raspbian image if you'd prefer. It uses EmulationStation as a graphical front-end for a library of open source emulators, including the Libretro emulators.
You don't need to understand a word of that to play your games, though.
What's great about it
It's very easy to get started. All you need to do is burn the image to an SD card, configure your controllers, copy your games over, and start killing bad guys.
The huge user base means that there is a wealth of support and information out there, and active online communities to turn to for questions.
In addition to the emulators that come installed with the Retropie image, there's a huge library of emulators you can install from the package manager, and it's growing all the time. Retropie also offers a user-friendly menu system to manage this, saving you time.
From the Retropie menu, it's easy to add Kodi and the Raspbian desktop, which comes with the Chromium web browser. This means your retro-gaming rig is also good for home theatre, YouTube, SoundCloud, and all those other “lounge room computer” goodies.
Retropie also has a number of other customization options: You can change the graphics in the menus, set up different control pad configurations for different emulators, make your Raspberry Pi file system visible to your local Windows network—all sorts of stuff.
Retropie is built on Raspbian, which means you have the Raspberry Pi's most popular operating system to explore. Most Raspberry Pi projects and tutorials you find floating around are written for Raspbian, making it easy to customize and install new things on it. I've used my Retropie rig as a wireless bridge, installed MIDI synthesizers on it, taught myself a bit of Python, and more—all without compromising its use as a gaming machine.
What's not so great about it
Retropie's simple installation and ease of use is, in a way, a double-edged sword. You can go for a long time with Retropie without ever learning simple stuff like sudo apt-get, which means you're missing out on a lot of the Raspberry Pi experience.
It doesn't have to be this way; the command line is still there under the hood when you want it, but perhaps users are a bit too insulated from a Bash shell that's ultimately a lot less scary than it looks. Retropie's main menu is operable only with a control pad, which can be annoying when you don't have one plugged in because you've been using the system for things other than gaming.
Who's it for?
Anyone who wants to get straight into some gaming, anyone who wants the biggest and best library of emulators, and anyone who wants a great way to start exploring Linux when they're not playing games.
Recalbox
Recalbox is a newer open source suite of emulators for the Raspberry Pi. It also supports other ARM-based small-board computers.
What is it?
Like Retropie, Recalbox is built on EmulationStation and Libretro. Where it differs is that it's not built on Raspbian, but on its own flavor of Linux: RecalboxOS.
What's great about it

The setup for Recalbox is even easier than for Retropie. You don't even need to image an SD card; simply copy some files over and go. It also has out-of-the-box support for some game controllers, getting you to Level 1 that little bit faster. Kodi comes preinstalled. This is a ready-to-go gaming and media rig.

What's not so great about it
Recalbox has fewer emulators than Retropie, fewer customization options, and a smaller user community.
Your Recalbox rig is probably always just going to be for emulators and Kodi, the same as when you installed it. If you feel like getting deeper into Linux, you'll probably want a new SD card for Raspbian.
Who's it for?
Recalbox is great if you want the absolute easiest retro gaming experience and can happily go without some of the more obscure gaming platforms, or if you are intimidated by the idea of doing anything a bit technical (and have no interest in growing out of that).
For most opensource.com readers, Recalbox will probably come in most handy to recommend to your not-so-technical friend or relative. Its super-simple setup and overall lack of options might even help you avoid having to help them with it.
Roll your own
Ok, if you've been paying attention, you might have noticed that both Retropie and Recalbox are built from many of the same open source components. So what's to stop you from putting them together yourself?
What is it?
Whatever you want it to be, baby. The nature of open source software means you could use an existing emulator suite as a starting point, or pilfer from them at will.
What's great about it
If you have your own custom interface in mind, I guess there's nothing to do but roll your sleeves up and get to it. This is also a way to install emulators that haven't quite found their way into Retropie yet, such as BeebEm or ArcEm.
What's not so great about it
Well, it's a bit of work, isn't it?
Who's it for?
Hackers, tinkerers, builders, seasoned hobbyists, and such.
Native RISC OS gaming
Now here's a dark horse: RISC OS, the original operating system for ARM devices.
What is it?
Before ARM went on to become the world's most popular CPU architecture, it was originally built to be the heart of the Acorn Archimedes. That's kind of a forgotten beast nowadays, but for a few years it was light years ahead as the most powerful desktop computer in the world, and it attracted a lot of games development.
Because the ARM processor in the Pi is the great-grandchild of the one in the Archimedes, we can still install RISC OS on it, and with a little bit of work, get these games running. This is different to the emulator options we've covered so far because we're playing our games on the operating system and CPU architecture for which they were written.
What's great about it
It's the perfect introduction to RISC OS. This is an absolute gem of an operating system and well worth checking out in its own right.
The fact that you're using much the same operating system as back in the day to load and play your games makes your retro gaming rig just that little bit more of a time machine. This definitely adds some charm and retro value to the project.
There are a few superb games that were released only on the Archimedes. The massive hardware advantage of the Archimedes also means that it often had the best graphics and smoothest gameplay of a lot of multi-platform titles. The rights holders to many of these games have been generous enough to make them legally available for free download.
What's not so great about it
Once you have installed RISC OS, it still takes a bit of elbow grease to get the games working. Here's a guide to getting started.
This is definitely not a great all-rounder for the lounge room. There's nothing like Kodi. There's a web browser, NetSurf, but it's struggling to catch up to the modern web. You won't get the range of titles to play as you would with an emulator suite. RISC OS Open is free for hobbyists to download and use and much of the source code has been made open. But despite the name, it's not a 100% open source operating system.
Who's it for?
This one's for novelty seekers, absolute retro heads, people who want to explore an interesting operating system from the '80s, people who are nostalgic for Acorn machines from back in the day, and people who want a totally different retro gaming project.
Command line gaming
Do you really need to install an emulator or an exotic operating system just to relive the glory days? Why not just install some native linux games from the command line?
What is it?
There's a whole range of native Linux games tested to work on the Raspberry Pi.
What's great about it
You can install most of these from packages using the command line and start playing. Easy. If you've already got Raspbian up and running, it's probably your fastest path to getting a game running.
What's not so great about it
This isn't, strictly speaking, actual retro gaming. Linux was born in 1991 and took a while longer to come together as a gaming platform. This isn't quite gaming from the classic 8- and 16-bit era; these are ports and retro-influenced games that were built later.
Who's it for?
If you're just after a bucket of fun, no problem. But if you're trying to relive the actual era, this isn't quite it."
gaming$poll,"Did your open source career begin with video games?
Many people first became fascinated by computers as gaming devices, and later turned this fascination into a technical career path. Were you one of them?
Certainly you don't need to be a gamer as a child to grow up and become a developer, nor does being a gamer automatically set you up for a career in technology.
But there's definitely a good bit of overlap between the two.
After listening to the first episode of Command Line Heroes, and reading Ross Turk's story of how MUDs led him to a career in coding, I've thought a bit about how gaming has influenced my own journey into technology, and how it lead to a career in open source.
For me, that first important game was WarCraft II. Sure, I played games before it, and after it. But shortly after my family replaced our faithful Apple IIc with a blazing fast (by comparison) 486 PC with amazing features like color, and a sound card, and even a 2400 baud modem (that would take about three months to download the equivalent of an hour of Netflix today).
WarCraft II was the first game I ever truly fell in love with. It was a real-time strategy game, in which you controlled armies of orcs or humans who do battle against one another in an attempt to gain control of Azeroth, the same lore from which the MMORPG World of WarCraft would spawn years later.
Make no mistake, there was nothing open source about WarCraft II itself. It was a proprietary boxed game in an era when every game was a proprietary boxed game. Its engine, graphics, sounds, formats, and everything else about it were closed as closed can be, and it only ran on proprietary Microsoft (and later, Apple) platforms. (Linux in those days was not known for its graphics capabilities for casual users.)
But without this proprietary game, I probably would have never taken those first steps to explore the technical underpinnings of my computer and the world beyond.

In the summer of 1998, a couple of years after the game was first release, I joined a gaming guild, and took my first swing at community management as I climbed the ranks and managed squads of fellow WarCraft II players around the globe, eventually climbing my way to the leadership of a small organization of dozens of dedicated players.
I taught myself HTML so I could design the guild's homepage. And I did so with all of the horrors that were animated GIFs and <blink> tags and auto-playing MIDI background music and JavaScript popups that served no real purpose except to welcome you to the page. Through a lot of trial and error, and copious amount of borrowing and modifying other people's code snippets, I managed to put something together. In fact, it would be easy to argue I couldn't have built it without being able to see and borrow from the code of others.
Eventually, this led to wanting to add guestbooks and visit counters and even eventually an online forum. Somewhere along the way, this required that I figure out a thing or two about Perl and shells scripts and what a cgi-bin directory in Apache web server was for and what the heck these ""file permissions"" were on this Linux machine hosting my website. (And what the heck were Linux and Apache, anyway?) And while I don't remember the exact first time I explicitly saw one of these scripts under an open source license, I do know that were it not for having source access and a community behind them, I never would have figured out how these things worked, how to configure them, and how to modify them to meet my needs.
And I wasn't satisfied with the game itself, either. Sure, it shipped with a level editor, but I wanted more. Eventually, I found myself joining a community of people who used hex editors to decode, document, and share the game's file formats, allowing people like me to edit nearly every aspect of the game from design to how it worked. Documenting the formats made me realize how important it is to have access to the innards software, to understand how it works and be able to share your changes.
Later, I even found myself helping out a bit with testing and documenting an open source rewrite of the game engine called FreeCraft (later renamed Stratagus).
And all of these experiences led to my teenage self's aspiration to become a game developer, learning everything I could about writing games on my own, first in QBasic and later in Pascal and C. While I ended up following other passions in college and the game developer career path never panned out, the skills I learned on the way stuck with me. So did my appreciation for sharing and community, two of the fundamental foundations of the open source movement.
Equally important, perhaps, was how it stoked a curiosity about the way things work, and a desire to take them apart and tinker with them and hopefully improve them. It's hard to imagine how to truly learn about technology in a world where you can't poke around under the case.
On one hand, it feels weird to thank a proprietary game for being a stepping stone in my life in open source, but on the other, almost all of us had a proprietary software story that came before our open source awakening, and why wouldn't it have been a game?
"
containers$DataScience$Kubernetes,"Why data scientists love Kubernetes
Kubernetes' features that streamline the software development workflow also support the data science workflow.
Let's start with an uncontroversial point: Software developers and system operators love Kubernetes as a way to deploy and manage applications in Linux containers. Linux containers provide the foundation for reproducible builds and deployments, but Kubernetes and its ecosystem provide essential features that make containers great for running real applications, like:
Continuous integration and deployment, so you can go from a Git commit to a passing test suite to new code running in production
Ubiquitous monitoring, which makes it easy to track the performance and other metrics about any component of a system and visualize them in meaningful ways
Declarative deployments, which allow you to rely on Kubernetes to recreate your production environment in a staging environment
Flexible service routing, which means you can scale services out or gradually roll updates out to production (and roll them back if necessary)
What you may not know is that Kubernetes also provides an unbeatable combination of features for working data scientists. The same features that streamline the software development workflow also support a data science workflow! To see why, let's first see what a data scientist's job looks like.

A data science project: predicting customer churn
Some people define data science broadly, including machine learning (ML), software engineering, distributed computing, data management, and statistics. Others define the field more narrowly as finding solutions to real-world problems by combining some domain expertise with machine learning or advanced statistics. We're not going to commit to an explicit definition of data scientist in this article, but we will tell you what data scientists might aim to do in a typical project and how they might do their work.
Consider a problem faced by any business with subscribing customers: Some might not renew. Customer churn detection seeks to proactively identify customers who are likely to not renew their contracts. Once these customers are identified, the business can choose to target their accounts with particular interventions (for example, sales calls or discounts) to make them less likely to leave. The overall churn-prevention problem has several parts: predicting which customers are likely to leave, identifying interventions that are likely to retain customers, and prioritizing which customers to target given a limited budget for interventions. A data scientist could work on any or all of these, but we'll use the first one as our running example.
The first part of the problem to solve is identifying an appropriate definition for ""churn"" to incorporate into a predictive model. We may have an intuitive definition of what it means to lose a customer, but a data scientist needs to formalize this definition, say, by defining the churn prediction problem as: ""Given this customer's activity in the last 18 months, how likely are they to cancel their contract in the next six?»
The data scientist then needs to decide which data about a customer's activity the model should consider—in effect, fleshing out and formalizing the first part of the churn definition. Concretely, a data scientist might consider any information available about a customer's actual use of the company's products over the historical window, the size of their account, the number of customer-service interactions they've had, or even the tone of their comments on support tickets they've filed. The measurements or data that our model considers are called features.
With a definition of churn and a set of features to consider, the data scientist can then begin exploratory analysis on historical data (that includes both the feature set and the ultimate outcome for a given customer in a given period). Exploratory analysis can include visualizing combinations of features and seeing how they correlate with whether a customer will churn. More generally, this part of the process seeks to identify structure in the historical data and whether it is possible to find a clean separation between retained customers and churning customers based on the data characterizing them.
For some problems, it won't be obvious that there's structure in the data—in these cases, the data scientist will have to go back to the drawing board and identify some new data to collect or perhaps a novel way to encode or transform the data available. However, exploratory analysis will often help a data scientist identify the features to consider while training a predictive model, as well as suggest some ways to transform those data. The data scientist's next job is feature engineering: finding a way to transform and encode the feature data—which might be in database tables, on event streams, or in data structures in a general-purpose programming language—so that it's suitable for input to the algorithm that trains a model. This generally means encoding these features as vectors of floating-point numbers. Just any encoding won't do; the data scientist needs to find an encoding that preserves the structure of the features so similar customers map to similar vectors—or else the algorithm will perform poorly.
Only now is the data scientist ready to train a predictive model. For the problem of predicting whether a customer will churn, the model-training pipeline starts with labeled historical data about customers. It then uses the techniques developed in the feature-engineering process to extract features from raw data, resulting in vectors of floating-point numbers labeled with ""true"" or ""false"" and corresponding to customers that will or will not churn in the window of interest. The model-training algorithm takes this collection of feature vectors as input and optimizes a process to separate between true and false vectors in a way that minimizes error. The predictive model will ultimately be a function that takes a feature vector and returns true or false, indicating whether the customer corresponding to that vector is likely to churn or not.
At any point in this process, the data scientist may need to revisit prior phases—perhaps to refine a feature-engineering approach, to collect different data, or even to change the metric they are trying to predict. In this way, the data science workflow is a lot like the traditional software development lifecycle: problems discovered during implementation can force an engineer to change the design of an interface or choice of a data structure. These problems can even cascade all the way back to requirements analysis, forcing a broader rethinking of the project's fundamentals. Fortunately, Kubernetes can support the data scientist's workflow in the same way it can support the software development lifecycle.
Kubernetes for data science
Data scientists have many of the same concerns that software engineers do: repeatable experiments (like repeatable builds); portable and reproducible environments (like having identical setups in development, stage, and production); credential management; tracking and monitoring metrics in production; flexible routing; and effortless scale-out. It's not hard to see some of the analogies between things application developers do with Kubernetes and things data scientists might want to do:
Repeatable batch jobs, like CI/CD pipelines, are analogous to machine learning pipelines in that multiple coordinated stages need to work together in a reproducible way to process data; extract features; and train, test, and deploy models.
Declarative configurations that describe the connections between services facilitate creating reproducible learning pipelines and models across platforms.
Microservice architectures enable simple debugging of machine learning models within the pipeline and aid collaboration between data scientists and other members of their team.
Data scientists share many of the same challenges as application developers, but they have some unique challenges related to how data scientists work and to the fact that machine learning models can be more difficult to test and monitor than conventional services. We'll focus on one problem related to workflow.
Most data scientists do their exploratory work in interactive notebooks. Notebook environments, such as those developed by Project Jupyter, provide an interactive literate programming environment in which users can mix explanatory text and code; run and change the code; and inspect its output.
These properties make notebook environments wonderfully flexible for exploratory analysis. However, they are not an ideal software artifact for collaboration or publishing—imagine if the main way software developers published their work was by posting transcripts from interactive REPLs to a pastebin service.
Sharing an interactive notebook with a colleague is akin to sharing a physical one—there's some good information in there, but they have to do some digging to find it. And due to the fragility and dependency of a notebook on its environment, a colleague may see different output when they run your notebook—or worse: it may crash.
Kubernetes for data scientists
Data scientists may not want to become Kubernetes experts—and that's fine! One of the strengths of Kubernetes is that it is a powerful framework for building higher-level tools.
One such tool is the Binder service, which takes a Git repository of Jupyter notebooks, builds a container image to serve them, then launches the image in a Kubernetes cluster with an exposed route so you can access it from the public internet. Since one of the big downsides of notebooks is that their correctness and functionality can be dependent on their environment, having a high-level tool that can build an immutable environment to serve a notebook on Kubernetes eliminates a huge source of headaches.
It's possible to use the hosted Binder service or run your own Binder instance, but if you want a little more flexibility in the process, you can also use the source-to-image (S2I) workflow and tool along with Graham Dumpleton's Jupyter S2I images to roll your own notebook service. In fact, the source-to-image workflow is a great starting point for infrastructure or packaging experts to build high-level tools that subject matter experts can use. For example, the Seldon project uses S2I to simplify publishing model services—simply provide a model object to the builder, and it will build a container exposing it as a service.
A great thing about the source-to-image workflow is that it enables arbitrary actions and transformations on a source repository before building an image. As an example of how powerful this workflow can be, we've created an S2I builder image that takes as its input a Jupyter notebook that shows how to train a model. It then processes this notebook to identify its dependencies and extract a Python script to train and serialize the model. Given these, the builder installs the necessary dependencies and runs the script in order to train the model. The ultimate output of the builder is a REST web service that serves the model built by the notebook. You can see a video of this notebook-to-model-service S2I in action. Again, this isn't the sort of tool that a data scientist would necessarily develop, but creating tools like this is a great opportunity for Kubernetes and packaging experts to collaborate with data scientists.
Kubernetes for machine learning in production
Kubernetes has a lot to offer data scientists who are developing techniques to solve business problems with machine learning, but it also has a lot to offer the teams who put those techniques in production. Sometimes machine learning represents a separate production workload—a batch or streaming job to train models and provide insights—but machine learning is increasingly put into production as an essential component of an intelligent application.
The Kubeflow project is targeted at machine learning engineers who need to stand up and maintain machine learning workloads and pipelines on Kubernetes. Kubeflow is also an excellent distribution for infrastructure-savvy data scientists. It provides templates and custom resources to deploy a range of machine learning libraries and tools on Kubernetes.
Kubeflow is an excellent way to run frameworks like TensorFlow, JupyterHub, Seldon, and PyTorch under Kubernetes and thus represents a path to truly portable workloads: a data scientist or machine learning engineer can develop a pipeline on a laptop and deploy it anywhere. This is a very fast-moving community developing some cool technology, and you should check it out!
Radanalytics.io is a community project targeted at application developers, and it focuses on the unique demands of developing intelligent applications that depend on scale-out compute in containers. The radanalytics.io project includes a containerized Apache Spark distribution to support scalable data transformation and machine learning model training, as well as a Spark operator and Spark management interface. The community also supports the entire intelligent application lifecycle by providing templates and images for Jupyter notebooks, TensorFlow training and serving, and S2I builders that can deploy an application along with the scale-out compute resources it requires. If you want to get started building intelligent applications on OpenShift or Kubernetes, a great place to start is one of the many example applications or conference talks on radanalytics.io.

"
containers$DevOps,"What containers can teach us about DevOps
The use of containers supports the three pillars of DevOps practices: flow, feedback, and continual experimentation and learning.
One can argue that containers and DevOps were made for one another. Certainly, the container ecosystem benefits from the skyrocketing popularity of DevOps practices, both in design choices and in DevOps’ use by teams developing container technologies. Because of this parallel evolution, the use of containers in production can teach teams the fundamentals of DevOps and its three pillars: The Three Ways.
Principles of flow
Container flow
A container can be seen as a silo, and from inside, it is easy to forget the rest of the system: the host node, the cluster, the underlying infrastructure. Inside the container, it might appear that everything is functioning in an acceptable manner. From the outside perspective, though, the application inside the container is a part of a larger ecosystem of applications that make up a service: the web API, the web app user interface, the database, the workers, and caching services and garbage collectors. Teams put constraints on the container to limit performance impact on infrastructure, and much has been done to provide metrics for measuring container performance because overloaded or slow container workloads have downstream impact on other services or customers.
Real-world flow
This lesson can be applied to teams functioning in a silo as well. Every process (be it code release, infrastructure creation or even, say, manufacturing of Spacely’s Sprockets), follows a linear path from conception to realization. In technology, this progress flows from development to testing to operations and release. If a team working alone becomes a bottleneck or introduces a problem, the impact is felt all along the entire pipeline. A defect passed down the line destroys productivity downstream. While the broken process within the scope of the team itself may seem perfectly correct, it has a negative impact on the environment as a whole.
DevOps and flow
The first way of DevOps, principles of flow, is about approaching the process as a whole, striving to comprehend how the system works together and understanding the impact of issues on the entire process. To increase the efficiency of the process, pain points and waste are identified and removed. This is an ongoing process; teams must continually strive to increase visibility into the process and find and fix trouble spots and waste.
“The outcomes of putting the First Way into practice include never passing a known defect to downstream work centers, never allowing local optimization to create global degradation, always seeking to increase flow, and always seeking to achieve a profound understanding of the system (as per Deming).”
–Gene Kim, The Three Ways: The Principles Underpinning DevOps, IT Revolution, 25 Apr. 2017
Principles of feedback
Container feedback
In addition to limiting containers to prevent impact elsewhere, many products have been created to monitor and trend container metrics in an effort to understand what they are doing and notify when they are misbehaving. Prometheus, for example, is all the rage for collecting metrics from containers and clusters. Containers are excellent at separating applications and providing a way to ship an environment together with the code, sometimes at the cost of opacity, so much is done to try to provide rapid feedback so issues can be addressed promptly within the silo.
Real-world feedback
The same is necessary for the flow of the system. From inception to realization, an efficient process quickly provides relevant feedback to identify when there is an issue. The key words here are “quick” and “relevant.” Burying teams in thousands of irrelevant notifications make it difficult or even impossible to notice important events that need immediate action, and receiving even relevant information too late may allow small, easily solved issues to move downstream and become bigger problems. Imagine if Lucy and Ethel had provided immediate feedback that the conveyor belt was too fast—there would have been no problem with the chocolate production (though that would not have been nearly as funny).
DevOps and feedback
The Second Way of DevOps, principles of feedback, is all about getting relevant information quickly. With immediate, useful feedback, problems can be identified as they happen and addressed before impact is felt elsewhere in the development process. DevOps teams strive to “optimize for downstream” and immediately move to fix problems that might impact other teams that come after them. As with flow, feedback is a continual process to identify ways to quickly get important data and act on problems as they occur.
“Creating fast feedback is critical to achieving quality, reliability, and safety in the technology value stream.”
–Gene Kim, et al., The DevOps Handbook: How to Create World-Class Agility, Reliability, and Security in Technology Organizations, IT Revolution Press, 2016
Principles of continual experimentation and learning
Container continual experimentation and learning
It is a bit more challenging applying operational learning to the Third Way of DevOps:continual experimentation and learning. Trying to salvage what we can grasp of the very edges of the metaphor, containers make development easy, allowing developers and operations teams to test new code or configurations locally and safely outside of production and incorporate discovered benefits into production in a way that was difficult in the past. Changes can be radical and still version-controlled, documented, and shared quickly and easily.
Real-world continual experimentation and learning
For example, consider this anecdote from my own experience: Years ago, as a young, inexperienced sysadmin (just three weeks into the job), I was asked to make changes to an Apache virtual host running the website of the central IT department for a university. Without an easy-to-use test environment, I made a configuration change to the production site that I thought would accomplish the task and pushed it out. Within a few minutes, I overheard coworkers in the next cube:
“Wait, is the website down?”
“Hrm, yeah, it looks like it. What the heck?”
There was much eye-rolling involved.
Mortified (the shame is real, folks), I sunk down as far as I could into my seat and furiously tried to back out the changes I’d introduced. Later that same afternoon, the director of the department—the boss of my boss’s boss—appeared in my cube to talk about what had happened. “Don’t worry,” she told me. “We’re not mad at you. It was a mistake and now you have learned.”
In the world of containers, this could have been easily changed and tested on my own laptop and the broken configuration identified by more skilled team members long before it ever made it into production.
DevOps continual experimentation and learning
A real culture of experimentation promotes the individual’s ability to find where a change in the process may be beneficial, and to test that assumption without the fear of retaliation if they fail. For DevOps teams, failure becomes an educational tool that adds to the knowledge of the individual and organization, rather than something to be feared or punished. Individuals in the DevOps team dedicate themselves to continuous learning, which in turn benefits the team and wider organization as that knowledge is shared.
As the metaphor completely falls apart, focus needs to be given to a specific point: The other two principles may appear at first glance to focus entirely on process, but continual learning is a human task—important for the future of the project, the person, the team, and the organization. It has an impact on the process, but it also has an impact on the individual and other people.
“Experimentation and risk-taking are what enable us to relentlessly improve our system of work, which often requires us to do things very differently than how we’ve done it for decades.”
–Gene Kim, et al., The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win, IT Revolution Press, 2013
Containers can teach us DevOps
Learning to work effectively with containers can help teach DevOps and the Three Ways: principles of flow, principles of feedback, and principles of continuous experimentation and learning. Looking holistically at the application and infrastructure rather than putting on blinders to everything outside the container teaches us to take all parts of the system and understand their upstream and downstream impacts, break out of silos, and work as a team to increase global performance and deep understanding of the entire system. Working to provide timely and accurate feedback teaches us to create effective feedback patterns within our organizations to identify problems before their impact grows. Finally, providing a safe environment to try new ideas and learn from them teaches us to create a culture where failure represents a positive addition to our knowledge and the ability to take big chances with educated guesses can result in new, elegant solutions to complex problems.
"
Kubernetes$containers,"5 reasons Kubernetes is the real deal
Kubernetes will be at the heart of a large and growing percentage of infrastructure—on premises and in the cloud.
I've been to a lot of tech conferences in my life, but there was something different about the December 2017 KubeCon/Cloud Native Computing Foundation (CNCF) Summit in Austin. Sure, there's a ton of hype around Kubernetes, but it's something more. Not only did attendance go up by a staggering amount vs. 2016 (there were more than 4,000 people in Austin) but it was about who was and wasn't there. The content was solid, the Linux Foundation did its usual fabulous job running the event, but the real highlight for me was about the who.
Developers showed up in droves, as did end users. It is unique to have such user attendance without a huge influx of sales people and the usual lurker crowd. It frankly reminded me of the early days of VMworld or possibly OpenStack. In a totally unscientific poll of some of the brightest people I know, every single one is convinced that Kubernetes will be at the heart of a large and growing percentage of infrastructure, on premises and in the cloud.
Here are five (plus one) reasons to think Kubernetes is the real deal.
1. Kubernetes solves a REAL problem
The beauty of containers is that they're both cheap to run and easy to spin up and spin down. The one sure thing is that, if you start using containers, you're going to have a lot of them. Not only a lot of them, but also a lot that change.
2. Kubernetes is NOT a science project
Many open source projects come from one developer's crazy/beautiful idea, but leave lots and lots to be done before the use cases are built out and proven. In this case, similar to MapReduce/Hadoop, the primary use case and even most key foundational technical elements have been proved out, in production at Google for years.
3. Commercial support is already here
Like many early adopters of other open source technology, early users of Kubernetes are picking up the raw upstream bits and making it work. But, like hiking with a good buddy, if you stumble, there's someone ready to give you a hand. Red Hat is the clear leader here with an impressive offering in OpenShift. Kudos to them for building a solid commercial offering while being one of the top contributors to the ecosystem (second to Google). But if OpenShift ain't your thing, there are a number of credible startups also ready to help. Google Cloud, Amazon Web Services, Microsoft Azure, VMware, and pretty much every major infrastructure incumbent either already has—or is sprinting to provide—top-shelf support for Kubernetes.
4. Plenty of meat is left on the Kubernetes bone for a thriving ecosystem
As important as container orchestration is, no one would ever argue that it's all you need to run a cloud infrastructure. Hardware, operating system, virtualization, storage, networking, monitoring, services... all represent real money Kubernetes users will spend. There's a race on to show users that X technology is the ""bestest"" partner for your Kubernetes deployment. As the pie continues to grow, look for more pieces—but there should be plenty of money to go around for those successful in showing customer value.
5. There's no elephant in the room
In the early days of an open source project, often one company dominates the community. Naturally, everyone else wonders about their motives, complains about their motives, struggles to get a voice. Often the project's spiritual leader(s) struggle with community vs. monetization. In this case, Google's motives are super clear: Strong Kubernetes adoption is better for Google's Cloud Platform than other options (especially pure VMs). Because this threatens few players in the ecosystem, this means there's a surprisingly low amount of political bickering and positioning in the project compared to what you might expect. This is an impressively functional community compared to other projects. (Looking at you, Node and Docker Moby.)
The fact that Kubernetes is part of larger umbrella (i.e., CNCF) also means that new projects can be incubated outside of Kubernetes, but in the same family. It's a great model and helps deal with the ""focus vs. let 1,000 flowers bloom"" conundrum many open source projects struggle with.

Bonus reason #6
Kelsey Hightower is dope, y'all! Having the industry's most impressive evangelist, able to pull out a brand new demo for every conference he goes to, imminently relatable, widely liked, and representing the project's largest, most respected contributor is a huge boon.
Conclusion
Kubernetes is here to stay. I predict an explosion of interest, investment, and bandwagon jumping to occur around Kubernetes over the next couple years (some may argue I'm a year late with this prediction, and they wouldn't be completely off). The only caution here is that Kubernetes' growth is inherently limited by the speed and appetite for re-writing legacy apps in a cloud-native way. It's going to be a fun few years."
containers$Linux$SysAdmin,"Just say no to root (in containers)
Even smart admins can make bad decisions.
I get asked all the time about the different security measures used to control what container processes on a system can do. Most of these I covered in previous articles on Opensource.com:
Are Docker containers really secure?
Bringing new security features to Docker
Docker security in the future
Almost all of the above articles are about controlling what a privileged process on a system can do in a container. For example: How do I run root in a container and not allow it to break out?
User namespace is all about running privileged processes in containers, such that if they break out, they would no-longer be privileged outside the container. For example, in the container my UID is 0 (zero), but outside of the container my UID is 5000. Due to problems with the lack of file system support, user namespace has not been the panacea it is cracked up to be. Until now.

OpenShift is Red Hat's container platform, built on Kubernetes, Red Hat Enterprise Linux, and OCI containers, and it has a great security feature: By default, no containers are allowed to run as root. An admin can override this, otherwise all user containers run without ever being root. This is particularly important in multi-tenant OpenShift Kubernetes clusters, where a single cluster may be serving multiple applications and multiple development teams. It is not always practical or even advisable for administrators to run separate clusters for each. Sadly one of the biggest complaints about OpenShift is that users can not easily run all of the community container images available at docker.io. This is because the vast majority of container images in the world today require root.
Why do all these images require root?
If you actually examine the reasons to be root, on a system, they are quite limited.
Modify the host system:
One major reason for being root on the system is to change the default settings on the system, like modifying the kernel's configuration.
In Fedora, CentOS, and Red Hat Enterprise Linux, we have the concept of system containers, which are privileged containers that can be installed on a system using the atomic command. They can run fully privileged and are allowed to modify the system as well as the kernel. In the case of system containers we are using the container image as a content delivery system, not really looking for container separation. System containers are more for the core operating system host services as opposed to the users app services that most containers run.
In application containers, we almost never want the processes inside the container to modify the kernel. This is definitely not required by default.
Unix/Linux tradition:
Operating system software vendors and developers have known for a long time running processes as root is dangerous, so the kernel added lots of Linux capabilities to allow a process to start as root and then drop privileges as quickly as possible. Most of the UID/GID capabilities allow a processes like a web service to start as root, then become non-root. This is done to bind to ports below 1024 (more on this later).
Container runtimes can start applications as non-root to begin with. Truth be known, so can systemd, but most software that has been built over the past 20 years assumes it is starting as root and dropping privileges.
Bind to ports < 1024
Way back in the 1960s and 1970s when there were few computers, the inability of unprivileged users to bind to network ports < 1024 was considered a security feature. Because only an admin could do this, you could trust the applications listening on these ports. Ports > 1024 could be bound by any user on the system so they could not be trusted. The security benefits of this are largely gone, but we still live with this restriction.
The biggest problem with this restriction is the web services where people love to have their web servers listening on port 80. This means the main reason apache or nginx start out running as root is so that they can bind to port 80 and then become non-root.
Container runtimes, using port forwarding, can solve this problem. You can set up a container to listen on any network port, and then have the container runtime map that port to port 80 on the host.
In this command, the podman runtime would run an apache_unpriv container on your machine listening on port 80 on the host, while the Apache process inside the container was never root, started as the apache user, and told to listen on port 8080.
podman run -d -p 80:8080 -u apache apache_unpriv
Alternatively:
docker run -d -p 80:8080 -u apache apache_unpriv
Installing software into a container image
When Docker introduced building containers with docker build, the content in the containers was just standard packaged software for distributions. The software usually came via rpm packages or Debian packages. Well, distributions package software to be installed by root. A package expects to be able to do things like manipulate the /etc/passwd file by adding users, and to put down content on the file system with different UID/GIDs. A lot of the software also expects to be started via the init system (systemd) and start as root and then drop privileges after it starts.
Sadly, five years into the container revolution, this is still the status quo. A few years ago, I attempted to get the httpd package to know when it is being installed by a non-root user and to have a different configuration. But I dropped the ball on this. We need to have packagers and package management systems start to understand the difference, and then we could make nice containers that run without requiring root.
One thing we could do now to fix this issue is to separate the build systems from the installing systems. One of my issues with #nobigfatdaemons is that the Docker daemon led to the containers running with the same privs for running a container as it did for building a container image.
If we change the system or use different tools, say Buildah, for building a container with looser constraints and CRI-O/Kubernetes/OpenShift for running the containers in production, then we can build with elevated privileges, but then run the containers with much tighter constraints,or hopefully as a non-root user.
Bottom line
Almost all software you are running in your containers does not require root. Your web applications, databases, load balancers, number crunchers, etc. do not need to be run as root ever. When we get people to start building container images that do not require root at all, and others to base their images off of non-privileged container images, we would see a giant leap in container security.
There is still a lot of educating needing to be done around running root inside of containers. Without education, smart administrators can make bad decisions.

"
SysAdmin$containers,"A sysadmin's guide to containers
What you need to know to understand how containers work.
The term ""containers"" is heavily overused. Also, depending on the context, it can mean different things to different people.
Traditional Linux containers are really just ordinary processes on a Linux system. These groups of processes are isolated from other groups of processes using resource constraints (control groups [cgroups]), Linux security constraints (Unix permissions, capabilities, SELinux, AppArmor, seccomp, etc.), and namespaces (PID, network, mount, etc.).
If you boot a modern Linux system and took a look at any process with cat /proc/PID/cgroup, you see that the process is in a cgroup. If you look at /proc/PID/status, you see capabilities. If you look at /proc/self/attr/current, you see SELinux labels. If you look at /proc/PID/ns, you see the list of namespaces the process is in. So, if you define a container as a process with resource constraints, Linux security constraints, and namespaces, by definition every process on a Linux system is in a container. This is why we often say Linux is containers, containers are Linux. Container runtimes are tools that modify these resource constraints, security, and namespaces and launch the container.
Docker introduced the concept of a container image, which is a standard TAR file that combines:
Rootfs (container root filesystem): A directory on the system that looks like the standard root (/) of the operating system. For example, a directory with /usr, /var, /home, etc.
JSON file (container configuration): Specifies how to run the rootfs; for example, what command or entrypoint to run in the rootfs when the container starts; environment variables to set for the container; the container's working directory; and a few other settings.
Docker ""tar's up"" the rootfs and the JSON file to create the base image. This enables you to install additional content on the rootfs, create a new JSON file, and tar the difference between the original image and the new image with the updated JSON file. This creates a layered image.
The definition of a container image was eventually standardized by the Open Container Initiative (OCI) standards body as the OCI Image Specification.
Tools used to create container images are called container image builders. Sometimes container engines perform this task, but several standalone tools are available that can build container images.
Docker took these container images (tarballs) and moved them to a web service from which they could be pulled, developed a protocol to pull them, and called the web service a container registry.
Container engines are programs that can pull container images from container registries and reassemble them onto container storage. Container engines also launch container runtimes (see below).

Container storage is usually a copy-on-write (COW) layered filesystem. When you pull down a container image from a container registry, you first need to untar the rootfs and place it on disk. If you have multiple layers that make up your image, each layer is downloaded and stored on a different layer on the COW filesystem. The COW filesystem allows each layer to be stored separately, which maximizes sharing for layered images. Container engines often support multiple types of container storage, including overlay, devicemapper, btrfs, aufs, and zfs.
After the container engine downloads the container image to container storage, it needs to create a container runtime configuration. The runtime configuration combines input from the caller/user along with the content of the container image specification. For example, the caller might want to specify modifications to a running container's security, add additional environment variables, or mount volumes to the container.

The layout of the container runtime configuration and the exploded rootfs have also been standardized by the OCI standards body as the OCI Runtime Specification.
Finally, the container engine launches a container runtime that reads the container runtime specification; modifies the Linux cgroups, Linux security constraints, and namespaces; and launches the container command to create the container's PID 1. At this point, the container engine can relay stdin/stdout back to the caller and control the container (e.g., stop, start, attach).
Note that many new container runtimes are being introduced to use different parts of Linux to isolate containers. People can now run containers using KVM separation (think mini virtual machines) or they can use other hypervisor strategies (like intercepting all system calls from processes in containers). Since we have a standard runtime specification, these tools can all be launched by the same container engines. Even Windows can use the OCI Runtime Specification for launching Windows containers.
At a much higher level are container orchestrators. Container orchestrators are tools used to coordinate the execution of containers on multiple different nodes. Container orchestrators talk to container engines to manage containers. Orchestrators tell the container engines to start containers and wire their networks together. Orchestrators can monitor the containers and launch additional containers as the load increases.
"
containers$Linux,"A history of low-level Linux container runtimes
""Container runtime"" is an overloaded term.
At Red Hat we like to say, ""Containers are Linux—Linux is Containers."" Here is what this means. Traditional containers are processes on a system that usually have the following three characteristics:
1. Resource constraints
When you run lots of containers on a system, you do not want to have any container monopolize the operating system, so we use resource constraints to control things like CPU, memory, network bandwidth, etc. The Linux kernel provides the cgroups feature, which can be configured to control the container process resources.

2. Security constraints
Usually, you do not want your containers being able to attack each other or attack the host system. We take advantage of several features of the Linux kernel to set up security separation, such as SELinux, seccomp, capabilities, etc.
3. Virtual separation
Container processes should not have a view of any processes outside the container. They should be on their own network. Container processes need to be able to bind to port 80 in different containers. Each container needs a different view of its image, needs its own root filesystem (rootfs). In Linux we use kernel namespaces to provide virtual separation.
Therefore, a process that runs in a cgroup, has security settings, and runs in namespaces can be called a container. Looking at PID 1, systemd, on a Red Hat Enterprise Linux 7 system, you see that systemd runs in a cgroup.
# tail -1 /proc/1/cgroup
1:name=systemd:/
The ps command shows you that the system process has an SELinux label ...
# ps -eZ | grep systemd
system_u:system_r:init_t:s0             1 ?     00:00:48 systemd
and capabilities.
# grep Cap /proc/1/status
...
CapEff: 0000001fffffffff
CapBnd: 0000001fffffffff
CapBnd:    0000003fffffffff
Finally, if you look at the /proc/1/ns subdir, you will see the namespace that systemd runs in.
ls -l /proc/1/ns
lrwxrwxrwx. 1 root root 0 Jan 11 11:46 mnt -> mnt:[4026531840]
lrwxrwxrwx. 1 root root 0 Jan 11 11:46 net -> net:[4026532009]
lrwxrwxrwx. 1 root root 0 Jan 11 11:46 pid -> pid:[4026531836]
...
If PID 1 (and really every other process on the system) has resource constraints, security settings, and namespaces, I argue that every process on the system is in a container.
Container runtime tools just modify these resource constraints, security settings, and namespaces. Then the Linux kernel executes the processes. After the container is launched, the container runtime can monitor PID 1 inside the container or the container's stdin/stdout—the container runtime manages the lifecycles of these processes.
Container runtimes
You might say to yourself, well systemd sounds pretty similar to a container runtime. Well, after having several email discussions about why container runtimes do not use systemd-nspawn as a tool for launching containers, I decided it would be worth discussing container runtimes and giving some historical context.
Docker is often called a container runtime, but ""container runtime"" is an overloaded term. When folks talk about a ""container runtime,"" they're really talking about higher-level tools like Docker, CRI-O, and RKT that come with developer functionality. They are API driven. They include concepts like pulling the container image from the container registry, setting up the storage, and finally launching the container. Launching the container often involves running a specialized tool that configures the kernel to run the container, and these are also referred to as ""container runtimes."" I will refer to them as ""low-level container runtimes."" Daemons like Docker and CRI-O, as well as command-line tools like Podman and Buildah, should probably be called ""container managers"" instead.
When Docker was originally written, it launched containers using the lxc toolset, which predates systemd-nspawn. Red Hat's original work with Docker was to try to integrate libvirt (libvirt-lxc) into Docker as an alternative to the lxc tools, which were not supported in RHEL. libvirt-lxc also did not use systemd-nspawn. At that time, the systemd team was saying that systemd-nspawn was only a tool for testing, not for production.
At the same time, the upstream Docker developers, including some members of my Red Hat team, decided they wanted a golang-native way to launch containers, rather than launching a separate application. Work began on libcontainer, as a native golang library for launching containers. Red Hat engineering decided that this was the best path forward and dropped libvirt-lxc.
Later, the Open Container Initiative (OCI) was formed, party because people wanted to be able to launch containers in additional ways. Traditional namespace-separated containers were popular, but people also had the desire for virtual machine-level isolation. Intel and Hyper.sh were working on KVM-separated containers, and Microsoft was working on Windows-based containers. The OCI wanted a standard specification defining what a container is, so the OCI Runtime Specification was born.
The OCI Runtime Specification defines a JSON file format that describes what binary should be run, how it should be contained, and the location of the rootfs of the container. Tools can generate this JSON file. Then other tools can read this JSON file and execute a container on the rootfs. The libcontainer parts of Docker were broken out and donated to the OCI. The upstream Docker engineers and our engineers helped create a new frontend tool to read the OCI Runtime Specification JSON file and interact with libcontainer to run the container. This tool, called runc, was also donated to the OCI. While runc can read the OCI JSON file, users are left to generate it themselves. runc has since become the most popular low-level container runtime. Almost all container-management tools support runc, including CRI-O, Docker, Buildah, Podman, and Cloud Foundry Garden. Since then, other tools have also implemented the OCI Runtime Spec to execute OCI-compliant containers.
Both Clear Containers and Hyper.sh's runV tools were created to use the OCI Runtime Specification to execute KVM-based containers, and they are combining their efforts in a new project called Kata. Last year, Oracle created a demonstration version of an OCI runtime tool called RailCar, written in Rust. It's been two months since the GitHub project has been updated, so it's unclear if it is still in development. A couple of years ago, Vincent Batts worked on adding a tool, nspawn-oci, that interpreted an OCI Runtime Specification file and launched systemd-nspawn, but no one really picked up on it, and it was not a native implementation.
If someone wants to implement a native systemd-nspawn --oci OCI-SPEC.json and get it accepted by the systemd team for support, then CRI-O, Docker, and eventually Podman would be able to use it in addition to runc and Clear Container/runV (Kata). (No one on my team is working on this.)
The bottom line is, back three or four years, the upstream developers wanted to write a low-level golang tool for launching containers, and this tool ended up becoming runc. Those developers at the time had a C-based tool for doing this called lxc and moved away from it. I am pretty sure that at the time they made the decision to build libcontainer, they would not have been interested in systemd-nspawn or any other non-native (golang) way of running ""namespace"" separated containers.

"
Kubernetes$containers$microservices,"How Kubernetes became the solution for migrating legacy applications
You don't have to tear down your monolith to modernize it. You can evolve it into a beautiful microservice using cloud-native technologies.
In the early days of the internet, if you wanted to launch an application, you had to buy or rent hardware. This was a physical server or a rack of servers, and you needed one server per application, so it was expensive. In 2001, VMware came out with virtualization—software that allowed users to run multiple applications on the same hardware. This meant you could split up a single box into multiple virtual boxes, each running its own environment and applications. The cost savings for businesses were tremendous.
Fast forward to 2006. Amazon popularized the concept of Infrastructure as a Service (IaaS) with Amazon Web Services and its Elastic Compute Cloud (EC2). You no longer had to buy your own hardware. You didn't even have to worry about running and managing virtual machines that run your applications. You were literally renting the computing environment and underlying infrastructure needed to run your services. You paid by the hour, like renting a conference room. This allowed companies to optimize resources to reduce costs and buy only as much computing power as they needed. It was revolutionary and led to an astounding decline in the cost of computing.
Three years after that, Heroku came up with the idea of Platform as a Service (PaaS). PaaS operated a layer above EC2 by removing administration of the virtual operating system. Heroku magically simplified deploying a new version of your application; now you only needed to type git push heroku. Some of the best-known web companies originated on Heroku.
These advances made deploying applications at any scale—large or small—much easier and more affordable. They led to the creation of many new businesses and powered a radical shift in how companies treated infrastructure, moving it from a capital expense to a variable operating expense.
As good as it all sounds, there was one big problem. All these providers were proprietary companies. It was vendor lock-in. Moving applications between the various environments was difficult. And mixing and matching on-premises and cloud-based applications was nearly impossible.
That's when open source stepped in. It was Linux all over again, except the software was Docker and Kubernetes, the Linux of the cloud.
Open source to the rescue
Emerging in 2013, Docker popularized the concept of containers. The name ""containers"" echoes the global revolution when shipping companies replaced ad hoc freight loading with large metal boxes that fit on ships, trucks, and rail cars. The payload didn't matter. The box was the standard.
Similar to shipping containers, Docker containers create a basic computing wrapper that can run on any infrastructure. Containers took the world by storm. Today, nearly all enterprises are planning future applications on top of container infrastructures—even if they are running them on their own private hardware. It's just a better way to manage modern applications, which are often sliced and diced into myriad components (microservices) that need to be easily moved around and connected.

This led to another problem. Managing containers taxed DevOps teams like nothing before. It created a step change in the number of moving parts and the dynamic activities around application deployment. Enter Kubernetes.
In 2015, Google released Kubernetes as an open source project. It was an implementation of Google's internal system called Borg. Google and the Linux Foundation created the Cloud-Native Computing Foundation (CNCF) to host Kubernetes (and other cloud-native projects) as an independent project governed by a community around it. Kubernetes quickly became one of the fastest growing open source projects in history, growing to thousands of contributors across dozens of companies and organizations.
What makes Kubernetes so incredible is its implementation of Google's own experience with Borg. Nothing beats the scale of Google. Borg launches more than 2-billion containers per week, an average of 3,300 per second. At its peak, it's many, many more. Kubernetes was born in a cauldron of fire, battle-tested and ready for massive workloads.
Building on the core ideas of Dockers and Kubernetes, CNCF became the home to many other cloud-native projects. Today there are more than 369 big and small projects in the cloud-native space. The critical cloud-native projects hosted by the CNCF include Kubernetes, Prometheus, OpenTracing, Fluentd, Linkerd, gRPC, CoreDNS, rkt, containerd, Container Networking Interface, CoreDNS, Envoy, Jaeger, Notary, The Update Framework, Rook, and Vitess.
However, learning from the mistakes of other open source projects, CNCF has been extra careful to ensure it selects only those technologies that work well together and can meet enterprises' and startups' needs. These technologies are enjoying mass adoption.
One of the biggest reasons companies flock to open source technologies is to avoid vendor lock-in and be able to move their containers across clouds and private infrastructure. With open source, you can easily switch vendors, or you can use a mix of vendors. If you have skills, you can manage your stack yourself.
Slicing off pieces of the monolith
Kubernetes and containers didn't only change the ability to manage at scale, but also to take massive, monolithic applications and more easily slice them into more manageable microservices. Each service can be managed to scale up and down as needed. Microservices also allow for faster deployments and faster iteration in keeping with modern continuous integration practices. Kubernetes-based orchestration allows you to improve efficiency and resource utilization by dynamically managing and scheduling these microservices. It also adds an extraordinary level of resiliency. You don't have to worry about container failure, and you can continue to operate as demand goes up and down.
Kubernetes has become the leading choice for cloud-native orchestration. It's also one of the highest velocity development projects in the history of open source and is backed by major players including AWS, Microsoft, Red Hat, SUSE, and many more.
All of this has a direct impact on businesses. According to Puppet's 2016 State of DevOps report, high-performing cloud-native architectures can have much more frequent developments, shorter lead times, lower failure rates, and faster recovery from failures. That means features get to market faster, projects can pivot faster, and engineering and developer teams wait around a lot less. Today, if you are building a new application from scratch, the cloud-native application architecture is the right way to do it. Equally important, cloud-native thinking provides a roadmap for how to take existing (brownfield) applications and slowly convert them into more efficient and resilient microservices-based architectures running on containers and Kubernetes. Brownfield, monolithic applications make up the majority of all software products today.
Monoliths are the antithesis of cloud-native. They are expensive, inflexible, tightly coupled, and brittle. The question is: how to break these monoliths into microservices? You might want to rewrite all your large legacy applications. The fact is, most rewrites end in failure. The first system, which you are trying to rewrite, is alive and evolving even as you try to replace it. Sometimes that first system evolves too quickly, and you can never catch up.
You can solve this problem in a more efficient manner. First, stop adding significant new functionality to your existing monolithic applications. There is a concept of ""lift and shift."" This means you can take a large application that requires gigabytes of RAM and wrap a container around it. Simple.
Examples of transitioning from monolithic to containers
Ticketmaster is a good example of that approach. It has code running on a PDP-11. It created a PDP-11 emulator running inside a Docker container to be able to containerize that legacy application. There is a specific technology for Kubernetes, called Stateful Sets (formerly known as PetSets), that allows you to lock a container to one piece of hardware to make sure that it has active performance.
Ticketmaster had a unique problem: Whenever it put tickets up for sale, it was essentially launching a distributed denial of service (DDoS) attack against itself because of all the people coming in. The company needed a set of frontend servers that could scale up and handle that demand rather than trying to write it in their legacy application. Instead, it deployed a new set of microservice containers in front of the legacy app for this purpose, minimizing ongoing sprawl in its legacy architecture.
As you are trying to move your workload from legacy applications to containers, you may also want to move some functionality from the application into microservices or use microservices to add new functionality, rather than add to the old codebase. For example, if you want to add OAuth functionality, there may be a simple Node.js application you can put in front of your legacy app. If you have a highly performance-sensitive task, you can write it in Golang and set it up as an API-driven service residing in front of your legacy monolith. You will still get API calls back to your existing legacy monolith.
These new functionalities can be written in more modern languages by different teams that can work with their own set of libraries and dependencies and start splitting up that monolith.
KeyBanc in North Carolina is a great example of this approach. It deployed Node.js application servers in front of its legacy Java application to handle mobile clients. This was simpler and more efficient than adding to legacy code and helped future-proof its infrastructure.
True cloud-native is a constellation of complementary projects
If you are moving into cloud-native technology, you should consider a constellation of complementary projects to deliver core functionality. For example, among the biggest priorities in a cloud-native environment are monitoring, tracing, and logging. You can go with Prometheus, OpenTracing, and Fluentd. Linkerd is a service mesh that supports more complicated versions of routing. gRPC is an extremely high-performance API system that can replace JSON responses for applications that need higher throughput. CoreDNS is a service-discovery platform. These are all part of CNCF today, and we expect to add more projects to our set of complementary solutions.
Greenfield, brownfield, any field can be cloud-native
When you are migrating legacy monoliths to cloud-native microservices, you don't need to go greenfield and rewrite everything. CNCF technologies like Kubernetes love brownfield applications. There is an evolutionary path that almost every enterprise and company out there should be on. You can stand up a brand-new application in Kubernetes or steadily evolve your monolith into a beautiful mesh of cloud-native microservices that will serve you for years to come.
"
Scale$Kubernetes$containers,"You got your VM in my container
Explore KubeVirt and Kata Containers, two fairly new projects that aim to combine Kubernetes with virtualization.
Steve Gordon co-wrote this article.
Containers and Kubernetes have been widely promoted as ""disruptive"" technologies that will replace everything that preceded them, most notably virtual machine (VM) management platforms such as vSphere and OpenStack. Instead, as with most platform innovations, Kubernetes is more often used to add a layer to (or complement) VMs. In this article, and in a presentation at SCALE16x, we'll be exploring two relatively new projects that aim to assist users in combining Kubernetes with virtualization: KubeVirt and Kata Containers.
Most organizations still have large existing investments in applications that run on virtualized hosts, infrastructure that runs them, and tools to manage them. We can envision this being true for a long time to come, just as remnants of previous generations of technology remain in place now. Additionally, VM technology still offers a level of isolation that container-enablement features, like user namespaces, have yet to meet. However, those same organizations want the ease-of-use, scalability, and developer appeal of Kubernetes, as well as a way to gradually transition from virtualized workloads to containerized ones.
Kubernetes' recently added Service Catalog feature addresses integration to an extent. This feature allows Kubernetes to create ""endpoints"" so containerized applications can route requests to applications running elsewhere, such as on an OpenStack cluster. However, this approach doesn't provide a way to unify management of virtualized and containerized applications, nor does it provide any way for Kubernetes applications to be a part of the VM platform's environment and namespace.
Developers have launched several projects in the last year to meet these integration requirements. While there are many differences in technical details, at a high level these projects can be divided into two contrasting usage targets:
Running traditional VM workloads alongside application containers as part of a complex application
Running application container workloads with hardware-assisted VM-level isolation for security and/or resource management
These use cases imply different types of source images and user expectations in terms of workflow, startup speed, and memory overhead.
In the traditional VM workload, the user expects to be able to run an existing VM image—or at least trivially convert one—that contains a full operating system and the application and comes with the boot time and memory overhead that implies. This may even be a different operating system from the host, and it may have specific requirements regarding virtual devices and memory addresses that can't be realized using containers.

In the application container workload case, users expect to be able to run an Open Containers Initiative-compliant container image as a VM. They also expect a fast launch like any other application container and a low memory overhead that more directly maps to what the application will use. Their reason for using a VM is that they want hardware-enforced isolation of workloads, for either security or performance.
Let's compare two such projects: KubeVirt for the traditional VM use case and Kata Containers for the isolation use case.
KubeVirt for traditional VMs
The KubeVirt project was launched by three Red Hat engineers in late 2016. The project's goal was to help enterprises move from a VM-based infrastructure to a Kubernetes-and-container-based stack, one application at a time. This meant providing a mechanism to treat applications built by existing VM development workflows like native Kubernetes applications, including management and routing. At the same time, many of these applications require a significant amount of VM-native configuration to function.
Kubernetes 1.7, released in June 2017, included Custom Resource Definitions (CRDs), the project's most powerful extension API. CRDs basically let developers create an entirely new type of object for the Kubernetes system and define the characteristics and functionality of that object, then load it into a running cluster. For the KubeVirt contributors, who wanted its VMs to behave like VMs, CRD was the perfect interface, and the project was refactored around it.
Users who have existing KVM-based VM workflows would need to add one automated step, which makes some changes to the image for the KubeVirt environment. Once that's done, the image can be deployed to the Kubernetes cluster using a YAML manifest, just like other Kubernetes objects. KubeVirt ""wraps"" each deployed VM in a pod, the basic unit of deployment in Kubernetes, that would normally contain one to several containers instead of a VM. This allows for almost-seamless integration with other services deployed as normal container-based pods. Assigned virtual disks become Kubernetes Persistent Volumes and can take advantage of any storage already available to the cluster.
The CRD mechanism also means that the KubeVirt project can define extra ""knobs"" for tuning behaviors that aren't available in regular containers, such as virtual devices and CPU characteristics. While not all this functionality is complete, existing features include:
Creation of VMs using different operating systems, including Windows
Configuration of CPU, RAM, and a virtual disk for each VM
Connecting the VM directly to a serial or network console
Using cloud-init for VM boot-time configuration
More importantly, KubeVirt VMs can be examined and controlled using Kubernetes' standard command-line interface, kubectl, as if they were regular pods. Because administrators also sometimes need to connect to the ""console"" for the VM, KubeVirt comes with an additional CLI tool, virtctl, which supports this:
virtctl console --kubeconfig=$KUBECONFIG testvm
The KubeVirt project is in rapid development and looking for more contributors. You can connect with the team via its GitHub repo and the forums mentioned there, or in the #virtualization channel in Kubernetes' Slack.
Kata Containers: VMs for isolation
Like VMs, containers are used to run application clouds. Container clouds are popular because of very rapid, automated container deployment and startup, as well as lower CPU and memory usage. These enable development and management models that are unattainable in a VM environment. However, secure isolation of workloads is also a goal of many cloud owners, which is more difficult to achieve in a container stack. The Kata Containers project aims to provide the security isolation of VMs with the speed of containers.
The Linux kernel on the host machine isolates workloads running in various application containers from each other using kernel namespaces, as well as various kernel process confinement technologies like SELinux and SECCOMP. While namespace isolation in the kernel continues to improve, it still does not—and arguably never will—offer the same level of isolation as hardware-assisted virtualization running on modern CPUs. Both hostile scripts and application bugs in a container environment will still have access to any unpatched kernel vulnerabilities, if nothing else.
A KubeVirt-like approach provides VM-level security isolation, but it also burdens administrators with additional management, slower deployments, and slower launch times for the VM pods. For developers who want rapid, container-based development workflows and don't care about compatibility with legacy platforms, that's a poor tradeoff. Kata Containers takes a different approach to gain container-like speed, using a stripped-down VM platform and a different Kubernetes API.
Intel launched a container project called Clear Containers in 2015. Part of Intel's Clear Linux initiative, Clear Containers implemented an approach to secure containers that took advantage of Intel CPU virtualization hardware. Concurrently, a Beijing-based startup called HyperHQ launched Hyper, which took a similar approach to solving the container isolation problem. At KubeCon, in December 2017, the two projects announced they were merging as the Kata Containers project, hosted by the OpenStack Foundation.
Since the Kata Containers project is still in early development, we'll explain the workflows and architecture of Clear Containers 3.0, with the belief that the eventual release version of Kata Containers will be very similar.
Rather than providing top-level API objects via a CRD like KubeVirt, Clear Containers 3.0 provides an Open Containers Initiative (OCI)-compatible runtime implementation called cc-runtime. It then plugs into Kubernetes using a different extension API, the Container Runtime Interface (CRI), which provides Kubernetes with its ability to run on different container platforms, including Docker, containerd, rkt, and CRI-O. In effect, Clear Containers VMs are treated by Kubernetes as just a different ""flavor"" of container. Practically, this does also entail some extra configuration of each Kubernetes node.
The cc-runtime ""containers"" run a ""mini O/S"" image, which, although technically a full VM, contains only a cut-down Linux kernel and little other operating system machinery. Combined with optimizations added to Intel's virtualization hardware, this allows the container VMs to launch much faster than traditional VMs, while also using fewer system resources. While they are slower than regular Linux containers, they are fast enough to integrate into container-based development workflows.
In a Kubernetes-Clear Containers cluster, pods can be specified as either trusted or untrusted. Trusted pods use a regular runc-based container runtime. For untrusted pods, the runtime creates a lightweight VM and then spawns all the containers in the pod inside that VM. Since Kubernetes clusters can be configured as trusted or untrusted by default, this means that administrators can add the additional isolation of hardware-assisted virtualization without having to change the workload definitions at all.
In the coming months, Intel, HyperHQ, and a growing number of supporting companies will be working to merge the Clear Containers and Hyper implementations into the new Kata Containers project.
VM-container fusion and the future
While both KubeVirt and Kata Containers share ideas, some of their supporting companies, and a desire to integrate VMs and containers, it seems unlikely that the two fundamentally different approaches can ever be reconciled. KubeVirt aims to provide as much VM functionality as possible, while Kata Containers tries to provide a container-like experience for VMs. As with other irreconcilable architecture tradeoffs, we can expect to see some users employing both approaches concurrently for different workloads.
There are, of course, other tools and implementations that aim to fuse VMs and containers, such as Virtlet and RancherVM. Generally speaking, these projects take one of the two approaches above, although they may have specific details that differ from KubeVirt and Kata Containers. Regardless of the tools employed, both VM admins who want to move to Kubernetes and container developers who want better isolation can look forward to an easier-to-manage and a more unified future.
"
DevOps$Agile,"What is Small Scale Scrum?
Here's how the scrum agile methodology can help teams of three or fewer work more efficiently.
Agile is fast becoming a mainstream way industries act, behave, and work as they look to improve efficiency, minimize costs, and empower staff. Most software developers naturally think, act, and work this way, and alignment towards agile software methodologies has gathered pace in recent years.
VersionOne’s 2018 State of Agile report shows that scrum and its variants remain the most popular implementation of agile. This is in part due to changes made to the Scrum Guide’s wording in recent years that make it more amenable to non-software industries.
The guide also changed its stance towards the team size needed to implement scrum: The previous recommended team size of 7±2 changed to a range of 3-9 people. 
[Download the Introduction to Small Scale Scrum guide]
Like in agile, scale is a hot topic in scrum, with a battleground of competing frameworks vying for supremacy. Scaling frameworks have focused on allowing multiple teams to coordinate in a seamless manner and, in essence, promote scrum at a large scale. However, scaling down (below the recommended minimum team size) is not gaining traction, even though it is the way many sectors operate.
For example, paid-for consultancy work typically involves one or two people working on a short-term project; since consultant costs are often charged by the hour or day, a smaller team provides maximum value for the customer. Open source contributors very often work alone, albeit part of a much larger community. And college students completing final-year projects or research assignments usually work in solo mode, with small teams forming in some cases.
All of these formats can follow an agile way of work. Scrum’s principles and execution have been applied to small-scale teams; however, they’re often applied in a way that leads to something slipping. In our experience, that is quality. We set out to investigate whether we could maintain a high level of quality and output with a reduced team size.
What is Small Scale Scrum?
The result of our research is Small Scale Scrum, a long-awaited and novel concept in agile “supporting planning, developing, and delivering production-quality software solutions.” Small, in this case, is a maximum of three people. According to an extensive survey, this is something the industry, customers, and small development teams have been asking for, as this team size is more realistic for their needs. Organizations that engage in consultancy projects are particularly asking for ways to run scrum projects with one or two developers, due to the industry-wide acknowledgment of the approach’s efficiency. Given that scrum has amended its guidebook to include non-software industries, this approach could benefit many different sectors.
"
DevOps$DevSecOps,"What is DevSecOps?
The journey to DevSecOps begins with empowerment, enablement, and education. Here's how to get started.
DevSecOps as a practice or an art form is an evolution on the concept of DevOps. To better understand DevSecOps, you should first have an understanding of what DevOps means.
DevOps was born from merging the practices of development and operations, removing the silos, aligning the focus, and improving efficiency and performance of both the teams and the product. A new synergy was formed, with DevOps focused on building products and services that are easy to maintain and that automate typical operations functions.
Security is a common silo in many organizations. Security’s core focus is protecting the organization, and sometimes this means creating barriers or policies that slow down the execution of new services or products to ensure that everything is well understood and done safely and that nothing introduces unnecessary risk to the organization.
Because of the distinct nature of the security silo and the friction it can introduce, development and operations sometimes bypass or work around security to meet their objectives. At some firms, the silo creates an expectation that security is entirely the responsibility of the security team and it is up to them to figure out what security defects or issues may be introduced as a result of a product.

DevSecOps looks at merging the security discipline within DevOps. By enhancing or building security into the developer and/or operational role, or including a security role within the product engineering team, security naturally finds itself in the product by design.
This allows companies to release new products and updates more quickly and with full confidence that security is embedded into the product.
Where does rugged software fit into DevSecOps?
Building rugged software is more an aspect of the DevOps culture than a distinct practice, and it complements and enhances a DevSecOps practice. Think of a rugged product as something that has been battle-hardened through experimentation or experience.
It’s important to note that rugged software is not necessarily 100% secure (although it may have been at some point in time). However, it has been designed to handle most of what is thrown at it.
The key tenets of a rugged software practice are fostering competition, experimentation, controlled failure, and cooperation.
How do you get started in DevSecOps?
Gettings started with DevSecOps involves shifting security requirements and execution to the earliest possible stage in the development process. It ultimately creates a shift in culture where security becomes everyone’s responsibility, not only the security team’s.
You may have heard teams talking about a ""shift left."" If you flatten the development pipeline into a horizontal line to include the key stages of the product evolution—from initiation to design, building, testing, and finally to operating—the goal of a security is to be involved as early as possible. This allows the risks to be better evaluated, socialized, and mitigated by design. The ""shift-left"" mentality is about moving this engagement far left in this pipeline.
This journey begins with three key elements:
empowerment
enablement
education
Empowerment, in my view, is about releasing control and allowing teams to make independent decisions without fear of failure or repercussion (within reason). The only caveat in this process is that information is critical to making informed decisions (more on that below).
To achieve empowerment, business and executive support (which can be created through internal sales, presentations, and establishing metrics to show the return on this investment) is critical to break down the historic barriers and siloed teams. Integrating security into the development and operations teams and increasing both communication and transparency can help you begin the journey to DevSecOps.
This integration and mobilization allows teams to focus on a single outcome: Building a product for which they share responsibility and collaborate on development and security in a reliable way. This will take you most of the way towards empowerment. It places the shared responsibility for the product with the teams building it and ensures that any part of the product can be taken apart and maintain its security.
Enablement involves placing the right tools and resources in the hands of the teams. It’s about creating a culture of knowledge-sharing through forums, wikis, and informal gatherings.
Creating a culture that focuses on automation and the concept that repetitive tasks should be coded will likely reduce operational overhead and strengthen security. This scenario is about more than providing knowledge; it is about making this knowledge highly accessible through multiple channels and mediums (which are enabled through tools) so that it can be consumed and shared in whatever way teams or individuals prefer. One medium might work best when team members are coding and another when they are on the road. Make the tools accessible and simple and let the team play with them.
Different DevSecOp teams will have different preferences, so allow them to be independent whenever possible. This is a delicate balancing exercise because you do want economies of scale and the ability to share among products. Collaboration and involvement in the selection and renewal of these tools will help lower the barriers of adoption.

Finally, and perhaps most importantly, DevSecOps is about training and awareness building. Meetups, social gatherings, or formal presentations within the organization are great ways for peers to teach and share their learnings. Sometimes these highlight shared challenges, concerns, or risks others may not have considered. Sharing and teaching are also effective ways to learn and to mentor teams.
In my experience, each organization's culture is unique, so you can’t take a “one-size-fits-all” approach. Reach out to your teams and find out what tools they want to use. Test different forums and gatherings and see what works best for your culture. Seek feedback and ask the teams what is working, what they like, and why. Adapt and learn, be positive, and never stop trying, and you’ll almost always succeed."
DevOps$AI$machinelearning,"Getting started with predictive analytics in DevOps
Use machine data to quickly respond to problems and identify when human involvement may be needed.
Data—it's the new currency. Many years ago, we measured the volume of data we processed in gigabytes; then we quickly moved to terabytes. Due to the influence of the smartphone and mobile devices, our volume of data is rapidly increasing to petabytes.
In addition to managing the size of our data, we need to process various kinds of data and begin to understand what data can tell us. One opportunity in DevOps is to analyze this large amount of machine data. More importantly, machine data, such as logs and metrics of multiple infrastructure-monitoring tools, can continue operating the current IT system throughout the hybrid cloud. Another opportunity is to use this machine data to quickly respond to problems and identify when human involvement may be needed.
It is critical to transform any DevOps initiative by using machine data, especially logs and metrics. With advanced analysis capabilities based on machine data, DevOps engineers or site reliability engineers (SREs) can understand these petabytes of data using statistics, indexing, filtering, and many other machine learning techniques.

Predictive analysis is a key area of advanced analytics that's used to make predictions about unknown future events. It analyzes current data from multiple applications across hybrid infrastructures and makes predictions about the future using methods such as data mining, statistics, modeling, deep learning, and artificial intelligence.
Predictive analysis software tools offer advanced analytical functions such as data mining, deep learning, statistical analysis, real-time scoring, predictive modeling, and optimization. They enable you to gather, analyze, and mine structured and unstructured data on what has happened and predict what is likely to happen based on past events in the applications in your infrastructure. The prediction model is typically composed of the classification and the regression models.
Here are three popular open source predictive analytics tools you can use as part of your DevOps initiative.
Anaconda is an open data science Python platform. It is a high-performance distribution of Python and R and contains more than 100 of the most popular Python, R, and Scala packages for data science.
H2O is an open source, scalable machine learning API for smarter applications including deep learning, gradient boost, random forest, and generalized linear modeling. It makes it easy to apply machine learning and predictive analysis.
Apache Mahout builds an environment that can quickly create a scalable performance machine learning application. It is a simple and extensible programming environment and framework for building scalable algorithms and includes various pre-algorithms for Scala, Apache Spark, H2O, and Apache Flink.
Adopting predictive analytics in the DevOps initiative is critical for many companies to improve efficiency and customer satisfaction. The rate of change is increasing due to new business models that continue to test new technologies, new competitors, and existing organizations in the market. Therefore, organizations must be able to respond quickly to changes while maintaining lower costs and higher quality than their competitors.
Predictive analytics are mainly used in DevOps initiatives to accelerate the application delivery capabilities in terms of tracking, security, quality, and performance. For example, if an automated testing tool in a DevOps pipeline detects new errors using a deep learning algorithm and alerts the QA team, they can accelerate the bug-fix process by creating a test pattern library. This capability improves test efficiency, application quality, and reduces go-to-market time. Another example might result in automatic provisioning of additional resources at peak loads or elimination of excessive resources during idle periods. It can also detect security problems, including the start of DDoS attacks and memory leaks.
As a result, DevOps engineers can benefit by identifying and addressing potential issues such as unexpected error codes, extended build time, reduced release speed, other bottlenecks, and unnecessary, time-consuming tasks.
Conclusion
Using predictive analytics in your DevOps initiative can provide great benefits in the software delivery lifecycle. These include eliminating technical debt by managing wasteful software development and reducing unnecessary alert storms with patterning to highlight only necessary alerts. In the end, the right predictive monitoring tool can trigger events earlier and prevent production failures.

"
RaspberryPi$AI$machinelearning,"3 cool machine learning projects using TensorFlow and the Raspberry Pi
TensorFlow and the Raspberry Pi are working together in the city and on the farm. Learn about three recent, innovative projects.
In early 2017, the Raspberry Pi Foundation announced a Google developer survey, which requested feedback from the maker community on what tools they wanted on the Raspberry Pi. The blog post says that Google has developed tools for machine learning, IoT, wearables, robotics, and home automation, and that the survey mentions face- and emotion-recognition, speech-to-text translation, natural language processing, and sentiment analysis. ""The survey will help them get a feel for the Raspberry Pi community, but it'll also help us get the kinds of services we need,"" the post explains. Meanwhile, data scientists aren't waiting around to put Google's TensorFlow, an open source software library for machine learning, to work on the Raspberry Pi.
Let's take a look at a few cool examples of machine learning with TensorFlow on the Raspberry Pi.
Caltrain project
The Caltrain project is a Silicon Valley Data Science trainspotting project. In an introduction to the project article series, data scientists Chloe Mawer, Matthew Rubashkin, and Colin Higgins write, ""Our interest stems from the fact that half of our employees rely on the Caltrain to get to work each day. We also want to give back to the community, and we love when we can do that with data.""
Because delay estimates provided by Caltrain ""can be a bit ... off"" (their words), the team wanted to integrate new data sources for delay predictions beyond what was available from Caltrain's API. They outline three questions they wanted an IoT Raspberry Pi train detector to answer:
Is there a train passing?
Which direction is it going?
How fast is the train moving?
Then they broke their process into a series of posts, which includes Image Processing in Python and Streaming Video Analysis with Python.
Then, in TensorFlow Image Recognition on a Raspberry Pi, Rubashkin writes, ""When we set up a new Raspberry Pi in our Mountain View office, we ran into a big problem: the Pi was not only detecting Caltrains (true positive), but also detecting Union Pacific freight trains and the VTA light rail (false positive). In order to reliably detect Caltrain delays, we would have to reliably classify the different trains."" He says 2016 was a good year to run into this classification problem because several deep learning image recognition technologies had been released as open source projects, including Google's TensorFlow.
To get experience creating an image classifier using TensorFlow, Rubashkin used the flowers tutorial on the tutorials page in TensorFlow for Poets, by Pete Warden. Then to get the train training data set, Rubashkin used 1,000 images from Google Images to create a vehicle classifier for Caltrains, freight trains, light rail, trucks, and cars.Read TensorFlow Image Recognition on a Raspberry Pi to learn more about how the Rubashkin tested and deployed the model, and his experience troubleshooting TensorFlow on the Raspberry Pi.
Meter Maid Monitor
At the TechCrunch Disrupt Hackathon in September 2016, John Naulty introduced Meter Maid Monitor. On the project website, Naulty says Meter Maid Monitor, which combines TensorFlow image classification with a Raspberry Pi motion detection and speed-measuring program, is an effort to avoid parking tickets. ""With this set of tools, one can park their car, knowing that a notification will arrive via text message notifying them of a passing metermaid,"" he explains. The idea is that this marks the official start of their two-hour parking time limit.
When an image of a moving car in the field of view is captured, it gets analyzed by TensorFlow with trained data (to recognize the meter maid vehicles). If the image is a meter maid match, a message gets sent via Twilio with a link to the image. Naulty includes the disclaimer, ""This is a free, open source project and the developers are in no way accountable for parking tickets because of rebellious, citation-breaking citizens.""
TensorFlow on the farm
TensorFlow isn't only for city slickers—it comes in handy on the farm, too. Makoto Koike, a former embedded systems designer from the Japanese automobile industry, started helping out at his parents' cucumber farm. An article on the Google Cloud Platform blog, How a Japanese cucumber farmer is using deep learning and TensorFlow, explains that Koike's mother spent up to eight hours a day sorting cucumbers by characteristics such as size, thickness, color, texture, small scratches, and whether they have prickles, which command higher prices on the market.
""The system uses Raspberry Pi 3 as the main controller to take images of the cucumbers with a camera, and in a first phase, runs a small-scale neural network on TensorFlow to detect whether or not the image is a cucumber,"" the article explains. ""It then forwards the image to a larger TensorFlow neural network running on a Linux server to perform a more detailed classification.""
"
machinelearning$AI,"Taking machine learning to the birds
Cacophony Project uses the latest technology to monitor and protect endangered bird populations against predators.
The Cacophony Project's broad vision is to bring back New Zealand's native birds using the latest technology to monitor bird populations and humanely eliminate the introduced predators that are endangering them.
The project started in our founder's backyard to measure the effectiveness of his efforts to protect the birds on his property. From this simple beginning, the project has quickly grown into a system that includes two edge devices, a cloud server, and automatic identification of animals using machine learning. The project has been completely open source from the beginning and sees regular contributions from a wide variety of volunteers.
What makes New Zealand's birds so special?
In New Zealand, our birds are our taonga, our precious things. New Zealand was isolated from the rest of the world for 70-million years, and our bird species evolved free from the presence of mammals (aside from small bats). As a result, many of New Zealand's unique bird species, including our national icon, the kiwi, do not have defense strategies against predators.
Unfortunately, the introduction of invasive species such as rats, stoats, and possums after European settlement over the last couple of centuries has decimated the population of many bird species. Now, 80% of native bird species are already endangered or in decline.
Over the years, conservationists have rescued many of our iconic species from the brink of extinction by relocating remaining populations to predator-free islands. These islands have been painstakingly cleared of all introduced mammals. On these islands, you can step back in time and hear the loud cacophony of birdsong that greeted the first European explorers.
These islands have been so wildly successful that they are now full to the brim, and conservationists have started to create predator-free sanctuaries on the mainland. The government has recently set the ambitious target to make New Zealand free of rats, stoats, and possums by 2050.

Not just conservationists are fighting for our birds. In recent years, many everyday New Zealanders have formed local groups to work together to trap predators in the hope of hearing our native birds' melodic birdsong in their backyards.
We believe we can make these groups far more effective by modernizing the tools they are using. By using digital lures combined with more effective elimination methods, we think these programs could be up to 80,000 times more effective.
Our technological ecosystem
Over the past few years, using an open source, rapid prototyping approach, we have built up an ecosystem of components for automated monitoring of bird populations and predators introduced to New Zealand. The system is summarized in the diagram below.
Our thermal video platform allows us to monitor and observe pest species' behavior. It is specifically designed to detect small and fast-moving predators such as stoats and rats. It features a ruggedized Raspberry Pi computer paired with a FLIR Lepton 3 thermal camera. Power is supplied by either batteries or AC power, and additional custom hardware provides power and watchdog capabilities, audio output to lure predators, and remote communication.
Our highly tuned motion detection algorithm watches the live thermal camera feed and detects warm, moving objects (hopefully an animal). When movement is detected, the camera begins recording and uploads it to the Cacophony Project API server for storage and analysis. By using the thermal camera feed, we can detect small mammals with much higher sensitivity than trail cameras (typically designed to detect larger mammals such as pigs and deer) could.
High sensitivity is important because we can't manage what we can't detect. Mammals such as stoats are otherwise difficult to detect because they are small and fast and rarely interact with other detection devices such as tracking tunnels and chew cards.
We also have created an Android audio recorder that runs on inexpensive mobile phones. When installed in the field, it makes regular, objective, GPS-located, time-stamped audio recordings that are uploaded to our API server. These recordings are ideal for monitoring bird populations over time.
Eventually, we plan to use machine learning on these recordings to determine bird population trends for a given location and to automatically identify bird species where possible. While we continue to work on the AI, we are already recording at several locations around the country to build up year-on-year datasets.
The captured audio and thermal video recordings are uploaded to our API server for storage and processing. Our machine learning pipeline processes the thermal video recordings and tags the animals that appear. All of the recordings are also converted to easily viewed formats.
The machine learning pipeline enables automatic identification of predators and is one aspect that makes our approach especially interesting. Uploaded thermal video recordings are first analyzed to look for warm, moving objects in each frame. These areas are clipped from each frame and linked together through time to form what we call «tracks.""
These tracks are then classified using a convolutional recurrent neural network (CRNN) model implemented using the popular TensorFlow platform. The model is trained using human-tagged tracks broken into three-second chunks. The input tracks are stretched, flipped, and clipped to increase the variety of input data and make the model more robust in practical use.
When classifying, the model is fed each frame individually from a track to produce the classification probabilities for each animal class at every point in time. The recurrent structure of the model means that its current output is influenced by previous outputs. This allows the model to account for the way an animal moves over time, greatly increasing classification accuracy. This intuitively makes sense: Human observers often find it difficult to identify an animal from a single video frame, but seeing the animal move for a few seconds usually gives the species away.
We created a web platform so users can view and listen to recordings made on their devices. Through this platform, users can also manually tag animals in videos and set up playback of audio to lure animals closer to the camera.
The final pieces of our system are device management tools. Our management server allows us to monitor the uptime of online devices and push out new versions of software and configuration to our devices.
Finally, we have Sidekick, an Android app that helps users set up the thermal cameras. For devices without mobile data coverage, Sidekick can wirelessly collect thermal recordings and upload them later when internet connectivity is available.
Our next moves
With a small team and minimal management, we move quickly to iterate, discover, and improve. There are so many good ideas floating around that one of the hardest things is for us to decide what to work on next. We always have to balance trying out new ideas with making our technology more reliable and easier for people to use.
Currently, we are working on adapting our artificial intelligence (AI) model to run on the thermal camera hardware to allow autonomous classification of predators. Once this is done, we plan to pair the camera with traps that trigger only when the camera identifies a target species. This allows for more effective trapping strategies that increase predator catch rates while eliminating by-catch of non-target animals.
Other upcoming projects include visualizations of our ever-increasing set of recordings, applying machine learning to evaluate birdsongs in audio recordings, and new device management features. We also have many other ideas to try out, both big and small—the team is never bored!
If you are keen to learn more, visit the Cacophony Project website. If you'd like to contribute, please use the contact form or email addresses listed there. With the project moving so fast, contacting us directly is usually the easiest way to identify the most interesting and useful ways for you to help.
"
machinelearning$AI,"How to get started in AI
Before you can begin working in artificial intelligence, you need to acquire some human intelligence.
I've both asked and been asked about the best way to learn more about artificial intelligence (AI). What should I read? What should I watch? I'll get to that. But, first, it's useful to break down this question, given that AI covers a lot of territory.
One important distinction to draw is between the research side of AI and the applied side. Cassie Kozyrkov of Google drew this distinction in a talk at the recent O'Reilly Artificial Intelligence Conference in London, and it's a good one.
Research AI is rather academic in nature and requires a heavy dose of math across a variety of disciplines before you even get to those parts that are specific to AI. This aspect of AI focuses on the algorithms and tools that drive the state of AI forward. For example, what neural network structures might improve vision recognition results? How might we make unsupervised learning a more generally useful approach? Can we find ways to understand better how deep learning pipelines come up with the answers they do?
Applied AI, on the other hand, is more about using existing tools to obtain useful results. Open source has played a big role here in providing free and often easy-to-use software in a variety of languages. Public cloud providers have also devoted a lot of attention to providing machine learning services, models, and datasets that make the onramp to getting started with AI much simpler than it would be otherwise.
I'll add at this point that applied AI practitioners shouldn't treat their tools as some sort of black box that spits out answers for mysterious reasons. At a minimum, they need to understand the limits and potential biases of different techniques, models, and data collection approaches. It's just that they don't necessarily need to delve deeply into all the theory underpinning every part of their toolchain.
Although it's probably less important for working in AI on a day-to-day basis, it's also useful to understand the broader context of AI. It goes beyond the narrow scope of deep learning on neural networks that have been so important to the gains made in reinforcement learning and supervised learning to date. For example, AI is often viewed as a way to augment (rather than replace) human judgment and decisions. But the handoff between machine and human has its own pitfalls.
With that background, here are some study areas and resources you may find useful.
Research AI
In a lot of respects, a list of resources for research AI mirror those in an undergraduate (or even graduate) computer science program that's focused on AI. The main difference is that the syllabus you draw up may be more interdisciplinary than more traditionally focused university curricula.
Where you start will depend on your computer science and math background.
If it's minimal or rusty, but you still want to develop a deep understanding of AI fundamentals, you'll benefit from taking some math courses to start. There are many options on massive online open courses (MOOCs) like the nonprofit edX platform and Coursera. (Both platforms charge for certifications, but edX makes all the content available for free to people just auditing the course.)
Typical foundational courses could include:
MIT's Calculus courses, starting with differentiation
Linear Algebra (University of Texas)
Probability and statistics, such as MIT's Probability—The Science of Uncertainty and Data
To get deeper into AI from a research perspective, you'll probably want to get into all these areas of mathematics and more. But the above should give you an idea of the general branches of study that are probably most important before delving into machine learning and AI proper.
In addition to MOOCs, resources such as MIT OpenCourseWare provide the syllabus and various supporting materials for a wide range of mathematics and computer science courses.
With the foundations in place, you can move onto more specialized courses in AI proper. Andrew Ng's AI MOOC, from when he was teaching at Stanford, was one of the early courses to popularize the whole online course space. Today, his Neural Networks and Deep Learning is part of the Deep Learning specialization at Coursera. There are corresponding programs on edX. For example, Columbia offers an Artificial Intelligence MicroMasters.
In addition to courses, a variety of textbooks and other learning material are also available online. These include:
Neural Networks and Deep Learning
Deep Learning from MIT Press by Ian Goodfellow and Yoshua Bengio and Aaron Courville
Applied AI
Applied AI is much more focused on using available tools than building new ones. Some appreciation of the mathematical underpinnings, especially statistics, is still useful—arguably even necessary—but you won't be majoring in that aspect of AI to the same degree you would in a research mode.
Programming is a core skill here. While different programming languages can come into play, a lot of libraries and toolsets—such as PyTorch—rely on Python, so that's a good skill to have. Especially if you have some level of programming background, MIT's Introduction to Computer Science and Programming Using Python, based on its on-campus 6.001 course, is a good primer. If you're truly new to programming, Charles Severance's Programming for Everybody (Getting Started with Python) from the University of Michigan doesn't toss you into the deep end of the pool the way the MIT course does.
The R programming language is also a useful skill to add to your toolbox. While it's less used in machine learning (ML) per se, it's common for a variety of other data science tasks, and applied AI/ML and data science often blend in practice. For example, many tasks associated with organizing and cleaning data apply equally whatever analysis techniques you'll eventually use. A MOOC sequence like Harvard's Data Science certificate is an example of a set of courses that provide a good introduction to working with data.
Another open source software library you're likely to encounter if you do any work with AI is TensorFlow. It was originally developed by researchers and engineers from the Google Brain team within Google's AI organization. Google offers a variety of tutorials to get started with TensorFlow using the high-level Keras API. You can run TensorFlow locally as well as online in Google Cloud.
In general, all of the big public cloud providers offer online datasets and ML services that can be an easy way to get started. However, especially as you move beyond ""play"" datasets and applications, you need to start thinking seriously about the degree to which you want to be locked into a single provider.
Datasets for your exploratory learning projects are available from many different sources. In addition to the public cloud providers, Kaggle is another popular source and also a good learning resource more broadly. Government data is also increasingly available in digital form. The US Federal Government's Data.gov claims over 300,000 datasets. State and local governments also publish data on everything from restaurant health ratings to dogs' names.
Miscellany
I'll close by noting that AI is a broad topic that isn't just about math, programming, and data. AI as a whole touches many other fields, including cognitive psychology, linguistics, game theory, operations research, and control systems. Indeed, a concern among at least some AI researchers today is that the field has become too fixated on a small number of techniques that have become powerful and interesting only quite recently because of the intersection of processing power and big data. Many longstanding problems in understanding how humans learn and reason remain largely unsolved. Developing at least some appreciation for these broader problem spaces will better enable you to place AI within a broader context.
One of my favorite examples is the Humans and Autonomy Lab at Duke. The work in this lab touches on all the challenges of humans working with machines, such as how autopilots can create ""Children of the Magenta"" who are unable to take control quickly if the automation fails. A basic brain-science course, such as MIT's Introduction to Psychology, provides some useful context for the relationship between human intelligence and machine intelligence. Another course in a similar vein, but taught by the late Marvin Minsky from MIT's Electrical Engineering and Computer Science department, is The Society of Mind.
If there's one key challenge to learning about AI, it's not that raw materials and tools aren't readily available. It's that there are so many of them. My objective hasn't been to give you a comprehensive set of pointers. Rather, it's been to both point out the different paths you can take and provide you with some possible starting points. Happy learning!

"
communitymanagement,"5 resolutions for open source project maintainers
No matter how you say it, good communication is essential to strong open source communities.
I'm generally not big on New Year's resolutions. I have no problem with self-improvement, of course, but I tend to anchor around other parts of the calendar. Even so, there's something about taking down this year's free calendar and replacing it with next year's that inspires some introspection.
In 2017, I resolved to not share articles on social media until I'd read them. I've kept to that pretty well, and I'd like to think it has made me a better citizen of the internet. For 2019, I'm thinking about resolutions to make me a better open source software maintainer.
Here are some resolutions I'll try to stick to on the projects where I'm a maintainer or co-maintainer.
1. Include a code of conduct
Jono Bacon included ""not enforcing the code of conduct"" in his article ""7 mistakes you're probably making."" Of course, to enforce a code of conduct, you must first have a code of conduct. I plan on defaulting to the Contributor Covenant, but you can use whatever you like. As with licenses, it's probably best to use one that's already written instead of writing your own. But the important thing is to find something that defines how you want your community to behave, whatever that looks like. Once it's written down and enforced, people can decide for themselves if it looks like the kind of community they want to be a part of.
2. Make the license clear and specific
You know what really stinks? Unclear licenses. ""This software is licensed under the GPL"" with no further text doesn't tell me much. Which version of the GPL? Do I get to pick? For non-code portions of a project, ""licensed under a Creative Commons license"" is even worse. I love the Creative Commons licenses, but there are several different licenses with significantly different rights and obligations. So, I will make it very clear which variant and version of a license applies to my projects. I will include the full text of the license in the repo and a concise note in the other files.
Sort of related to this is using an OSI-approved license. It's tempting to come up with a new license that says exactly what you want it to say, but good luck if you ever need to enforce it. Will it hold up? Will the people using your project understand it?
3. Triage bug reports and questions quickly
Few things in technology scale as poorly as open source maintainers. Even on small projects, it can be hard to find the time to answer every question and fix every bug. But that doesn't mean I can't at least acknowledge the person. It doesn't have to be a multi-paragraph reply. Even just labeling the GitHub issue shows that I saw it. Maybe I'll get to it right away. Maybe I'll get to it a year later. But it's important for the community to see that, yes, there is still someone here.

4. Don't push features or bug fixes without accompanying documentation
For as much as my open source contributions over the years have revolved around documentation, my projects don't reflect the importance I put on it. There aren't many commits I can push that don't require some form of documentation. New features should obviously be documented at (or before!) the time they're committed. But even bug fixes should get an entry in the release notes. If nothing else, a push is a good opportunity to also make a commit to improving the docs.
5. Make it clear when I'm abandoning a project
I'm really bad at saying ""no"" to things. I told the editors I'd write one or two articles for Opensource.com and here I am almost 60 articles later. Oops. But at some point, the things that once held my interests no longer do. Maybe the project is unnecessary because its functionality got absorbed into a larger project. Maybe I'm just tired of it. But it's unfair to the community (and potentially dangerous, as the recent event-stream malware injection showed) to leave a project in limbo. Maintainers have the right to walk away whenever and for whatever reason, but it should be clear that they have. 
"
OpenStack$cloud,"What's new in OpenStack?
The world of open infrastructure is constantly changing. Learn how OpenStack is pivoting to meet the needs of its user community.
The OpenStack global community is gathering together in Vancouver, British Columbia this week to collaborate, learn, and build the future of open source cloud computing.
As OpenStack Foundation Chief Operating Officer Mark Collier referenced in his opening keynote, the uses which OpenStack is seeing today expand far beyond what most who were involved in the early days of the project could have ever imagined. While OpenStack started out primarily in the traditional data center and found many large-scale users, particularly in the telecommunications industry, who were using it to manage huge installations of traditional x86 server hardware, the flexibility of OpenStack has today allowed it to thrive in many other environments and use cases.
Today, we see OpenStack powering everything from academic and research projects to media and gaming services, from online retail and e-commerce to manufacturing and industrial applications, and from finance to healthcare. OpenStack is found in all of these different places not just because it is cheaper than using the public cloud, not just because it makes compliance with various regulations easier, but because its open source code makes it flexible to all sort of different situations.
For example, it can run applications like machine learning and artificial intelligence that make use of graphics processing units (GPUs) instead of traditional CPUs to power their performance, and divide those GPUs across multiple applications running inside of virtual machines. It can allow for deployment to the diverse locations around the globe where scientific research takes place, which generates orders of magnitude more data than could ever be captured and uploaded to the public cloud. But perhaps most importantly, it allows organizations to mix and match the software they use to meet their actual needs, rather than relying on a single vendor’s proprietary solutions.
One change we’ve seen in recent years within the OpenStack community is a shift in focus away from the infrastructure itself to empowering the applications which it supports. To that end, two topics getting much attention at this summit are containers, which enable faster and denser deployments of applications, and continuous integration and development, which allow faster development cycles between coding and production.
Containers
While containers aren’t new to the OpenStack ecosystem, it seems as if every summit introduces new ways in which container technologies are interacting with OpenStack; either on top, beneath, or alongside OpenStack’s services.
The big news from yesterday was the announcement of Kata Containers hitting its 1.0 release. As a refresher, Kata Containers is a new project within the OpenStack Foundation aiming to build lightweight virtual machines around containers to allow applications to function with the speed and composability of containers while maintaining the security and isolation of virtual machines.
While still a relatively new player, they are already designed to plug in seamlessly with existing projects, including Kubernetes, by adhering to the Open Container Initiative specification.
CI/CD
A relatively new feature of the OpenStack community is the addition of the developer-focused OpenDev event, which this year is focused on continuous integration and development within the context of OpenStack.
While there are a number of CI/CD systems which might fit into the workflow of an OpenStack user, the big focus of this event was Zuul, whose tagline “stop merging broken code” is probably something many organizations can identify with.
Zuul isn’t new, but its place in the OpenStack umbrella is new. Originally developed as the CI/CD system for OpenStack, it now sits as a stand-alone project which can be used as a CI/CD system for any large project which spans multiple repositories or has many external dependencies which may be seeing development concurrently.
In OpenStack, projects depend on one another to function properly. In order to properly test code, you need to know not just that it doesn’t break the project you’re committing code to, but also that it won’t break any of the other projects that depend on the project you’re working on. By better coordinating across projects, Zuul’s goal is to go beyond simple code testing and instead focus on enabling cross-project collaboration and coordination.
Changing times
You may have noticed that several of the topics coming out of this OpenStack Summit don't fit neatly under the umbrella of what had been previously called OpenStack. They go far beyond just provisioning compute, network, and storage resources.
That's by design.
OpenStack Summit, and the OpenStack Foundation supporting it, have broadened their focus to include sessions on many of the other projects, technologies, and other things that modern IT operations professionals need to be familiar with. In many ways, this represents a shift from a focus on software to a focus on people and what those people need to be able to do their jobs.
To this end, in addition to 250+ talks on OpenStack, the Vancouver agenda included 70+ Kubernetes sessions, as well as talks on other technologies like Docker, OPNFV, ONAP, Ceph, Ansible, Tensorflow, and many others.
It’s an exciting time to be working in open infrastructure!"
OpenStack$cloud$CI/CD,"Mastering CI/CD at OpenDev
The future of development requires a strong understanding of CI/CD pipelines.
After launching in 2017, the OpenDev Conference is now an annual event. At the inaugural event last September, the conference focus was on edge computing. This year's event, taking place May 22-23, will be focused on Continuous Integration/Continuous Deployment (CI/CD) and will be co-located with the OpenStack Summit in Vancouver.
I was invited to participate in the program committee for OpenDev CI/CD based on my background on the CI/CD system for the OpenStack project and my recent move into the container space. Today I frequently talk about CI/CD pipelines using various open source technologies, including Jenkins, GitLab, Spinnaker, and Artifactory.

This event is exciting for me because we're bringing two open source infrastructure ideas together into one event. First, we'll be discussing CI/CD tooling that can be used by any organization. To this end, in the keynotes we'll hear practical talks about open source CI/CD tooling, including a talk about Spinnaker from Boris Renski and one from Jim Blair on Zuul. The keynotes also will include higher-level talks about the preference for open technologies, especially across communities and inside open source projects themselves. From Fatih Degirmenci and Daniel Farrell we'll hear about sharing continuous delivery practices across communities, and Benjamin Mako Hill will join us to talk about why free software needs free tools.
Given the relative newness of CI/CD, the rest of the event is a mix of talks, workshops, and collaborative discussions. When selecting from talks and workshops submitted, and coming up with collaborative discussion topics, we wanted to make sure there was a diverse schedule so anyone on the open CI/CD spectrum would find something interesting.
The talks will be standard conference style, selected to cover key topics like crafting CI/CD pipelines, improving security when practicing DevOps, and more specific solutions like container-based Aptomi on Kubernetes and doing CI/CD in ETSI NFV environments. Many of these sessions will serve as an introduction to these topics, ideal for those who are new to the CI/CD space or any of these specific technologies.
The hands-on workshops are longer and will have specific outcomes in mind for attendees. These include ""Anomaly Detection in Continuous Integration Jobs,"" ""How to Install Zuul and Configure Your First Jobs,"" and ""Spinnaker 101: Releasing Software with Velocity and Confidence."" (Note that space is limited in these workshops, so an RSVP system has been set up. You'll find an RSVP button on the session links provided here.)
Perhaps what I'm most excited about are the collaborative discussions, and these take up over half of the conference schedule. The topics were chosen by the program committee based on what we've been seeing in our communities. These are ""fishbowl""-style sessions, where several people get in a room together to discuss a specific topic around CI/CD.
The idea for this style of session was taken from developer summits that the Ubuntu community pioneered and the OpenStack community continued. Topics for these collaborative discussions include separate sessions for CI and CD fundamentals, improvements that can be made to encourage cross-community collaboration, driving CI/CD culture in organizations, and why open source CI/CD tooling is so important. Shared documents are used to take notes during these sessions to make sure that as much knowledge shared during the session is retained as possible. It's also common for action items to come from these discussions, so community members can push forward initiatives related to the topic being covered.
The event concludes with a Joint Conclusion Session, which will be summarizing the key points from the collaborative discussions and identifying work areas that attendees wish to work on in future.
Registration for this event is included in OpenStack Summit registration, or tickets for this event only can be purchased for $199 onsite at the Vancouver Convention Center. Learn more about tickets and the full agenda on the OpenDev website.
"
CI/CD$SysAdmin$DevOps,"7 CI/CD tools for sysadmins
An easy guide to the top open source continuous integration, continuous delivery, and continuous deployment tools.
Continuous integration, continuous delivery, and continuous deployment (CI/CD) have all existed in the developer community for many years. Some organizations have involved their operations counterparts, but many haven't. For most organizations, it's imperative for their operations teams to become just as familiar with CI/CD tools and practices as their development compatriots are.
CI/CD practices can equally apply to infrastructure and third-party applications and internally developed applications. Also, there are many different tools but all use similar models. And possibly most importantly, leading your company into this new practice will put you in a strong position within your company, and you'll be a beacon for others to follow.
Some organizations have been using CI/CD practices on infrastructure, with tools like Ansible, Chef, or Puppet, for several years. Other tools, like Test Kitchen, allow tests to be performed on infrastructure that will eventually host applications. In fact, those tests can even deploy the application into a production-like environment and execute application-level tests with production loads in more advanced configurations. However, just getting to the point of being able to test the infrastructure individually is a huge feat. Terraform can also use Test Kitchen for even more ephemeral and idempotent infrastructure configurations than some of the original configuration-management tools. Add in Linux containers and Kubernetes, and you can now test full infrastructure and application deployments with prod-like specs and resources that come and go in hours rather than months or years. Everything is wiped out before being deployed and tested again.
However, you can also focus on getting your network configurations or database data definition language (DDL) files into version control and start running small CI/CD pipelines on them. Maybe it just checks syntax or semantics or some best practices. Actually, this is how most development pipelines started. Once you get the scaffolding down, it will be easier to build on. You'll start to find all kinds of use cases for pipelines once you get started.
For example, I regularly write a newsletter within my company, and I maintain it in version control using MJML. I needed to be able to host a web version, and some folks liked being able to get a PDF, so I built a pipeline. Now when I create a new newsletter, I submit it for a merge request in GitLab. This automatically creates an index.html with links to HTML and PDF versions of the newsletter. The HTML and PDF files are also created in the pipeline. None of this is published until someone comes and reviews these artifacts. Then, GitLab Pages publishes the website and I can pull down the HTML to send as a newsletter. In the future, I'll automatically send the newsletter when the merge request is merged or after a special approval step. This seems simple, but it has saved me a lot of time. This is really at the core of what these tools can do for you. They will save you time.
The key is creating tools to work in the abstract so that they can apply to multiple problems with little change. I should also note that what I created required almost no code except some light HTML templating, some node to loop through the HTML files, and some more node to populate the index page with all the HTML pages and PDFs.
Some of this might look a little complex, but most of it was taken from the tutorials of the different tools I'm using. And many developers are happy to work with you on these types of things, as they might also find them useful when they're done. The links I've provided are to a newsletter we plan to start for DevOps KC, and all the code for creating the site comes from the work I did on our internal newsletter.
Many of the tools listed below can offer this type of interaction, but some offer a slightly different model. The emerging model in this space is that of a declarative description of a pipeline in something like YAML with each stage being ephemeral and idempotent. Many of these systems also ensure correct sequencing by creating a directed acyclic graph (DAG) over the different stages of the pipeline.
These stages are often run in Linux containers and can do anything you can do in a container. Some tools, like Spinnaker, focus only on the deployment component and offer some operational features that others don't normally include. Jenkins has generally kept pipelines in an XML format and most interactions occur within the GUI, but more recent implementations have used a domain specific language (DSL) using Groovy. Further, Jenkins jobs normally execute on nodes with a special Java agent installed and consist of a mix of plugins and pre-installed components.
Jenkins introduced pipelines in its tool, but they were a bit challenging to use and contained several caveats. Recently, the creator of Jenkins decided to move the community toward a couple different initiatives that will hopefully breathe new life into the project—which is the one that really brought CI/CD to the masses. I think its most interesting initiative is creating a Cloud Native Jenkins that can turn a Kubernetes cluster into a Jenkins CI/CD platform.
As you learn more about these tools and start bringing these practices into your company or your operations division, you'll quickly gain followers. You will increase your own productivity as well as that of others. We all have years of backlog to get to—how much would your co-workers love if you could give them enough time to start tackling that backlog? Not only that, but your customers will start to see increased application reliability, and your management will see you as a force multiplier. That certainly can't hurt during your next salary negotiation or when interviewing with all your new skills.
Let's dig into the tools a bit more. We'll briefly cover each one and share links to more information.
GitLab CI
GitLab is a fairly new entrant to the CI/CD space, but it's already achieved the top spot in the Forrester Wave for Continuous Integration Tools. That's a huge achievement in such a crowded and highly qualified field. What makes GitLab CI so great? It uses a YAML file to describe the entire pipeline. It also has a functionality called Auto DevOps that allows for simpler projects to have a pipeline built automatically with multiple tests built-in. This system uses Herokuish buildpacks to determine the language and how to build the application. Some languages can also manage databases, which is a real game-changer for building new applications and getting them deployed to production from the beginning of the development process. The system has native integrations into Kubernetes and will deploy your application automatically into a Kubernetes cluster using one of several different deployment methodologies, like percentage-based rollouts and blue-green deployments.
In addition to its CI functionality, GitLab offers many complementary features like operations and monitoring with Prometheus deployed automatically with your application; portfolio and project management using GitLab Issues, Epics, and Milestones; security checks built into the pipeline with the results provided as an aggregate across multiple projects; and the ability to edit code right in GitLab using the WebIDE, which can even provide a preview or execute part of a pipeline for faster feedback.
GoCD
GoCD comes from the great minds at Thoughtworks, which is testimony enough for its capabilities and efficiency. To me, GoCD's main differentiator from the rest of the pack is its Value Stream Map (VSM) feature. In fact, pipelines can be chained together with one pipeline providing the ""material"" for the next pipeline. This allows for increased independence for different teams with different responsibilities in the deployment process. This may be a useful feature when introducing this type of system in older organizations that intend to keep these teams separate—but having everyone using the same tool will make it easier later to find bottlenecks in the VSM and reorganize the teams or work to increase efficiencies.
It's incredibly valuable to have a VSM for each product in a company; that GoCD allows this to be described in JSON or YAML in version control and presented visually with all the data around wait times makes this tool even more valuable to an organization trying to understand itself better. Start by installing GoCD and mapping out your process with only manual approval gates. Then have each team use the manual approvals so you can start collecting data on where bottlenecks might exist.
Travis CI
Travis CI was my first experience with a Software as a Service (SaaS) CI system, and it's pretty awesome. The pipelines are stored as YAML with your source code, and it integrates seamlessly with tools like GitHub. I don't remember the last time a pipeline failed because of Travis CI or the integration—Travis CI has a very high uptime. Not only can it be used as SaaS, but it also has a version that can be hosted. I haven't run that version—there were a lot of components, and it looked a bit daunting to install all of it. I'm guessing it would be much easier to deploy it all to Kubernetes with Helm charts provided by Travis CI. Those charts don't deploy everything yet, but I'm sure it will grow even more in the future. There is also an enterprise version if you don't want to deal with the hassle.
However, if you're developing open source code, you can use the SaaS version of Travis CI for free. That is an awesome service provided by an awesome team! This alleviates a lot of overhead and allows you to use a fairly common platform for developing open source code without having to run anything.
Jenkins
Jenkins is the original, the venerable, de facto standard in CI/CD. If you haven't already, you need to read ""Jenkins: Shifting Gears"" from Kohsuke, the creator of Jenkins and CTO of CloudBees. It sums up all of my feelings about Jenkins and the community from the last decade. What he describes is something that has been needed for several years, and I'm happy CloudBees is taking the lead on this transformation. Jenkins will be a bit overwhelming to most non-developers and has long been a burden on its administrators. However, these are items they're aiming to fix.
Jenkins Configuration as Code (JCasC) should help fix the complex configuration issues that have plagued admins for years. This will allow for a zero-touch configuration of Jenkins masters through a YAML file, similar to other CI/CD systems. Jenkins Evergreen aims to make this process even easier by providing predefined Jenkins configurations based on different use cases. These distributions should be easier to maintain and upgrade than the normal Jenkins distribution.
Jenkins 2 introduced native pipeline functionality with two types of pipelines, which I discuss in a LISA17 presentation. Neither is as easy to navigate as YAML when you're doing something simple, but they're quite nice for doing more complex tasks.
Jenkins X is the full transformation of Jenkins and will likely be the implementation of Cloud Native Jenkins (or at least the thing most users see when using Cloud Native Jenkins). It will take JCasC and Evergreen and use them at their best natively on Kubernetes. These are exciting times for Jenkins, and I look forward to its innovation and continued leadership in this space.
Concourse CI
I was first introduced to Concourse through folks at Pivotal Labs when it was an early beta version—there weren't many tools like it at the time. The system is made of microservices, and each job runs within a container. One of its most useful features that other tools don't have is the ability to run a job from your local system with your local changes. This means you can develop locally (assuming you have a connection to the Concourse server) and run your builds just as they'll run in the real build pipeline. Also, you can rerun failed builds from your local system and inject specific changes to test your fixes.
Concourse also has a simple extension system that relies on the fundamental concept of resources. Basically, each new feature you want to provide to your pipeline can be implemented in a Docker image and included as a new resource type in your configuration. This keeps all functionality encapsulated in a single, immutable artifact that can be upgraded and modified independently, and breaking changes don't necessarily have to break all your builds at the same time.
Spinnaker
Spinnaker comes from Netflix and is more focused on continuous deployment than continuous integration. It can integrate with other tools, including Travis and Jenkins, to kick off test and deployment pipelines. It also has integrations with monitoring tools like Prometheus and Datadog to make decisions about deployments based on metrics provided by these systems. For example, the canary deployment uses a judge concept and the metrics being collected to determine if the latest canary deployment has caused any degradation in pertinent metrics and should be rolled back or if deployment can continue.
A couple of additional, unique features related to deployments cover an area that is often overlooked when discussing continuous deployment, and might even seem antithetical, but is critical to success: Spinnaker helps make continuous deployment a little less continuous. It will prevent a stage from running during certain times to prevent a deployment from occurring during a critical time in the application lifecycle. It can also enforce manual approvals to ensure the release occurs when the business will benefit the most from the change. In fact, the whole point of continuous integration and continuous deployment is to be ready to deploy changes as quickly as the business needs to change.
Screwdriver
Screwdriver is an impressively simple piece of engineering. It uses a microservices approach and relies on tools like Nomad, Kubernetes, and Docker to act as its execution engine. There is a pretty good deployment tutorial for deploying to AWS and Kubernetes, but it could be improved once the in-progress Helm chart is completed.
Screwdriver also uses YAML for its pipeline descriptions and includes a lot of sensible defaults, so there's less boilerplate configuration for each pipeline. The configuration describes an advanced workflow that can have complex dependencies among jobs. For example, a job can be guaranteed to run after or before another job. Jobs can run in parallel and be joined afterward. You can also use logical operators to run a job, for example, if any of its dependencies are successful or only if all are successful. Even better is that you can specify certain jobs to be triggered from a pull request. Also, dependent jobs won't run when this occurs, which allows easy segregation of your pipeline for when an artifact should go to production and when it still needs to be reviewed."
security&privacy,"What is a certificate?
What is a certificate, why do they expire, and what could happen when they do?
Certificates, it turns out, are important.  Expired certificates can cause you huge problems - that's what the stories in the news are telling us. But what is a certificate, why do they expire, and why would that have such a big impact? To answer these questions, let's step back a bit and look at why you need certificates in the first place.
Let's assume that two people, Alice and Bob, want to exchange secret information. Let's go further, and say that Bob is really Bobcorp, Alice's bank, and she wants to be able to send and receive her bank statements in encrypted form. There are well-established ways to do this, and the easiest way is for them to agree on a shared key they will use to both encrypt and decrypt each other's messages. How do they agree on this key? Luckily, there are some clever ways in which they can manage a ""handshake"" between the two of them, even if they've not communicated before, which ends in both having a copy of the key without the chance of anybody else getting hold of it.
The problem is that Alice can't be sure that she's actually talking to Bobcorp (or vice versa). Bobcorp probably doesn't mind at this point, because he can ask Alice to provide her login credentials, which will allow him to authenticate her. But Alice really does care: she certainly shouldn't be handing her login details to somebody—let's call her ""Eve""—who's just pretending to be Bob.
The solution to this problem comes in two parts: certificates and certificate authorities (CAs). A CA is a well-known and trusted party with whom Bobcorp has already established a relationship, typically by providing company details, website details, and the like. Bobcorp also creates and sends the CA a special key and specific information about itself (maybe including the business name, address, and website information). The CA, having established Bobcorp's bona fides, creates a certificate for Bobcorp, incorporating the information that was requested; in fact, some of the information Bobcorp sends the CA is usually in the form of a ""self-signed certificate,"" so pretty much all the CA needs to do is provide its own signature.
Astute readers will be asking themselves: ""How did this help? Alice still needs to trust the CA, right?"" The answer is that she does. But there will typically be a small number of CAs in comparison to Bobcorp-type companies, so all Alice needs to do is ensure that she can trust a few CAs, and she's now good to go. In a web-browsing scenario, Alice will usually have downloaded a browser that already has appropriate, built-in trust relationships with the main CAs. She can now perform safe handshakes with lots of companies, and as long as she (or her browser) checks that they provide certificates signed by a CA that she trusts, she's relatively safe.
But there's a twist. The certificates that the CA issues to Bobcorp (and others) typically have an expiration date on them. This isn't just to provide the CA with a recurring revenue stream—though I'm sure that's a nice benefit—but it's also in case Bobcorp's situation changes: what if it has gone bankrupt, for instance, or changed its country of business?1 So after a period of time (typically a year or two, but maybe less or more), Bobcorp must reapply to get a new certificate.
What if Bobcorp forgets? Well, when Alice visits Bobcorp's site and the browser notices an expired certificate, it should want her not to proceed, and she shouldn't give them any information until it's renewed. This sounds like a pain, and it is: Bobcorp and its customers are going to be severely inconvenienced. Somebody within Bobcorp whose job it was to renew the certificate is going to be in trouble.
Life is even worse in the case where no actual people are involved. If, instead of Alice, we have an automated system A, and instead of Bob, we have an automated system, B. A still needs to trust that it's talking to the real B—in case an evil system, E, is pretending to be B—so certificates are still required. In this case, if B's certificate expires, A should quite rightly refuse to connect to it.  There is no easy way to fix such a problem or to tell the many, many A-type systems that might have been trying to communicate with the B system(s) to carry on regardless. And so, for want of a nail, the kingdom was lost.
"
security&privacy$tools,"How to manage your passwords with Bitwarden, a LastPass alternative
Learn how to set up and use open source password manager Bitwarden.
Do you ever feel you have more passwords than you can keep track of? It's probably more than just a feeling. Like most of us, you probably have a hard time remembering all those passwords, no matter how simple or complex they are.
Many people turn to popular services like LastPass and 1Password to help them wrangle their passwords. While solid, those services are also proprietary and closed source. So where can an open source enthusiast turn to find an alternative?
Enter Bitwarden, an application that's aiming to become the go-to open source password manager on the web. Let's take a quick look at how to use it.
Note: I'm not going to cover all Bitwarden's features in this article, just its core password management ones. You've been warned.
Getting started
Sign up for an account. It's free (although there are also paid plans). Your account gives you access to a secure space (called a vault) to store your passwords.
When you're signing up, you'll be asked to create a master password. That's the one that will keep your other passwords safe. It's in your best interest to make your master password as strong and complex as you can—and as you can remember.
If you want a little more control and to embrace your inner geek, you can grab the source code on GitHub and install Bitwarden on your server. There's even a Docker image.
Me? I went with the hosted edition. I know ...
Once you've set up your account, grab the Bitwarden extension for one of the supported browsers (you probably use at least one of them): Chrome, Firefox, Opera, Edge, Safari, Vivaldi, Brave (you can install the extension from within the browser), or Tor Browser.
Now you're ready to go.
Using Bitwarden
You've got your Bitwarden account set up and the browser extension installed. Now what? Head over to a website that you want to sign up for or where you already have an account. When you enter your username and password, Bitwarden will ask you if you want to save your login information. Click Yes, Save Now.
The next time you want access to that site, head on over to the site's login page. Click the Bitwarden icon on your browser's toolbar, then click on the login to fill in your information.
The browser extension has a setting that automatically fills in your username and password. You can enable that by clicking the Bitwarden icon, selecting Settings, and clicking Options. From there, click Enable Auto-fill On Page Load. I don't use that feature—I've run into sites where it didn't work. Anyway, an extra click isn't going to do me any harm.
Importing your passwords from another service
What if you're using another password manager and want to move to Bitwarden? You definitely don't want to type in all those logins again, do you? Bitwarden has an import function that you can use to import passwords from a couple dozen other tools, including LastPass, 1Password, KeePass, and several web browsers.
To get started, you'll need to export your passwords as a CSV, HTML, XML, or JSON file (depending on which password manager you're using). Then, log into your Bitwarden vault. Click Tools and Import Data. Select the application you're importing passwords from, then upload the file containing the passwords. Click Import.
The import is surprisingly quick, even with a large number of passwords. Just remember to securely delete the import file after you've done the deed. You don't want to leave the keys to your various kingdoms lying around, especially if they're not encrypted.
How safe is it?
How safe is anything, really? The folks behind Bitwarden try to make it as secure as possible. Things can happen, though, and someone could breach your account.
It always helps, as I mentioned earlier in this article, to have a strong, complex master password. You can also set up two-factor authentication to further harden your account.
If you decide to use Bitwarden, the only advice I can give you is to not store logins to financial institutions or other sites that contain sensitive information. In the end, it's up to you to decide how and with what you use Bitwarden.
Final thought
Until about a year ago, I was a dedicated user of LastPass. But Bitwarden won me over. While it might not have all the bells and whistles of its competitors, Bitwarden does what I need it to do, and it does it securely."
language$guide,"Which programming languages should you learn?
Learning a new programming language is a great way to get ahead in your career. But which one?
If you want to get started or get ahead in your programming career, learning a new language is a smart idea. But the huge number of languages in active use invites the question: Which programming language is the best one to know? To answer that, let's start with a simplifying question: What sort of programming do you want to do?
If you want to do web programming on the client side, then the specialized languages HTML, CSS, and JavaScript—in one of its seemingly infinite dialects—are de rigueur.
If you want to do web programming on the server side, the options include all of the familiar general-purpose languages: C++, Golang, Java, C#, Node.js, Perl, Python, Ruby, and so on. As a matter of course, server-side programs interact with datastores, such as relational and other databases, which means query languages such as SQL may come into play.
If you're writing native apps for mobile devices, knowing the target platform is important. For Apple devices, Swift has supplanted Objective C as the language of choice. For Android devices, Java (with dedicated libraries and toolsets) remains the dominant language. There are special languages such as Xamarin, used with C#, that can generate platform-specific code for Apple, Android, and Windows devices.

What about general-purpose languages? There are various choices within the usual pigeonholes. Among the dynamic or scripting languages (e.g., Perl, Python, and Ruby), there are newer offerings such as Node.js. Java and C#, which are more alike than their fans like to admit, remain the dominant statically compiled languages targeted at a virtual machine (the JVM and CLR, respectively). Among languages that compile into native executables, C++ is still in the mix, along with later arrivals such as Golang and Rust. General-purpose functional languages abound (e.g., Clojure, Haskell, Erlang, F#, Lisp, and Scala), often with passionately devoted communities. It's worth noting that object-oriented languages such as Java and C# have added functional constructs (in particular, lambdas), and the dynamic languages have had functional constructs from the start.

Let me end with a pitch for C, which is a small, elegant, and extensible language not to be confused with C++. Modern operating systems are written mostly in C, with the rest in assembly language. The standard libraries on any platform are likewise mostly in C. For example, any program that issues the Hello, world! greeting does so through a call to the C library function named write.
C serves as a portable assembly language, exposing details about the underlying system that other high-level languages deliberately hide. To understand C is thus to gain a better grasp of how programs contend for the shared system resources (processors, memory, and I/O devices) required for execution. C is at once high-level and close-to-the-metal, so unrivaled in performance—except, of course, for assembly language. Finally, C is the lingua franca among programming languages, and almost every general-purpose language supports C calls in one form or another.
For a modern introduction to C, consider my book C Programming: Introducing Portable Assembler. No matter how you go about it, learn C and you'll learn a lot more than just another programming language.

"
AWK,"Drinking coffee with AWK
Keep track of what your office mates owe for the coffee they drink with a simple AWK program.
The following is based on a true story, although some names and details have been changed.
A long time ago, in a place far away, there was an office. The office did not, for various reasons, buy instant coffee. Some workers in that office got together and decided to institute the ""Coffee Corner.""
A member of the Coffee Corner would buy some instant coffee, and the other members would pay them back. It came to pass that some people drank more coffee than others, so the level of a ""half-member"" was added: a half-member was allowed a limited number of coffees per week and would pay half of what a member paid.
Managing this was a huge pain. I had just read The Unix Programming Environment and wanted to practice my AWK programming. So I volunteered to create a system.
Step 1: I kept a database of members and their debt to the Coffee Corner. I did it in an AWK-friendly format, where fields are separated by colons:
member:john:1:22
member:jane:0.5:33
member:pratyush:0.5:17
member:jing:1:27
The first field above identifies what kind of row this is (member). The second field is the member's name (i.e., their email username without the @). The next field is their membership level (full=1 or half=0.5). The last field is their debt to the Coffee Corner. A positive number means they owe money, a negative number means the Coffee Corner owes them.
Step 2: I kept a log of inputs to and outputs from the Coffee Corner:
payment:jane:33
payment:pratyush:17
bought:john:60
payback:john:50
Jane paid $33, Pratyush paid $17, John bought $60 worth of coffee, and the Coffee Corner paid John $50.
Step 3: I was ready to write some code. The code would process the members and payments and spit out an updated members file with the new debts.
#!/usr/bin/env --split-string=awk -F: -f
The shebang (#!) line required some work! I used the env command to allow passing multiple arguments from the shebang: specifically, the -F command-line argument to AWK tells it what the field separator is.

An AWK program is a sequence of rules. (It can also contain function definitions, but I don't need any for the Coffee Corner.)
The first rule reads the members file. When I run the command, I always give it the members file first, and the payments file second. It uses AWK associative arrays to record membership levels in the members array and current debt in the debt array.
$1 == ""member"" {
   members[$2]=$3
   debt[$2]=$4
   total_members += $3
}
The second rule reduces the debt when a payment is recorded.
$1 == ""payment"" {
   debt[$2] -= $3
}
Payback is the opposite: it increases the debt. This elegantly supports the case of accidentally giving someone too much money.
$1 == ""payback"" {
   debt[$2] += $3
}
The most complicated part happens when someone buys (""bought"") instant coffee for the Coffee Club's use. It is treated as a payment and the person's debt is reduced by the appropriate amount. Next, it calculates the per-member fee. It iterates over all members and increases their debt, according to their level of membership.
$1 == ""bought"" {
   debt[$2] -= $3
   per_member = $3/total_members
   for (x in members) {
       debt[x] += per_member * members[x]
   }
}
The END pattern is special: it happens exactly once, when AWK has no more lines to process. At this point, it spits out the new members file with updated debt levels.
END {
   for (x in members) {
       printf ""%s:%s:%s\n"", x, members[x], debt[x]
   }
}
Along with a script that iterates over the members and sends a reminder email to people to pay their dues (for positive debts), this system managed the Coffee Corner for quite a while.


"
containers$Podman$root,"How does rootless Podman work?
Learn how Podman takes advantage of user namespaces to run in rootless mode.
In my previous article on user namespace and Podman, I discussed how you can use Podman commands to launch different containers with different user namespaces giving you better separation between containers. Podman also takes advantage of user namespaces to be able to run in rootless mode. Basically, when a non-privileged user runs Podman, the tool sets up and joins a user namespace. After Podman becomes root inside of the user namespace, Podman is allowed to mount certain filesystems and set up the container. Note there is no privilege escalation here other then additional UIDs available to the user, explained below.
How does Podman create the user namespace?
shadow-utils
Most current Linux distributions include a version of shadow-utils that uses the /etc/subuid and /etc/subgid files to determine what UIDs and GIDs are available for a user in a user namespace.
$ cat /etc/subuid
dwalsh:100000:65536
test:165536:65536
$ cat /etc/subgid
dwalsh:100000:65536
test:165536:65536
The useradd program automatically allocates 65536 UIDs for each user added to the system. If you have existing users on a system, you would need to allocate the UIDs yourself. The format of these files is username:STARTUID:TOTALUIDS. Meaning in my case, dwalsh is allocated UIDs 100000 through 165535 along with my default UID, which happens to be 3265 defined in /etc/passwd. You need to be careful when allocating these UID ranges that they don't overlap with any real UID on the system. If you had a user listed as UID 100001, now I (dwalsh) would be able to become this UID and potentially read/write/execute files owned by the UID.
Shadow-utils also adds two setuid programs (or setfilecap). On Fedora I have:
$ getcap /usr/bin/newuidmap
/usr/bin/newuidmap = cap_setuid+ep
$ getcap /usr/bin/newgidmap
/usr/bin/newgidmap = cap_setgid+ep
Podman executes these files to set up the user namespace. You can see the mappings by examining /proc/self/uid_map and /proc/self/gid_map from inside of the rootless container.
$ podman run alpine cat /proc/self/uid_map /proc/self/gid_map
        0       3267            1
        1       100000          65536
        0       3267            1
        1       100000          65536
As seen above, Podman defaults to mapping root in the container to your current UID (3267) and then maps ranges of allocated UIDs/GIDs in /etc/subuid and /etc/subgid starting at 1. Meaning in my example, UID=1 in the container is UID 100000, UID=2 is UID 100001, all the way up to 65536, which is 165535.
Any item from outside of the user namespace that is owned by a UID or GID that is not mapped into the user namespace appears to belong to the user configured in the kernel.overflowuid sysctl, which by default is 35534, which my /etc/passwd file says has the name nobody. Since your process can't run as an ID that isn't mapped, the owner and group permissions don't apply, so you can only access these files based on their ""other"" permissions. This includes all files owned by real root on the system running the container, since root is not mapped into the user namespace.
The Buildah command has a cool feature, buildah unshare. This puts you in the same user namespace that Podman runs in, but without entering the container's filesystem, so you can list the contents of your home directory.
$ ls -ild /home/dwalsh
8193 drwx--x--x. 290 dwalsh dwalsh 20480 Jan 29 07:58 /home/dwalsh
$ buildah unshare ls -ld /home/dwalsh
drwx--x--x. 290 root root 20480 Jan 29 07:58 /home/dwalsh
Notice that when listing the home dir attributes outside the user namespace, the kernel reports the ownership as dwalsh, while inside the user namespace it reports the directory as owned by root. This is because the home directory is owned by 3267, and inside the user namespace we are treating that UID as root.
What happens next in Podman after the user namespace is set up?
Podman uses containers/storage to pull the container image, and containers/storage is smart enough to map all files owned by root in the image to the root of the user namespace, and any other files owned by different UIDs to their user namespace UIDs. By default, this content gets written to ~/.local/share/containers/storage. Container storage works in rootless mode with either the vfs mode or with Overlay. Note: Overlay is supported only if the fuse-overlayfs executable is installed.
The kernel only allows user namespace root to mount certain types of filesystems; at this time it allows mounting of procfs, sysfs, tmpfs, fusefs, and bind mounts (as long as the source and destination are owned by the user running Podman. OverlayFS is not supported yet, although the kernel teams are working on allowing it).

Podman then mounts the container's storage if it is using fuse-overlayfs; if the storage driver is using vfs, then no mounting is required. Podman on vfs requires a lot of space though, since each container copies the entire underlying filesystem.
Podman then mounts /proc and /sys along with a few tmpfs and creates the devices in the container.
In order to use networking other than the host networking, Podman uses the slirp4netns program to set up User mode networking for unprivileged network namespace. Slirp4netns allows Podman to expose ports within the container to the host. Note that the kernel still will not allow a non-privileged process to bind to ports less than 1024. Podman-1.1 or later is required for binding to ports.
Rootless Podman can use user namespace for container separation, but you only have access to the UIDs defined in the /etc/subuid file.
Conclusion
The Podman tool is enabling people to build and use containers without sacrificing the security of the system; you can give your developers the access they need without giving them root.
And when you put your containers into production, you can take advantage of the extra security provided by the user namespace to keep the workloads isolated from each other."
containers$Linux,"Do Linux distributions still matter with containers?
There are two major trends in container builds: using a base image and building from scratch. Each has engineering tradeoffs.
Some people say Linux distributions no longer matter with containers. Alternative approaches, like distroless and scratch containers, seem to be all the rage. It appears we are considering and making technology decisions based more on fashion sense and immediate emotional gratification than thinking through the secondary effects of our choices. We should be asking questions like: How will these choices affect maintenance six months down the road? What are the engineering tradeoffs? How does this paradigm shift affect our build systems at scale?
It's frustrating to watch. If we forget that engineering is a zero-sum game with measurable tradeoffs—advantages and disadvantages, with costs and benefits of different approaches— we do ourselves a disservice, we do our employers a disservice, and we do our colleagues who will eventually maintain our code a disservice. Finally, we do all of the maintainers (hail the maintainers!) a disservice by not appreciating the work they do.
Understanding the problem
To understand the problem, we have to investigate why we started using Linux distributions in the first place. I would group the reasons into two major buckets: kernels and other packages. Compiling kernels is actually fairly easy. Slackware and Gentoo (I still have a soft spot in my heart) taught us that. 
On the other hand, the tremendous amount of development and runtime software that needs to be packaged for a usable Linux system can be daunting. Furthermore, the only way you can ensure that millions of permutations of packages can be installed and work together is by using the old paradigm: compile it and ship it together as a thing (i.e., a Linux distribution). So, why do Linux distributions compile kernels and all the packages together? Simple: to make sure things work together.
First, let's talk about kernels. The kernel is special. Booting a Linux system without a compiled kernel is a bit of a challenge. It's the core of a Linux operating system, and it's the first thing we rely on when a system boots. Kernels have a lot of different configuration options when they're being compiled that can have a tremendous effect on how hardware and software run on one. A secondary problem in this bucket is that system software, like compilers, C libraries, and interpreters, must be tuned for the options you built into the kernel. Gentoo taught us this in a visceral way, which turned everyone into a miniature distribution maintainer.
Embarrassingly (because I have worked with containers for the last five years), I must admit that I have compiled kernels quite recently. I had to get nested KVM working on RHEL 7 so that I could run OpenShift on OpenStack virtual machines, in a KVM virtual machine on my laptop, as well as our Container Development Kit (CDK). #justsayin Suffice to say, I fired RHEL7 up on a brand new 4.X kernel at the time. Like any good sysadmin, I was a little worried that I missed some important configuration options and patches. And, of course, I had missed some things. Sleep mode stopped working right, my docking station stopped working right, and there were numerous other small, random errors. But it did work well enough for a live demo of OpenShift on OpenStack, in a single KVM virtual machine on my laptop. Come on, that's kinda' fun, right? But I digress…
Now, let's talk about all the other packages. While the kernel and associated system software can be tricky to compile, the much, much bigger problem from a workload perspective is compiling thousands and thousands of packages to give us a useable Linux system. Each package requires subject matter expertise. Some pieces of software require running only three commands: ./configure, make, and make install. Others require a lot of subject matter expertise ranging from adding users and configuring specific defaults in etc to running post-install scripts and adding systemd unit files. The set of skills necessary for the thousands of different pieces of software you might use is daunting for any single person. But, if you want a usable system with the ability to try new software whenever you want, you have to learn how to compile and install the new software before you can even begin to learn to use it. That's Linux without a Linux distribution. That's the engineering problem you are agreeing to when you forgo a Linux distribution.
The point is that you have to build everything together to ensure it works together with any sane level of reliability, and it takes a ton of knowledge to build a usable cohort of packages. This is more knowledge than any single developer or sysadmin is ever going to reasonably learn and retain. Every problem I described applies to your container host (kernel and system software) and container image (system software and all other packages)—notice the overlap; there are compilers, C libraries, interpreters, and JVMs in the container image, too.
The solution
You already know this, but Linux distributions are the solution. Stop reading and send your nearest package maintainer (again, hail the maintainers!) an e-card (wait, did I just give my age away?). Seriously though, these people do a ton of work, and it's really underappreciated. Kubernetes, Istio, Prometheus, and Knative: I am looking at you. Your time is coming too, when you will be in maintenance mode, overused, and underappreciated. I will be writing this same article again, probably about Kubernetes, in about seven to 10 years.
First principles with container builds
There are tradeoffs to building from scratch and building from base images.
Building from base images
Building from base images has the advantage that most build operations are nothing more than a package install or update. It relies on a ton of work done by package maintainers in a Linux distribution. It also has the advantage that a patching event six months—or even 10 years—from now (with RHEL) is an operations/systems administrator event (yum update), not a developer event (that requires picking through code to figure out why some function argument no longer works).
Let's double-click on that a bit. Application code relies on a lot of libraries ranging from JSON munging libraries to object-relational mappers. Unlike the Linux kernel and Glibc, these types of libraries change with very little regard to breaking API compatibility. That means that three years from now your patching event likely becomes a code-changing event, not a yum update event. Got it, let that sink in. Developers, you are getting paged at 2 AM if the security team can't find a firewall hack to block the exploit.
Building from a base image is not perfect; there are disadvantages, like the size of all the dependencies that get dragged in. This will almost always make your container images larger than building from scratch. Another disadvantage is you will not always have access to the latest upstream code. This can be frustrating for developers, especially when you just want to get something out the door, but not as frustrating as being paged to look at a library you haven't thought about in three years that the upstream maintainers have been changing the whole time.
If you are a web developer and rolling your eyes at me, I have one word for you: DevOps. That means you are carrying a pager, my friend.
Building from scratch
Scratch builds have the advantage of being really small. When you don't rely on a Linux distribution in the container, you have a lot of control, which means you can customize everything for your needs. This is a best-of-breed model, and it's valid in certain use cases. Another advantage is you have access to the latest packages. You don't have to wait for a Linux distro to update anything. You are in control, so you choose when to spend the engineering work to incorporate new software.
Remember, there is a cost to controlling everything. Often, updating to new libraries with new features drags in unwanted API changes, which means fixing incompatibilities in code (in other words, shaving yaks). Shaving yaks at 2 AM when the application doesn't work is not fun. Luckily, with containers, you can roll back and shave the yaks the next business day, but it will still eat into your time for delivering new value to the business, new features to your applications. Welcome to the life of a sysadmin.
OK, that said, there are times that building from scratch makes sense. I will completely concede that statically compiled Golang programs and C programs are two decent candidates for scratch/distroless builds. With these types of programs, every container build is a compile event. You still have to worry about API breakage three years from now, but if you are a Golang shop, you should have the skillset to fix things over time.
Conclusion
Basically, Linux distributions do a ton of work to save you time—on a regular Linux system or with containers. The knowledge that maintainers have is tremendous and leveraged so much without really being appreciated. The adoption of containers has made the problem even worse because it's even further abstracted.
With container hosts, a Linux distribution offers you access to a wide hardware ecosystem, ranging from tiny ARM systems, to giant 128 CPU x86 boxes, to cloud-provider VMs. They offer working container engines and container runtimes out of the box, so you can just fire up your containers and let somebody else worry about making things work.
For container images, Linux distributions offer you easy access to a ton of software for your projects. Even when you build from scratch, you will likely look at how a package maintainer built and shipped things—a good artist is a good thief—so, don't undervalue this work.
So, thank you to all of the maintainers in Fedora, RHEL (Frantisek, you are my hero), Debian, Gentoo, and every other Linux distribution. I appreciate the work you do, even though I am a ""container guy."""
DevOps$Kuberneters$containers,"Deploy InfluxDB and Grafana on Kubernetes to collect Twitter stats
Monitor your Twitter stats with a Python script, InfluxDB, and Grafana running in Kubernetes or OKD.
Kubernetes is the de facto leader in container orchestration on the market, and it is an amazingly configurable and powerful orchestration tool. As with many powerful tools, it can be somewhat confusing at first. This walk-through will cover the basics of creating multiple pods, configuring them with secret credentials and configuration files, and exposing the services to the world by creating an InfluxDB and Grafana deployment and Kubernetes cron job to gather statistics about your Twitter account from the Twitter developer API, all deployed on Kubernetes or OKD (formerly OpenShift Origin).
Requirements
A Twitter account to monitor
A Twitter developer API account for gathering stats
A Kubernetes or OKD cluster (or MiniKube or MiniShift)
The kubectl or oc command-line interface (CLI) tools installed
What you'll learn


This walkthrough will introduce you to a variety of Kubernetes concepts. You'll learn about Kubernetes cron jobs, ConfigMaps, Secrets, Deployments, Services, and Ingress.
If you choose to dive in further, the included files can serve as an introduction to Tweepy, an ""easy-to-use Python module for accessing the Twitter API,"" InfluxDB configuration, and automated Grafana dashboard providers.
Architecture
This app consists of a Python script that polls the Twitter developer API on a schedule for stats about your Twitter account and stores them in InfluxDB as time-series data. Grafana displays the data in human-friendly formats (counts and graphs) on customizable dashboards.
All of these components run in Kubernetes- or OKD-managed containers.
Prerequisites
Get a Twitter developer API account
Follow the instructions to sign up for a Twitter developer account, which allows access to the Twitter API. Record your API_KEY, API_SECRET, ACCESS_TOKEN, and ACCESS_SECRET to use later.
Clone the TwitterGraph repo
The TwitterGraph GitHub repo contains all the files needed for this project, as well as a few to make life easier if you want to do it all over again.
Set up InfluxDB
InfluxDB is an open source data store designed specifically for time-series data. Since this project will poll Twitter on a schedule using a Kubernetes cron job, InfluxDB is perfect for holding the data.
The Docker-maintained InfluxDB image on DockerHub will work fine for this project. It works out-of-the-box with both Kubernetes and OKD.
Create a deployment
A Kubernetes deployment describes the desired state of a resource. For InfluxDB, this is a single container in a pod running an instance of the InfluxDB image.
A barebones InfluxDB deployment can be created with the kubectl create deployment command:
kubectl create deployment influxdb --image=docker.io/influxdb:1.6.4
The newly created deployment can be seen with the kubectl get deployment command:
kubectl get deployments
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
influxdb   1         1         1            1           7m40s
Specific details of the deployment can be viewed with the kubectl describe deployment command:
kubectl describe deployment influxdb
Name:                   influxdb
Namespace:              twittergraph
CreationTimestamp:      Mon, 14 Jan 2019 11:31:12 -0500
Labels:                 app=influxdb
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=influxdb
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=influxdb
  Containers:
   influxdb:
    Image:        docker.io/influxdb:1.6.4
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   influxdb-85f7b44c44 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  8m    deployment-controller  Scaled up replica set influxdb-85f7b44c44 to 1
Configure InfluxDB credentials using secrets
Currently, Kubernetes is running an InfluxDB container with the default configuration from the docker.io/influxdb:1.6.4 image, but that is not necessarily very helpful for a database server. The database needs to be configured to use a specific set of credentials and to store the database data between restarts.
Kuberenetes secrets are a way to store sensitive information (such as passwords) and inject them into running containers as either environment variables or mounted volumes. This is perfect for storing database credentials and connection information, both to configure InfluxDB and to tell Grafana and the Python cron job how to connect to it.
You need four bits of information to accomplish both tasks:
INFLUXDB_DATABASE—the name of the database to use
INFLUXDB_HOST—the hostname where the database server is running
INFLUXDB_USERNAME—the username to log in with
INFLUXDB_PASSWORD—the password to log in with
Create a secret using the kubectl create secret command and some basic credentials:
kubectl create secret generic influxdb-creds \
  --from-literal=INFLUXDB_DATABASE=twittergraph \
  --from-literal=INFLUXDB_USERNAME=root \
  --from-literal=INFLUXDB_PASSWORD=root \
  --from-literal=INFLUXDB_HOST=influxdb
This command creates a ""generic-type"" secret (as opposed to ""tls-"" or ""docker-registry-type"" secrets) named influxdb-creds populated with some default credentials. Secrets use key/value pairs to store data, and this is perfect for use as environment variables within a container.
As with the examples above, the secret can be seen with the kubectl get secret command:
kubectl get secret influxdb-creds
NAME             TYPE      DATA      AGE
influxdb-creds   Opaque    4         11s
The keys contained within the secret (but not the values) can be seen using the kubectl describe secret command. In this case, the INFLUXDB*_ keys are listed in the influxdb-creds secret:
kubectl describe secret influxdb-creds
Name:         influxdb-creds
Namespace:    twittergraph
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
INFLUXDB_DATABASE:  12 bytes
INFLUXDB_HOST:      8 bytes
INFLUXDB_PASSWORD:  4 bytes
INFLUXDB_USERNAME:  4 bytes
Now that the secret has been created, it can be shared with the InfluxDB pod running the database as an environment variable.
To share the secret with the InfluxDB pod, it needs to be referenced as an environment variable in the deployment created earlier. The existing deployment can be edited with the kubectl edit deployment command, which will open the deployment object in your system's default editor set. When the file is saved, Kubernetes will apply the changes to the deployment.
To add environment variables for each of the secrets, the pod spec contained in the deployment needs to be modified. Specifically, the .spec.template.spec.containers array needs to be modified to include an envFrom section.
Using the command kubectl edit deployment influxdb, find that section in the deployment (this example is truncated):
spec:
  template:
    spec:
      containers:
      - image: docker.io/influxdb:1.6.4
        imagePullPolicy: IfNotPresent
        name: influxdb
This section describes a very basic InfluxDB container. Secrets can be added to the container with an env array for each key/value to be mapped in. Alternatively, envFrom can be used to map all the key/value pairs into the container, using the key names as the variables.
For the values in the influxdb-creds secret, the container spec would look like this:
spec:
  containers:
  - name: influxdb
    envFrom:
    - secretRef:
        name: influxdb-creds
After editing the deployment, Kubernetes will destroy the running pod and create a new one with the mapped environment variables. Remember, the deployment describes the desired state, so Kubernetes replaces the old pod with a new one matching that state.
You can validate that the environment variables are included in your deployment with kubectl describe deployment influxdb:
Environment Variables from:
  influxdb-creds  Secret  Optional: false
Configure persistent storage for InfluxDB
A database is not very useful if all of its data is destroyed each time the service is restarted. In the current InfluxDB deployment, all of the data is stored in the container and lost when Kubernetes destroys and recreates pods. A PersistentVolume is needed to store data permanently.
To get persistent storage in a Kubernetes cluster, a PersistentVolumeClaim (PVC) is created that describes the type and details of the volume needed, and Kubernetes will find a previously created volume that fits the request (or create one with a dynamic volume provisioner, if there is one).
Unfortunately, the kubectl CLI tool does not have the ability to create PVCs directly, but a PVC can be specified as a YAML file and created with kubectl create -f <filename>:
Create a file named pvc.yaml with a generic 2G claim:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: influxdb
    project: twittergraph
  name: influxdb
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
Then, create the PVC:
kubectl create -f pvc.yaml
You can validate that the PVC was created and bound to a PersistentVolume with kubectl get pvc:
kubectl get pvc
NAME       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
influxdb   Bound     pvc-27c7b0a7-1828-11e9-831a-0800277ca5a7   2Gi        RWO            standard       173m
From the output above, you can see the PVC influxdb was matched to a PV (or Volume) named pvc-27c7b0a7-1828-11e9-831a-0800277ca5a7 (your name will vary) and bound (STATUS: Bound).
If your PVC does not have a volume, or the status is something other than Bound, you may need to talk to your cluster administrator. (This process should work fine with MiniKube, MiniShift, or any cluster with dynamically provisioned volumes.)
Once a PersistentVolume has been assigned to the PVC, the volume can be mounted into the container to provide persistent storage. Once again, this entails editing the deployment, first to add a volume object and second to reference that volume within the container spec as a volumeMount.
Edit the deployment with kubectl edit deployment influxdb and add a .spec.template.spec.volumes section below the containers section (example truncated for brevity):
spec:
  template:
    spec:
      volumes:
      - name: var-lib-influxdb
        persistentVolumeClaim:
          claimName: influxdb
In this example, a volume named var-lib-influxdb is added to the deployment, which references the PVC influxdb created earlier.
Now, add a volumeMount to the container spec. The volume mount references the volume added earlier (name: var-lib-influxdb) and mounts the volume to the InfluxDB data directory, /var/lib/influxdb:
spec:
  template:
    spec:
      containers:
        volumeMounts:
        - mountPath: /var/lib/influxdb
          name: var-lib-influxdb
The InfluxDB deployment
After the above, you should have a deployment for InfluxDB that looks something like this:
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""3""
  creationTimestamp: null
  generation: 1
  labels:
    app: influxdb
    project: twittergraph
  name: influxdb
  selfLink: /apis/extensions/v1beta1/namespaces/twittergraph/deployments/influxdb
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: influxdb
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: influxdb
    spec:
      containers:
      - envFrom:
        - secretRef:
            name: influxdb-creds
        image: docker.io/influxdb:1.6.4
        imagePullPolicy: IfNotPresent
        name: influxdb
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/influxdb
          name: var-lib-influxdb
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - name: var-lib-influxdb
        persistentVolumeClaim:
          claimName: influxdb
status: {}
Expose InfluxDB (to the cluster only) with a Service
By default, pods in this project are unable to talk to one another. A Kubernetes Service is required to ""expose"" the pod to the cluster or to the public. In the case of InfluxDB, the pod needs to be able to accept traffic on TCP port 8086 from the Grafana and cron job pods (which will be created later). To do this, expose (i.e., create a service for) the pod using a Cluster IP. Cluster IPs are available only to other pods in the cluster. Do this with the kubectl expose command:
kubectl expose deployment influxdb --port=8086 --target-port=8086 --protocol=TCP --type=ClusterIP
The newly created service can be verified with the kubectl describe service command:
kubectl describe service influxdb
Name:              influxdb
Namespace:         twittergraph
Labels:            app=influxdb
                   project=twittergraph
Annotations:       <none>
Selector:          app=influxdb
Type:              ClusterIP
IP:                10.108.196.112
Port:              <unset>  8086/TCP
TargetPort:        8086/TCP
Endpoints:         172.17.0.5:8086
Session Affinity:  None
Events:            <none>
Some of the details (specifically the IP addresses) will vary from the example. The ""IP"" is an IP address internal to your cluster that's been assigned to the service through which other pods can communicate with InfluxDB. The ""Endpoints"" are the container's IP and port that's listening for connections. The service will route traffic to the internal cluster IP to the container itself.
Now that InfluxDB is set up, move on to Grafana.
Set up Grafana
Grafana is an open source project for visualizing time-series data (think: pretty, pretty graphs).
As with InfluxDB, the official Grafana image on DockerHub works out-of-the-box for this project, both with Kubernetes and OKD.
Create a deployment
Just as before, create a deployment based on the official Grafana image:
kubectl create deployment grafana --image=docker.io/grafana/grafana:5.3.2
There should now be a grafana deployment alongside the influxdb deployment:
kubectl get deployments
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
grafana    1         1         1            1           7s
influxdb   1         1         1            1           5h12m
Set up Grafana credentials and config files with secrets and ConfigMaps
Building on what you've already learned, configuring Grafana should be both similar and easier. Grafana doesn't require persistent storage, since it's reading its data out of the InfluxDB database. It does, however, need two configuration files to set up a dashboard provider to load dashboards dynamically from files, the dashboard file itself, a file to connect the dashboard file to InfluxDB as a data source, and finally a secret to store default login credentials.
The credentials secret works the same as the influxdb-creds secret already created. By default, the Grafana image looks for environment variables named GF_SECURITY_ADMIN_USER and GF_SECURITY_ADMIN_PASSWORD to set the admin username and password on startup. These can be whatever you like, but remember them so you can use them to log into Grafana when you have it configured.
Create a secret named grafana-creds for the Grafana credentials with the kubectl create secret command:
kubectl create secret generic grafana-creds \
  --from-literal=GF_SECURITY_ADMIN_USER=admin \
  --from-literal=GF_SECURITY_ADMIN_PASSWORD=graphsRcool
Share this secret as an environment variable using envFrom, this time in the Grafana deployment. Edit the deployment with kubectl edit deployment grafana and add the environment variables to the container spec:
spec:
  containers:
  - name: grafana
    envFrom:
    - secretRef:
        name: grafana-creds
Validate that the environment variables have been added to the deployment with kubectl describe deployment grafana:
Environment Variables from:
  grafana-creds  Secret  Optional: false
That's all that's required to start using Grafana. The rest of the configuration can be done in the web interface if desired, but with just a few config files, Grafana can be fully configured when it starts.
Kubernetes ConfigMaps are similar to secrets and can be consumed the same way by a pod, but they don't store the information obfuscated within Kubernetes. Config maps are useful for adding configuration files or variables into the containers in a pod.
The Grafana instance in this project has three config files that need to be written into the running container:
influxdb-datasource.yml—tells Grafana how to talk to the InfluxDB database
grafana-dashboard-provider.yml—tells Grafana where to look for JSON files describing dashboards
twittergraph-dashboard.json—describes the dashboard for displaying the Twitter data collected
Kubernetes makes it easy to add these files: they can all be added to the same config map at once, and they can be mounted to different locations on the filesystem despite being in the same config map.
If you have not done so already, clone the TwitterGraph GitHub repo. These files are really specific to this project, so the easiest way to consume them is directly from the repo (although they could certainly be written manually).
From the directory with the contents of the repo, create a config map named grafana-config using the kubectl create configmap command:
kubectl create configmap grafana-config \
  --from-file=influxdb-datasource.yml=influxdb-datasource.yml \
  --from-file=grafana-dashboard-provider.yml=grafana-dashboard-provider.yml \
  --from-file=twittergraph-dashboard.json=twittergraph-dashboard.json
The kubectl create configmap command creates a config map named grafana-config and stores the contents as the value for the key specified. The --from-file argument follows the form --from-file=<keyname>=<pathToFile>, so in this case, the filename is being used as the key for future clarity.
Like secrets, details of a config map can be seen with kubectl describe configmap. Unlike secrets, the contents of the config map are visible in the output. Use kubectl describe configmap grafana-config to see the three files stored as keys in the config map (results are truncated because they're looooooong):
kubectl describe configmap grafana-config
kubectl describe cm grafana-config
Name:         grafana-config
Namespace:    twittergraph
Labels:       <none>
Annotations:  <none>

Data
====
grafana-dashboard-provider.yml:
----
apiVersion: 1

providers:
- name: 'default'
  orgId: 1
  folder: ''
  type: file
<snip>
Each of the filenames should be stored as keys and their contents as the values (such as the grafana-dashboard-provider.yml above).
While config maps can be shared as environment variables (as the credential secrets were above), the contents of this config map need to be mounted into the container as files. To do this, a volume can be created from config map in the grafana deployment. Similar to the persistent volume, use kubectl edit deployment grafana to add volume .spec.template.spec.volumes:
spec:
  template:
    spec:
      volumes:
      - configMap:
          name: grafana-config
        name: grafana-config
Then edit the container spec to mount each of the keys stored in the config map as files in their respective locations in the Grafana container. Under .spec.template.spec.containers, add a volumeMounts section for the volumes:
spec:
  template:
    spec:
      containers:
      - name: grafana
        volumeMounts:
        - mountPath: /etc/grafana/provisioning/datasources/influxdb-datasource.yml
          name: grafana-config
          readOnly: true
          subPath: influxdb-datasource.yml
        - mountPath: /etc/grafana/provisioning/dashboards/grafana-dashboard-provider.yml
          name: grafana-config
          readOnly: true
          subPath: grafana-dashboard-provider.yml
        - mountPath: /var/lib/grafana/dashboards/twittergraph-dashboard.json
          name: grafana-config
          readOnly: true
          subPath: twittergraph-dashboard.json
The name section references the name of the config map volume and adding the subPath items allows Kubernetes to mount each file without overwriting the rest of the contents of that directory. Without it, /etc/grafana/provisioning/datasources/influxdb-datasource.yml for example, would be the only file in /etc/grafana/provisioning/datasources.
Each of the files can be verified by looking at them within the running container using the kubectl exec command. First, find the Grafana pod's current name. The pod will have a randomized name similar to grafana-586775fcc4-s7r2z and should be visible when running the command kubectl get pods:
kubectl get pods
NAME                        READY     STATUS    RESTARTS   AGE
grafana-586775fcc4-s7r2z    1/1       Running   0          93s
influxdb-595487b7f9-zgtvx   1/1       Running   0          18h
Substituting the name of your Grafana pod, you can verify the contents of the influxdb-datasource.yml file, for example (truncated for brevity):
kubectl exec -it grafana-586775fcc4-s7r2z cat /etc/grafana/provisioning/datasources/influxdb-datasource.yml
# config file version
apiVersion: 1

# list of datasources to insert/update depending
# what's available in the database
datasources:
  # <string, required> name of the datasource. Required
- name: influxdb
Expose the Grafana service
Now that it's configured, expose the Grafana service so it can be viewed in a browser. Because Grafana should be visible from outside the cluster, the LoadBalancer service type will be used rather than the internal-only ClusterIP type.
For production clusters or cloud environments that support LoadBalancer services, an external IP is dynamically provisioned when the service is created. For MiniKube or MiniShift, LoadBalancer services are available via the minikube service command, which opens your default browser to a URL and port where the service is available on your host VM.
The Grafana deployment is listening on port 3000 for HTTP traffic. Expose it using the LoadBalancer-type service using the kubectl expose command:
kubectl expose deployment grafana --type=LoadBalancer --port=80 --target-port=3000 --protocol=TCP
service/grafana exposed
After the service is exposed, you can validate the configuration with kubectl get service grafana:
kubectl get service grafana
NAME      TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
grafana   LoadBalancer   10.101.113.249   <pending>     80:31235/TCP   9m35s
As mentioned above, MiniKube and MiniShift deployments will not automatically assign an EXTERNAL-IP and will be listed as <pending>. Running minikube service grafana (or minikube service grafana --namespace <namespace> if you created your deployments in a namespace other than Default) will open your default browser to the IP and port combo where Grafana is exposed on your host VM.
At this point, Grafana is configured to talk to InfluxDB and has automatically provisioned a dashboard to display the Twitter stats. Now it's time to get some actual stats and put them into the database.
Create the cron job
A Kubernetes cron job, like its namesake cron, is a way to run a job on a particular schedule. In the case of Kubernetes, the job is a task running in a container: a Kubernetes job scheduled and tracked by Kubernetes to ensure its completion.
For this project, the cron job is a single container running a Python script to gather Twitter stats.
Create a secret for the Twitter API credentials
The cron job uses your Twitter API credentials to connect to the API and pull the stats from environment variables inside the container. Create a secret to store the Twitter API credentials and the name of the account to gather the stats from (substitute your own credentials and account name below):
kubectl create secret generic twitter-creds \
    --from-literal=TWITTER_ACCESS_SECRET=<your twitter access secret> \
    --from-literal=TWITTER_ACCESS_TOKEN=<your twitter access token> \
    --from-literal=TWITTER_API_KEY=<your twitter api key > \
    --from-literal=TWITTER_API_SECRET=<your twitter api secret> \
    --from-literal=TWITTER_USER=<your twitter username>
Create a cron job
Finally, it's time to create the cron job to gather statistics. Unfortunately, kubectl doesn't have a way to create a cron job directly, so once again the object must be described in a YAML file and loaded with kubectl create -f <filename>.
Create a file named cronjob.yml describing the job to run:
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  labels:
    app: twittergraph
  name: twittergraph
spec:
  concurrencyPolicy: Replace
  failedJobsHistoryLimit: 3
  jobTemplate:
    metadata:
    spec:
      template:
        metadata:
        spec:
          containers:
          - envFrom:
            - secretRef:
                name: twitter-creds
            - secretRef:
                name: influxdb-creds
            image: docker.io/clcollins/twittergraph:1.0
            imagePullPolicy: Always
            name: twittergraph
          restartPolicy: Never
  schedule: '*/3 * * * *'
  successfulJobsHistoryLimit: 3
Looking over this file, the key pieces of a Kubernetes cron job are evident. The cron job spec contains a jobTemplate describing the Kubernetes job to run. In this case, the job consists of a single container with the Twitter and InfluxDB credentials' secrets shared as environment variables using the envFrom that was used in the deployments.
This job uses a custom image from Docker Hub, clcollins/twittergraph:1.0. The image is just Python 3.6 and contains the app.py Python script for TwitterGraph. (If you'd rather build the image yourself, you can follow the instructions in BUILDING.md in the GitHub repo to build the image with Source-To-Image.)
Wrapping the job template spec are the cron job spec options. The most important part, outside of the job itself, is arguably the schedule, set here to run every 3 minutes forever. The other important bit is the concurrencyPolicy, which is set to replace, so if the previous job is still running when it's time to start a new one, the pod running the old job is destroyed and replaced with a new pod.
Use the kubectl create -f cronjob.yml command to create the cron job:
kubectl create -f cronjob.yaml
cronjob.batch/twittergraph created
The cron job can then be validated with kubectl describe cronjob twittergraph (example truncated for brevity):
kubectl describe cronjob twitterGraph
Name:                       twittergraph
Namespace:                  twittergraph
Labels:                     app=twittergraph
Annotations:                <none>
Schedule:                   */3 * * * *
Concurrency Policy:         Replace
Suspend:                    False
Starting Deadline Seconds:  <unset>
Note: With a schedule set to */3 * * * * , Kubernetes won't immediately start the new job. It will wait three minutes for the first period to pass. If you'd like to see immediate results, you can edit the cron job with kubectl edit cronjob twittergraph, and (temporarily) change the schedule to * * * * * to run every minute. Just don't forget to change it back when you're done.
Success!
That should be it. If you've followed all the steps correctly, you will have an InfluxDB database, a cron job collecting stats from your Twitter account, and a Grafana deployment to view the data. For production clusters or cloud deployments of Kubernetes or OpenShift, visit the LoadBalancer IP to log into Grafana using the credentials you set earlier with the GF_SECURITY_ADMIN_USER and GF_SECURITY_ADMIN_PASSWORD. After logging in, select the TwitterGraph dashboard from the Home dropdown at the top-left of the screen. You should see something like the image below, with current counts for your followers, folks you are following, status updates, likes, and lists. It's probably a bit boring at first, but if you leave it running, over time and with more data collection, the graphs will start to look more interesting and provide more useful data!
Where to go from here
The data collected by the TwitterGraph script is relatively simplistic. The stats that are collected are described in the data_points dictionary in the app.py script, but there's a ton of data available. Adding a new cron job that runs daily to collect the day's activity (number of posts, follows, etc.) would be a natural extension of the data. More interesting, probably, would be correlating the daily data collection, e.g., how many followers were gained or lost based on the number of posts that day, etc.
"
Database$JavaScript$Node.js,"Node.js integrates with M: Next big thing in healthcare IT
Join the M revolution and the next big thing in healthcare IT: the integration of the node.js programming language with the NoSQL hierarchical database, M.
M was developed to organize and access with high efficiency the type of data that is typically managed in healthcare, thus making it uniquely well-suited for the job.
One of the biggest reasons for the success of M is that it integrates the database into the language in a natural and seamless way. The growth and involvement of th community of M developers however, has been below the radar for educators and the larger IT community. As a consequece it has been facing challenges for recruiting young new developers, despite the critical importance of this technology for supporting the Health IT infrastructure of the US.
At the recent 26th VistA Community Meeting, an exciting alternative was presented by Rob Tweed. I summarize it as: Node.js meets the M Database.
In his work, Rob has created an intimate integration between the M database and the language features of node.js. The result is a new way of accessing the M database from Javascript code in such a way that the developer doesn't feel that is accessing a database.
It is now possible to access M from node.js, both when using theM implementation of Intersystems Cache and with the open source M implementation of GT.M. This second interface was implemented by David Wicksell, based on the API previously defined for Cache in the GlobalsDB project.
In a recent blog post, Rob describes some of the natural notation in node.js that provides access to the M hierarchical database by nicely following the language patterns of Javascript. Here are some of Rob's examples:
The M expression:
set town = ^patient(123456, ""address"", ""town"")
becomes the Javascript expression:
var town = patient.$('address').$('town')._value;
with some flavor of jQuery.
The following M expression of a healthcare typical example:
^patient(123456,""birthdate"")=-851884200 ^patient(123456,""conditions"",0,""causeOfDeath"")="""" ^patient(123456,""conditions"",0,""codes"",""ICD-10-CM"",0)=""I21.01"" ^patient(123456,""conditions"",0,""codes"",""ICD-9-CM"",0)=""410.00"" ^patient(123456,""conditions"",0,""description"")=""Diagnosis, Active: Hospital Measures - AMI (Code List: 2.16.840.1.113883.3.666.5.3011)"" ^patient(123456,""conditions"",0,""end_time"")=1273104000
becomes the following JSON datastructure that can be manipulated with Javascript:
var patient = new ewd.GlobalNode(""patient"", [123456]); patient._delete(); var document = { ""birthdate"": -851884200, ""conditions"": [ { ""causeOfDeath"": null, ""codes"": { ""ICD-9-CM"": [ ""410.00"" ], ""ICD-10-CM"": [ ""I21.01"" ] }, ""description"": ""Diagnosis, Active: Hospital Measures - AMI (Code List: 2.16.840.1.113883.3.666.5.3011)"", ""end_time"": 1273104000 } ] };
More detailed examples are provided in Rob's blog post. The M module for node.js is available here.
What this achieves is seamless integration between the powerful M hierarchical database and the language features of the very popular node.js implementation of Javascript. This integration becomes a great opportunity for hundreds of node.js developers to join the space of healthcare IT, and to do, as Tim O'Reilly advises: Work on Stuff that Matters!
M is currently being used in hundreds of hospitals in the public sector:
The Department of Veterans Affairs
The Department of Defense
The Indian Health Service
As well as hundreds of hospitals in the private sector:
Kaiser Permanente hospital system
Johns Hopkins
Beth Israel Deaconess Medical Center
Harvard Medical School
In particular at deploments of these EHR systems:
Epic
GE/Centricity
McKesson
Meditech
Given this, and the large popularity of Javascript and the high efficiency of node.js, this may be the most significant event happening in healthcare IT in recent years.
If you are an enthusiast of node.js, or you are looking for the best next language to learn, or you want to do some social good, this could be the thing for you.

"
security&privacy,"Reducing security risks with centralized logging
Centralizing logs and structuring log data for processing can mitigate risks related to insufficient logging.
Logging and log analysis are essential to securing infrastructure, particularly when we consider common vulnerabilities. This article, based on my lightning talk Let's use centralized log collection to make incident response teams happy at FOSDEM'19, aims to raise awareness about the security concerns around insufficient logging, offer a way to avoid the risk, and advocate for more secure practices (disclaimer: I work for NXLog).
Why log collection and why centralized logging?
Logging is, to be specific, an append-only sequence of records written to disk. In practice, logs help you investigate an infrastructure issue as you try to find a cause for misbehavior. A challenge comes up when you have heterogeneous systems with their own standards and formats, and you want to be able to handle and process these in a dependable way. This often comes at the cost of metadata. Centralized logging solutions require commonality, and that commonality often removes the rich metadata many open source logging tools provide.
The security risk of insufficient logging and monitoring
The Open Web Application Security Project (OWASP) is a nonprofit organization that contributes to the industry with incredible projects (including many tools focusing on software security). The organization regularly reports on the riskiest security challenges for application developers and maintainers. In its most recent report on the top 10 most critical web application security risks, OWASP added Insufficient Logging and Monitoring to its list. OWASP warns of risks due to insufficient logging, detection, monitoring, and active response in the following types of scenarios.
Important auditable events, such as logins, failed logins, and high-value transactions are not logged.
Warnings and errors generate none, inadequate, or unclear log messages.
Logs are only being stored locally.
The application is unable to detect, escalate, or alert for active attacks in real time or near real time.
These instances can be mitigated by centralizing logs (i.e., not storing logs locally) and structuring log data for processing (i.e., in alerting dashboards and security suites).
For example, imagine a DNS query leads to a malicious site named hacked.badsite.net. With DNS monitoring, administrators monitor and proactively analyze DNS queries and responses. The efficiency of DNS monitoring relies on both sufficient logging and log collection in order to catch potential issues as well as structuring the resulting DNS log for further processing:
2019-01-29
Time (GMT)      Source                  Destination             Protocol-Info
12:42:42.112898 SOURCE_IP               xxx.xx.xx.x             DNS     Standard query 0x1de7  A hacked.badsite.net
You can try this yourself and run through other examples and snippets with the NXLog Community Edition (disclaimer again: I work for NXLog).
Important aside: unstructured vs. structured data
It's important to take a moment and consider the log data format. For example, let's consider this log message:
debug1: Failed password for invalid user amy from SOURCE_IP port SOURCE_PORT ssh2
This log contains a predefined structure, such as a metadata keyword before the colon (debug1). However, the rest of the log field is an unstructured string (Failed password for invalid user amy from SOURCE_IP port SOURCE_PORT ssh2). So, while the message is easily available in a human-readable format, it is not a format a computer can easily parse.
Unstructured event data poses limitations including difficulty of parsing, searching, and analyzing the logs. The important metadata is too often set in an unstructured data field in the form of a freeform string like the example above. Logging administrators will come across this problem at some point as they attempt to standardize/normalize log data and centralize their log sources.
Where to go next
Alongside centralizing and structuring your logs, make sure you're collecting the right log data—Sysmon, PowerShell, Windows EventLog, DNS debug, NetFlow, ETW, kernel monitoring, file integrity monitoring, database logs, external cloud logs, and so on. Also have the right tools and processes in place to collect, aggregate, and help make sense of the data.
Hopefully, this gives you a starting point to centralize log collection across diverse sources; send them to outside sources like dashboards, monitoring software, analytics software, specialized software like security information and event management (SEIM) suites; and more."
security&privacy,"Will quantum computing break security?
Do you want J. Random Hacker to be able to pretend they're your bank?
Over the past few years, a new type of computer has arrived on the block: the quantum computer. It's arguably the sixth type of computer:
Humans: Before there were artificial computers, people used, well, people. And people with this job were called ""computers.""
Mechanical analogue: These are devices such as the Antikythera mechanism, astrolabes, or slide rules.
Mechanical digital: In this category, I'd count anything that allowed discrete mathematics but didn't use electronics for the actual calculation: the abacus, Babbage's Difference Engine, etc.
Electronic analogue: Many of these were invented for military uses such as bomb sights, gun aiming, etc.
Electronic digital: I'm going to go out on a limb here and characterise Colossus as the first electronic digital computer1: these are basically what we use today for anything from mobile phones to supercomputers.
Quantum computers: These are coming and are fundamentally different from all of the previous generations.
What is quantum computing?
Quantum computing uses concepts from quantum mechanics to allow very different types of calculations from what we're used to in ""classical computing."" I'm not even going to try to explain, because I know I'd do a terrible job, so I suggest you try something like Wikipedia's definition as a starting point. What's important for our purposes is to understand that quantum computers use qubits to do calculations, and for quite a few types of mathematical algorithms—and therefore computing operations––they can solve problems much faster than classical computers.
What's ""much faster""? Much, much faster: orders of magnitude faster. A calculation that might take years or decades with a classical computer could, in certain circumstances, take seconds. Impressive, yes? And scary. Because one of the types of problems that quantum computers should be good at solving is decrypting encrypted messages, even without the keys.
This means that someone with a sufficiently powerful quantum computer should be able to read all of your current and past messages, decrypt any stored data, and maybe fake digital signatures. Is this a big thing? Yes. Do you want J. Random Hacker to be able to pretend they're your bank?2 Do you want that transaction on the blockchain where you were sold a 10 bedroom mansion in Mayfair to be ""corrected"" to be a bedsit in Weston-super-Mare?3
Some good news
This is all scary stuff, but there's good news of various types.
The first is that, in order to make any of this work at all, you need a quantum computer with a good number of qubits operating, and this is turning out to be hard.4 The general consensus is that we've got a few years before anybody has a ""big"" enough quantum computer to do serious damage to classical encryption algorithms.
The second is that, even with a sufficient number of qubits to attacks our existing algorithms, you still need even more to allow for error correction.
The third is that, although there are theoretical models to show how to attack some of our existing algorithms, actually making them work is significantly harder than you or I5 might expect. In fact, some of the attacks may turn out to be infeasible or just take more years to perfect than we worry about.
The fourth is that there are clever people out there who are designing quantum-computation-resistant algorithms (sometimes referred to as ""post-quantum algorithms"") that we can use, at least for new encryption, once they've been tested and become widely available.
All in all, in fact, there's a strong body of expert opinion that says we shouldn't be overly worried about quantum computing breaking our encryption in the next five or even 10 years.
And some bad news
It's not all rosy, however. Two issues stick out to me as areas of concern.
People are still designing and rolling out systems that don't consider the issue. If you're coming up with a system that is likely to be in use for 10 or more years or will be encrypting or signing data that must remain confidential or attributable over those sorts of periods, then you should be considering the possible impact of quantum computing on your system.
Some of the new, quantum-computing-resistant algorithms are proprietary. This means that when you and I want to start implementing systems that are designed to be quantum-computing resistant, we'll have to pay to do so. I'm a big proponent of open source, and particularly of open source cryptography, and my big worry is that we just won't be able to open source these things, and worse, that when new protocol standards are created––either de-facto or through standards bodies––they will choose proprietary algorithms that exclude the use of open source, whether on purpose, through ignorance, or because few good alternatives are available.
What to do?
Luckily, there are things you can do to address both of the issues above. The first is to think and plan when designing a system about what the impact of quantum computing might be on it. Often—very often—you won't need to implement anything explicit now (and it could be hard to, given the current state of the art), but you should at least embrace the concept of crypto-agility: designing protocols and systems so you can swap out algorithms if required.7
The second is a call to arms: Get involved in the open source movement and encourage everybody you know who has anything to do with cryptography to rally for open standards and for research into non-proprietary, quantum-computing-resistant algorithms. This is something that's very much on my to-do list, and an area where pressure and lobbying is just as important as the research itself.

1. I think it's fair to call it the first electronic, programmable computer. I know there were earlier non-programmable ones, and that some claim ENIAC, but I don't have the space or the energy to argue the case here.
2. No.
3. See 2. Don't get me wrong, by the way—I grew up near Weston-super-Mare, and it's got things going for it, but it's not Mayfair.
4. And if a quantum physicist says something's hard, then to my mind, it's hard.
5. And I'm assuming that neither of us is a quantum physicist or mathematician.6
6. I'm definitely not.
7. And not just for quantum-computing reasons: There's a good chance that some of our existing classical algorithms may just fall to other, non-quantum attacks such as new mathematical approaches. 

"
security&privacy,"The definitive guide to MongoDB security
A step-by-step survival guide to reduce your vulnerability to the next database disaster.
A sinking feeling starts to set in as you go into denial. No way this is happening to me! Then reality sets in—your company's database has been deleted and backed up to another server. Welcome to the MongoDB apocalypse!
Stories like this were all too common between December 2016 and the first quarter of 2017, when nearly 30,000 MongoDB databases were hacked, deleted, and put behind a ransom request.
While in most cases, the demands hackers make to release a database seem paltry (around USD$500), it's the irreparable damage such intrusions cause to a company's reputation and market worth that gives their executives sleepless nights.
Clients, suppliers, and partners may walk away, legal consequences may follow, employees might be fired, and the company may never recover from the experience. As many as 60% of small businesses that experience a cybersecurity attack go out of business within six months.
How do these hacks happen and what can you do to mitigate the risk? Here's what you need to know to reduce your MongoDB attack surface.
Is your database the problem? Or you?
While MongoDB 2.6.0 and above can be accessed only by local connections, the previous versions did not come with default authentication features. Consequently, they would accept requests from remote connections unless the user took the time to set up restrictions.
We are still seeing MongoDB database breaches, as demonstrated by the exposure of 445 million records in one 2018 incident. Hackers search for unsecured databases using a search engine such as ZoomEye. A simple query could result in thousands of prospective victims in a matter of minutes.
After discovering their marks, hackers simply copy the entire dataset to an offsite server and delete it from the target database. The user then gets a message telling them to send a certain number of bitcoins to an address to have their database released.
Many have assumed that MongoDB's security configuration and options are the cause of its security vulnerabilities. But the main reason for the success of these hacks is that most organizations are in the habit of using default database presets rather than configuring their installations personally.
In other words, the whole thing could have been avoided had organizations simply added a few lines of code to their database config files. Clearly, users are often at fault. While MongoDB has addressed the issue by creating stronger default settings, there are still steps you can take to ensure you never wake up to another MongoDB apocalypse.
Creating a foolproof MongoDB security policy
By now you're probably reasonably worried. When it comes to security, worried is good! Here's a step-by-step survival guide you can use to reduce your vulnerability to the next database disaster.
Authentication
Authentication is the process of verifying the identity of a client that is trying to connect with a database. MongoDB offers various methods to verify a client's identity. Challenge-based default measures include:
SCRAM-SHA-1: Salted Challenge Authentication Mechanism uses simple text-based usernames and passwords transmitted over a channel protected by transport layer security (TLS).
MongoDB-CR: Like SCRAM, MongoDB-CR verifies a username and password against an authentication database. MongoDB-CR was removed from Version 3.0, and only older iterations use it today.
Either method sends passwords encrypted, and a different hash is generated for each new session so no one can sniff them out.
MongoDB can employ external authentication protocols as well:
LDAP: Lightweight Directory Access Protocol allows users to log in using their centralized passwords. LDAP is designed to help anyone locate and access information they need in either a public or private network.
Kerberos: This is a secret key authentication protocol for server-client interactions. Using Kerberos, users can log in only once using an access ticket.
MongoDB authentication best practices
First things first: Always enable auth on a production install. For Version 3.5 onwards, this feature is enabled by default.
Enable access control and use one of MongoDB's authentication mechanisms mentioned above. If your deployment is clustered, each instance should be individually configured.
Always start by creating an administrator user. Then add additional users as needed.
Encrypt all communications between mongod and mongos instances as well as internal and external communications using TLS/SSL.
Encrypt data on each MongoDB host using filesystem, device, or physical encryption.
Run MongoDB on a trusted network only. Do not allow your database to be routable outbound to the public internet, even when inside a trusted network, and don't let it run on any more interfaces than it has to. This prevents a bad actor from having a means of moving your data from the server to another offsite location (for hardware, that is—software-based routers and static routing tables can still be modified by hackers).
Make a habit of tracking changes in both the database and data. If you are using MongoDB Enterprise, then use its auditing facility for analysis.
Authorization/role-based security
Role-based access control (RBAC) is one of MongoDB's best features. While you can find well-defined roles within MongoDB that can cover most users, custom roles can be created as well.
A role essentially determines what permissions a user has and what he/she can access. Once a user has been defined by a role, the user cannot access the system beyond it.
You can enable authorization using the --auth or the security-authorization setting. --auth enables authorization to control a user's access to a database and its resources. This feature also enforces authentication after it's enabled—it requires all clients to verify their identities before being given access.
Access-control best practices
Giving users too much access gives way to potential misuse of privileges, which is why exercising due diligence is important when assigning roles. A report by Gartner reveals that 62% of company insiders indulged in actions that gave them a second income. This is usually done by misusing company data.
Too much access is an issue that constantly figures in the top-10 lists of IT issues companies wrestle with. To ensure privileges are never misused, consider following these guidelines:
Understand each role right down to its most minute detail. The better roles are understood, the more accurately privileges can be assigned to them.
It's best to follow a principle of least privilege. Assign to users only those roles they need to get the job done. More privileges can be assigned if needed.
Create a new MongoDB user for each application/use case of the database. For example, create a user named ""webapp"" (with least privileges) to run your web application, while creating another user, ""analytics,"" (perhaps read-only) for a business analytics system. This creates isolated privileges and allows granular control of application usage of the database.
Create a resource to help users understand basic information security. Run drills to ensure employees understand the security requirements and are clear on what consequences they can face should the requirements not be met.
Revoke access of users no longer with your organization as soon as they leave.
Implement user provisioning software to manage multiple users more efficiently.
MongoDB 3.5 onwards comes with client source filtering that allows you to filter connections based on IP addresses or IP ranges. Use it to have better control over who can access an environment.
Implement user-level access control lists to grant permissions to individual users.
TLS/SSL encryption
MongoDB offers network encryption and can pass through disk encryption to help you protect your database and communications. TLS and SSL are both standard technologies that are used for encrypting network traffic.
As of MongoDB 2.6, both SSL and TLS are supported by the x.509 certificates, and clients can use the latter to authenticate their identities rather than a username and password. A minimum of 128-bit key length is required by MongoDB for all connections when using TLS/SSL.
While MongoDB can use any valid certificate, self-signed certificates are best avoided as there will be no validation of server identity, even though the communication channel will remain encrypted. In such scenarios, databases may become vulnerable to man-in-the-middle attacks.
How to configure mongod and mongos for TLS/SSL
To use TLS/SSL in your MongoDB deployment, include the following runtime options with mongod and mongos:
net.ssl.mode set to requireSSL. This setting restricts each server to use only TLS/SSL encrypted connections. You can also specify either the value allowSSL or preferSSL to set up the use of mixed TLS/SSL modes on a port. See net.ssl.mode for details.
PEMKeyfile with the .pem file that contains the TLS/SSL certificate and key.
Hardening your MongoDB database
While these steps will help your database survive malicious online activity, going the extra mile hardens your defenses even further. ""Hardening"" essentially refers to a layer-by-layer method of adding security, where each part of a database is given its own security measures.
MongoDB databases come with their own hardening features.
Configuration hardening with IP binding
For Version 3.6 onwards, MongoDB binds to localhost by default, while for versions 2.6 to 3.4, only binaries from the official MongoDB RPM and DEB packages will bind to localhost by default.
Network hardening with firewalls and VPNs
Firewalls: These can help you exert much finer control over network communications. Firewalls limit incoming traffic, particularly from untrusted sources. As ransomware and other kinds of attacks target specific ports, having well-configured firewalls is an ideal defense.    On Linux systems, administrators can use the iptables interface to access the underlying netfilter firewall. On Windows systems, the netsh command-line interface can be used to access the Windows firewall.
Virtual private networks (VPN): VPNs are ideal for connecting two endpoints over a less-than-secure communication network. Depending on features and how they are configured, VPNs allow for certificate validation and encryption protocols. As VPNs provide a secure tunnel between client and server, they can also be used along with self-generated certificates without needing to worry about man-in-the-middle attacks.
Understanding the anatomy of a ransomware attack
Now that you know how to reduce threats to your network, let's take a look at how ransomware attacks work. While they may have begun as a small-time nuisance, ransomware attacks have become an extremely sophisticated business that is costing businesses millions of dollars.
A study by Ostermon Research found that 40% of companies in the US, UK, Canada, and Germany have suffered a ransomware attack. Of them, a third lost revenue and 20% had to suspend all business activities until the situation was brought under control.
So, just how does a ransomware attack work? While they may differ in execution, most attacks share some common elements. Here's a quick rundown of how a typical ransomware attack works.
Step 1: Delivery
This is the phase where the malware enters into the targeted network. There are a number of delivery models attackers can use. Some of the most common ones are:
Email: By far the biggest target. The vast majority of ransomware attempts originate as a phishing attack that uses authentic-looking email with malicious content.
Web-based attacks: Downloading scripts and unregistered software, cross-site scripting attacks, infected ads, and social media intrusions are the choice of entry points here.
Networks: Automated search tools are used to scan a network for vulnerabilities. Once found, they may be used to deploy any manner of malware. Network-based attacks were what most MongoDB databases experienced.
Web-based applications: Webmail, shopping sites, and online forms are sometimes susceptible to malware infections.
Step 2: Infection
Provided a user unwittingly clicks on a malicious piece of code and executes it, the malware, an advanced persistent threat, will rapidly start to encrypt targeted files. In some cases (like MongoDB), it deletes them altogether.
The user is presented a ransom note that tells them to send some money (typically bitcoins, as they are untraceable) to an address then follow up through email to have their system unlocked.
Step 3: Recovery
The user now has two choices—pay up and hope the attackers will restore their system. Or, if they have been smart, simply restore their system using backup copies. You should know that only 19% of ransomware victims who pay actually get their files back. So the odds are you will be without your data and your money at the end of the day.
In any case, the business now has to clean the infected file and get its system back online. A fully updated network and endpoint security system will prove crucial to helping them save time. The steps differ from company to company, but all involve a thorough analysis of system logs, backup copies, and before/after states to find the best course of action.
Concluding thoughts
The business cost of cybercrime is pegged at around US$600 billion annually, according to a McAfee report, a trend that shows no signs of stopping. Cybercriminals are using newer, more robust techniques, so much so that today we have cybercrime-as-a-service business models. The rise of cryptocurrency and anonymous browsers such as Tor has only emboldened cybercriminals as their activities become nearly impossible to trace.
The unfortunate truth is no technology can ever be completely safe from an attack, as the human component of it will always be open to manipulation. Even a glance at how most cybercrimes work lays bare the fact that they primarily rely on human error, ignorance, or oversight as their modus operandi. If a user willingly gives system access to a malicious piece of code, then there's precious little that any security system can do to stop it.
So, while having an up-to-date security system is a good start, creating strong and well-thought-out security policies is what will ultimately help you fend off attacks and mitigate the risk to your system. Consider the following steps:
Always set your database management system (DBMS) to require a strong password.
Stay away from default users and demo databases. Since this information is public, it can be used against you.
Never use standard usernames like root, user, or app, as they are the easiest to guess.
Restrict public network access to the greatest extent possible. Only IP addresses that need to talk to your database server should be given access to it. Using a good quality VPN is highly recommended.
Set up monitoring systems to look out for high CPU usage and I/O activity. Doing so will alert you to unusual patterns that are typical with cyberattacks.
Again, follow the principle of least privilege when assigning user roles. This cannot be emphasized enough.
Conduct database audits at regular intervals. The longer your audit trail, the more secure you are.
Encrypt your backup data. Ransomware attacks have begun to infect backups as well.
Consider hiring ethical hackers to gain an outsider perspective and probe your database for weaknesses.
Always ascertain the identity of a person before accepting any communications from them. Ask yourself:
Do I know this person?
Do I absolutely have to click on the link or open that attachment?
Is the name and email of the person the same as what's in my contact list?
Was I expecting an email from them?
Keep yourself up to date on the latest goings-on in the security world.
Digital security is a shifting target that's never the same from one moment to the next. As every business has its own set of strengths and weaknesses, it is best that every organization's systems and policies be based on them.

Topics"
security&privacy,"The Evil-Twin Framework: A tool for testing WiFi security
Learn about a pen-testing tool intended to test the security of WiFi access points for all types of threats.
The increasing number of devices that connect over-the-air to the internet over-the-air and the wide availability of WiFi access points provide many opportunities for attackers to exploit users. By tricking users to connect to rogue access points, hackers gain full control over the users' network connection, which allows them to sniff and alter traffic, redirect users to malicious sites, and launch other attacks over the network..
To protect users and teach them to avoid risky online behaviors, security auditors and researchers must evaluate users' security practices and understand the reasons they connect to WiFi access points without being confident they are safe. There are a significant number of tools that can conduct WiFi audits, but no single tool can test the many different attack scenarios and none of the tools integrate well with one another.
The Evil-Twin Framework (ETF) aims to fix these problems in the WiFi auditing process by enabling auditors to examine multiple scenarios and integrate multiple tools. This article describes the framework and its functionalities, then provides some examples to show how it can be used.
The ETF architecture
The ETF framework was written in Python because the development language is very easy to read and make contributions to. In addition, many of the ETF's libraries, such as Scapy, were already developed for Python, making it easy to use them for ETF.
The ETF architecture (Figure 1) is divided into different modules that interact with each other. The framework's settings are all written in a single configuration file. The user can verify and edit the settings through the user interface via the ConfigurationManager class. Other modules can only read these settings and run according to them.
The ETF supports multiple user interfaces that interact with the framework. The current default interface is an interactive console, similar to the one on Metasploit. A graphical user interface (GUI) and a command line interface (CLI) are under development for desktop/browser use, and mobile interfaces may be an option in the future. The user can edit the settings in the configuration file using the interactive console (and eventually with the GUI). The user interface can interact with every other module that exists in the framework.
The WiFi module (AirCommunicator) was built to support a wide range of WiFi capabilities and attacks. The framework identifies three basic pillars of Wi-Fi communication: packet sniffing, custom packet injection, and access point creation. The three main WiFi communication modules are AirScanner, AirInjector, and AirHost, which are responsible for packet sniffing, packet injection, and access point creation, respectively. The three classes are wrapped inside the main WiFi module, AirCommunicator, which reads the configuration file before starting the services. Any type of WiFi attack can be built using one or more of these core features.
To enable man-in-the-middle (MITM) attacks, which are a common way to attack WiFi clients, the framework has an integrated module called ETFITM (Evil-Twin Framework-in-the-Middle). This module is responsible for the creation of a web proxy used to intercept and manipulate HTTP/HTTPS traffic.
There are many other tools that can leverage the MITM position created by the ETF. Through its extensibility, ETF can support them—and, instead of having to call them separately, you can add the tools to the framework just by extending the Spawner class. This enables a developer or security auditor to call the program with a preconfigured argument string from within the framework.
The other way to extend the framework is through plugins. There are two categories of plugins: WiFi plugins and MITM plugins. MITM plugins are scripts that can run while the MITM proxy is active. The proxy passes the HTTP(S) requests and responses through to the plugins where they can be logged or manipulated. WiFi plugins follow a more complex flow of execution but still expose a fairly simple API to contributors who wish to develop and use their own plugins. WiFi plugins can be further divided into three categories, one for each of the core WiFi communication modules.
Each of the core modules has certain events that trigger the execution of a plugin. For instance, AirScanner has three defined events to which a response can be programmed. The events usually correspond to a setup phase before the service starts running, a mid-execution phase while the service is running, and a teardown or cleanup phase after a service finishes. Since Python allows multiple inheritance, one plugin can subclass more than one plugin class.
Figure 1 above is a summary of the framework's architecture. Lines pointing away from the ConfigurationManager mean that the module reads information from it and lines pointing towards it mean that the module can write/edit configurations.
Examples of using the Evil-Twin Framework
There are a variety of ways ETF can conduct penetration testing on WiFi network security or work on end users' awareness of WiFi security. The following examples describe some of the framework's pen-testing functionalities, such as access point and client detection, WPA and WEP access point attacks, and evil twin access point creation.
These examples were devised using ETF with WiFi cards that allow WiFi traffic capture. They also utilize the following abbreviations for ETF setup commands:
APS access point SSID
APB access point BSSID
APC access point channel
CM client MAC address
In a real testing scenario, make sure to replace these abbreviations with the correct information.
Capturing a WPA 4-way handshake after a de-authentication attack
This scenario (Figure 2) takes two aspects into consideration: the de-authentication attack and the possibility of catching a 4-way WPA handshake. The scenario starts with a running WPA/WPA2-enabled access point with one connected client device (in this case, a smartphone). The goal is to de-authenticate the client with a general de-authentication attack then capture the WPA handshake once it tries to reconnect. The reconnection will be done manually immediately after being de-authenticated.
The consideration in this example is the ETF's reliability. The goal is to find out if the tools can consistently capture the WPA handshake. The scenario will be performed multiple times with each tool to check its reliability when capturing the WPA handshake.
There is more than one way to capture a WPA handshake using the ETF. One way is to use a combination of the AirScanner and AirInjector modules; another way is to just use the AirInjector. The following scenario uses a combination of both modules.
The ETF launches the AirScanner module and analyzes the IEEE 802.11 frames to find a WPA handshake. Then the AirInjector can launch a de-authentication attack to force a reconnection. The following steps must be done to accomplish this on the ETF:
Enter the AirScanner configuration mode: config airscanner
Configure the AirScanner to not hop channels: config airscanner
Set the channel to sniff the traffic on the access point channel (APC): set fixed_sniffing_channel = <APC>
Start the AirScanner module with the CredentialSniffer plugin: start airscanner with credentialsniffer
Add a target access point BSSID (APS) from the sniffed access points list: add aps where ssid = <APS>
Start the AirInjector, which by default lauches the de-authentication attack: start airinjector
This simple set of commands enables the ETF to perform an efficient and successful de-authentication attack on every test run. The ETF can also capture the WPA handshake on every test run. The following code makes it possible to observe the ETF's successful execution.
[+] Do you want to load an older session? [Y/n]: n
[+] Creating new temporary session on 02/08/2018
[+] Enter the desired session name: 
ETF[etf/aircommunicator/]::> config airscanner
ETF[etf/aircommunicator/airscanner]::> listargs 
  sniffing_interface =               wlan1; (var)
              probes =                True; (var)
             beacons =                True; (var)
        hop_channels =               false; (var)
fixed_sniffing_channel =                  11; (var)
ETF[etf/aircommunicator/airscanner]::> start airscanner with 
arpreplayer        caffelatte         credentialsniffer  packetlogger       selfishwifi        
ETF[etf/aircommunicator/airscanner]::> start airscanner with credentialsniffer
[+] Successfully added credentialsniffer plugin.
[+] Starting packet sniffer on interface 'wlan1'
[+] Set fixed channel to 11
ETF[etf/aircommunicator/airscanner]::> add aps where ssid = CrackWPA
ETF[etf/aircommunicator/airscanner]::> start airinjector
ETF[etf/aircommunicator/airscanner]::> [+] Starting deauthentication attack 
                    - 1000 bursts of 1 packets 
                    - 1 different packets
[+] Injection attacks finished executing.
[+] Starting post injection methods
[+] Post injection methods finished
[+] WPA Handshake found for client '70:3e:ac:bb:78:64' and network 'CrackWPA'
Launching an ARP replay attack and cracking a WEP network
The next scenario (Figure 3) will also focus on the Address Resolution Protocol (ARP) replay attack's efficiency and the speed of capturing the WEP data packets containing the initialization vectors (IVs). The same network may require a different number of caught IVs to be cracked, so the limit for this scenario is 50,000 IVs. If the network is cracked during the first test with less than 50,000 IVs, that number will be the new limit for the following tests on the network. The cracking tool to be used will be aircrack-ng.
The test scenario starts with an access point using WEP encryption and an offline client that knows the key—the key for testing purposes is 12345, but it can be a larger and more complex key. Once the client connects to the WEP access point, it will send out a gratuitous ARP packet; this is the packet that's meant to be captured and replayed. The test ends once the limit of packets containing IVs is captured.
ETF uses Python's Scapy library for packet sniffing and injection. To minimize known performance problems in Scapy, ETF tweaks some of its low-level libraries to significantly speed packet injection. For this specific scenario, the ETF uses tcpdump as a background process instead of Scapy for more efficient packet sniffing, while Scapy is used to identify the encrypted ARP packet.
This scenario requires the following commands and operations to be performed on the ETF:
Enter the AirScanner configuration mode: config airscanner
Configure the AirScanner to not hop channels: set hop_channels = false
Set the channel to sniff the traffic on the access point channel (APC): set fixed_sniffing_channel = <APC>
Enter the ARPReplayer plugin configuration mode: config arpreplayer
Set the target access point BSSID (APB) of the WEP network: set target_ap_bssid <APB>
Start the AirScanner module with the ARPReplayer plugin: start airscanner with arpreplayer
After executing these commands, ETF correctly identifies the encrypted ARP packet, then successfully performs an ARP replay attack, which cracks the network.
Launching a catch-all honeypot
The scenario in Figure 4 creates multiple access points with the same SSID. This technique discovers the encryption type of a network that was probed for but out of reach. By launching multiple access points with all security settings, the client will automatically connect to the one that matches the security settings of the locally cached access point information.
Using the ETF, it is possible to configure the hostapd configuration file then launch the program in the background. Hostapd supports launching multiple access points on the same wireless card by configuring virtual interfaces, and since it supports all types of security configurations, a complete catch-all honeypot can be set up. For the WEP and WPA(2)-PSK networks, a default password is used, and for the WPA(2)-EAP, an ""accept all"" policy is configured.
For this scenario, the following commands and operations must be performed on the ETF:
Enter the APLauncher configuration mode: config aplauncher
Set the desired access point SSID (APS): set ssid = <APS>
Configure the APLauncher as a catch-all honeypot: set catch_all_honeypot = true
Start the AirHost module: start airhost
With these commands, the ETF can launch a complete catch-all honeypot with all types of security configurations. ETF also automatically launches the DHCP and DNS servers that allow clients to stay connected to the internet. ETF offers a better, faster, and more complete solution to create catch-all honeypots. The following code enables the successful execution of the ETF to be observed.
[+] Do you want to load an older session? [Y/n]: n
[+] Creating ne´,cxzw temporary session on 03/08/2018
[+] Enter the desired session name: 
ETF[etf/aircommunicator/]::> config aplauncher
ETF[etf/aircommunicator/airhost/aplauncher]::> setconf ssid CatchMe
ssid = CatchMe
ETF[etf/aircommunicator/airhost/aplauncher]::> setconf catch_all_honeypot true
catch_all_honeypot = true
ETF[etf/aircommunicator/airhost/aplauncher]::> start airhost
[+] Killing already started processes and restarting network services
[+] Stopping dnsmasq and hostapd services
[+] Access Point stopped...
[+] Running airhost plugins pre_start
[+] Starting hostapd background process
[+] Starting dnsmasq service
[+] Running airhost plugins post_start
[+] Access Point launched successfully
[+] Starting dnsmasq service
Conclusions and future work
These scenarios use common and well-known attacks to help validate the ETF's capabilities for testing WiFi networks and clients. The results also validate that the framework's architecture enables new attack vectors and features to be developed on top of it while taking advantage of the platform's existing capabilities. This should accelerate development of new WiFi penetration-testing tools, since a lot of the code is already written. Furthermore, the fact that complementary WiFi technologies are all integrated in a single tool will make WiFi pen-testing simpler and more efficient.
The ETF's goal is not to replace existing tools but to complement them and offer a broader choice to security auditors when conducting WiFi pen-testing and improving user awareness.
The ETF is an open source project available on GitHub and community contributions to its development are welcomed. Following are some of the ways you can help.
One of the limitations of current WiFi pen-testing is the inability to log important events during tests. This makes reporting identified vulnerabilities both more difficult and less accurate. The framework could implement a logger that can be accessed by every class to create a pen-testing session report.
The ETF tool's capabilities cover many aspects of WiFi pen-testing. On one hand, it facilitates the phases of WiFi reconnaissance, vulnerability discovery, and attack. On the other hand, it doesn't offer a feature that facilitates the reporting phase. Adding the concept of a session and a session reporting feature, such as the logging of important events during a session, would greatly increase the value of the tool for real pen-testing scenarios.
Another valuable contribution would be extending the framework to facilitate WiFi fuzzing. The IEEE 802.11 protocol is very complex, and considering there are multiple implementations of it, both on the client and access point side, it's safe to assume these implementations contain bugs and even security flaws. These bugs could be discovered by fuzzing IEEE 802.11 protocol frames. Since Scapy allows custom packet creation and injection, a fuzzer can be implemented through it.
"
government$opensource,"Doing your civic duty one line of code at a time
Take an active role in shaping your country by contributing to open source code.
When it comes to doing our civic duty in today's technologically driven world, there is a perception that we don't care like older generations did. History teaches us that in the early 20th century's New Deal, Americans stepped up to the nation's challenges on a wide range of government-financed public works projects. Airport construction. Infrastructure improvements. Building dams, bridges, hospitals. This was more than just individuals ""pulling themselves up by their bootstraps"" but, by design, performing incredible civic duties. Quite an amazing feat when you think about it.
In our modern digital world, though, many believe we have lost that sense of civic duty. There is a debate brewing that as a society we are losing ourselves in technology instead of being inspired by it. This isn't just a single generation, but multiple generations indulging in innovative technology. But in that indulgence, are we missing opportunities to push technology forward?
Deep question, we know. Just track with us here.
Technology is more than just a service, more than just a convenience. Technology in our smartphone- and desktop-driven culture is a gateway to civic service. This is what the U.S. government offers at Code.gov. Through our website, we offer you an opportunity to work with open source software (OSS) that belongs to you—after all, you, the taxpayer, paid for the development of this code. This is one of many reasons we call what we offer on our website ""America's Code."" One option we offer at Code.gov to enable you to take on your digital civic duty is our Open Tasks section. Here you'll find tasks ranging from debugging code to developing a new capability or improvement to existing code. When you undertake a single task or an entire project (or something in between), you are not only honing your coding skills but also fulfilling—again, by design—a service to your government to help it improve and innovate.
In turn, the code you work with through Code.gov is yours. Do you need a base to build your app on? Do you or your business need the software a federal agency is implementing? We want to inspire your creativity and ingenuity. We want to hear your story concerning your experience with Code.gov and how we have inspired you, so share your stories with us through email, Twitter, LinkedIn, or the comments section below.
Perhaps we are far-removed from the ""pulling up by the bootstraps"" generation of the New Deal, but we are hardly lacking opportunities in taking an active role in shaping our country. Code.gov offers you a chance to fulfill a civic duty, this time on a digital platform, by answering challenges offered by the government's projects. We hope to see you online, to answer whatever questions you may have for us, and to stand alongside you as you fulfill your own civic duty, one line of code at a time.
Code on, and thank you.
"
OpenStack$cloud,"What's happening in the OpenStack community?
In many ways, 2018 was a transformative year for the OpenStack Foundation.
Since 2010, the OpenStack community has been building open source software to run cloud computing infrastructure. Initially, the focus was public and private clouds, but open infrastructure has been pulled into many new important use cases like telecoms, 5G, and manufacturing IoT.
As OpenStack software matured and grew in scope to support new technologies like bare metal provisioning and container infrastructure, the community widened its thinking to embrace users who deploy and run the software in addition to the developers who build the software. Questions like, ""What problems are users trying to solve?"" ""Which technologies are users trying to integrate?"" and ""What are the gaps?"" began to drive the community's thinking and decision making.
In response to those questions, the OSF reorganized its approach and created a new ""open infrastructure"" framework focused on use cases, including edge, container infrastructure, CI/CD, and private and hybrid cloud. And, for the first time, the OSF is hosting open source projects outside of the OpenStack project.
Following are three highlights from the OSF 2018 Annual Report; I encourage you to read the entire report for more detailed information about what's new.
Pilot projects
On the heels of launching Kata Containers in December 2017, the OSF launched three pilot projects in 2018—Zuul, StarlingX, and Airship—that help further our goals of taking our technology into additional relevant markets. Each project follows the tenets we consider key to the success of true open source, the Four Opens: open design, open collaboration, open development, and open source. While these efforts are still new, they have been extremely valuable in helping us learn how we should expand the scope of the OSF, as well as showing others the kind of approach we will take.
While the OpenStack project remained at the core of the team's focus, pilot projects are helping expand usage of open infrastructure across markets and already benefiting the OpenStack community. This has attracted dozens of new developers to the open infrastructure community, which will ultimately benefit the OpenStack community and users.
There is direct benefit from these contributors working upstream in OpenStack, such as through StarlingX, as well as indirect benefit from the relationships we've built with the Kubernetes community through the Kata Containers project. Airship is similarly bridging the gaps between the Kubernetes and OpenStack ecosystems. This shows users how the technologies work together. A rise in contributions to Zuul has provided the engine for OpenStack CI and keeps our development running smoothly.
Containers collaboration
In addition to investing in new pilot projects, we continued efforts to work with key adjacent projects in 2018, and we made particularly good progress with Kubernetes. OSF staffer Chris Hoge helps lead the cloud provider special interest group, where he has helped standardize how Kubernetes deployments expect to run on top of various infrastructure. This has clarified OpenStack's place in the Kubernetes ecosystem and led to valuable integration points, like having OpenStack as part of the Kubernetes release testing process.

Additionally, OpenStack Magnum was certified as a Kubernetes installer by the CNCF. Through the Kata Containers community, we have deepened these relationships into additional areas within the container ecosystem resulting in a number of companies getting involved for the first time.
Evolving events
We knew heading into 2018 that the environment around our events was changing and we needed to respond. During the year, we held two successful project team gatherings (PTGs) in Dublin and Denver, reaching capacity for both events while also including new projects and OpenStack operators. We held OpenStack Summits in Vancouver and Berlin, both experiencing increases in attendance and project diversity since Sydney in 2017, with each Summit including more than 30 open source projects. Recognizing this broader audience and the OSF's evolving strategy, the OpenStack Summit was renamed the Open Infrastructure Summit, beginning with the Denver event coming up in April.
In 2018, we boosted investment in China, onboarding a China Community Manager based in Shanghai and hosting a strategy day in Beijing with 30+ attendees from Gold and Platinum Members in China. This effort will continue in 2019 as we host our first Summit in China: the Open Infrastructure Summit Shanghai in November.
We also worked with the community in 2018 to define a new model for events to maximize participation while saving on travel and expenses for the individuals and companies who are increasingly stretched across multiple open source communities. We arrived at a plan that we will implement and iterate on in 2019 where we will collocate PTGs as standalone events adjacent to our Open Infrastructure Summits.
Looking ahead
We've seen impressive progress, but the biggest accomplishment might be in establishing a framework for the future of the foundation itself. In 2018, we advanced the open infrastructure mission by establishing OSF as an effective place to collaborate for CI/CD, container infrastructure, and edge computing, in addition to the traditional public and private cloud use cases. The open infrastructure approach opened a lot of doors in 2018, from the initial release of software from each pilot project, to live 5G demos, to engagement with hyperscale public cloud providers.
Ultimately, our value comes from the effectiveness of our communities and the software they produce. As 2019 unfolds, our community is excited to apply learnings from 2018 to the benefit of developers, users, and the commercial ecosystem across all our projects.

"
cloud,"Meet TiDB: An open source NewSQL database
5 key differences between MySQL and TiDB for scaling in the cloud
As businesses adopt cloud-native architectures, conversations will naturally lead to what we can do to make the database horizontally scalable. The answer will likely be to take a closer look at TiDB.
TiDB is an open source NewSQL database released under the Apache 2.0 License. Because it speaks the MySQL protocol, your existing applications will be able to connect to it using any MySQL connector, and most SQL functionality remains identical (joins, subqueries, transactions, etc.).
Step under the covers, however, and there are differences. If your architecture is based on MySQL with Read Replicas, you'll see things work a little bit differently with TiDB. In this post, I'll go through the top five key differences I've found between TiDB and MySQL.
1. TiDB natively distributes query execution and storage
With MySQL, it is common to scale-out via replication. Typically you will have one MySQL master with many slaves, each with a complete copy of the data. Using either application logic or technology like ProxySQL, queries are routed to the appropriate server (offloading queries from the master to slaves whenever it is safe to do so).

Scale-out replication works very well for read-heavy workloads, as the query execution can be divided between replication slaves. However, it becomes a bottleneck for write-heavy workloads, since each replica must have a full copy of the data. Another way to look at this is that MySQL Replication scales out SQL processing, but it does not scale out the storage. (By the way, this is true for traditional replication as well as newer solutions such as Galera Cluster and Group Replication.)

TiDB works a little bit differently:
Query execution is handled via a layer of TiDB servers. Scaling out SQL processing is possible by adding new TiDB servers, which is very easy to do using Kubernetes ReplicaSets. This is because TiDB servers are stateless; its TiKV storage layer is responsible for all of the data persistence.
The data for tables is automatically sharded into small chunks and distributed among TiKV servers. Three copies of each data region (the TiKV name for a shard) are kept in the TiKV cluster, but no TiKV server requires a full copy of the data. To use MySQL terminology: Each TiKV server is both a master and a slave at the same time, since for some data regions it will contain the primary copy, and for others, it will be secondary.
TiDB supports queries across data regions or, in MySQL terminology, cross-shard queries. The metadata about where the different regions are located is maintained by the Placement Driver, the management server component of any TiDB Cluster. All operations are fully ACID compliant, and an operation that modifies data across two regions uses a two-phase commit.
For MySQL users learning TiDB, a simpler explanation is the TiDB servers are like an intelligent proxy that translates SQL into batched key-value requests to be sent to TiKV. TiKV servers store your tables with range-based partitioning. The ranges automatically balance to keep each partition at 96MB (by default, but configurable), and each range can be stored on a different TiKV server. The Placement Driver server keeps track of which ranges are located where and automatically rebalances a range if it becomes too large or too hot.
This design has several advantages of scale-out replication:
It independently scales the SQL Processing and Data Storage tiers. For many workloads, you will hit one bottleneck before the other.
It incrementally scales by adding nodes (for both SQL and Data Storage).
It utilizes hardware better. To scale out MySQL to one master and four replicas, you would have five copies of the data. TiDB would use only three replicas, with hotspots automatically rebalanced via the Placement Driver.
2. TiDB's storage engine is RocksDB
MySQL's default storage engine has been InnoDB since 2010. Internally, InnoDB uses a B+tree data structure, which is similar to what traditional commercial databases use.
By contrast, TiDB uses RocksDB as the storage engine with TiKV. RocksDB has advantages for large datasets because it can compress data more effectively and insert performance does not degrade when indexes can no longer fit in memory.
Note that both MySQL and TiDB support an API that allows new storage engines to be made available. For example, Percona Server and MariaDB both support RocksDB as an option.
3. TiDB gathers metrics in Prometheus/Grafana
Tracking key metrics is an important part of maintaining database health. MySQL centralizes these fast-changing metrics in Performance Schema. Performance Schema is a set of in-memory tables that can be queried via regular SQL queries.
With TiDB, rather than retaining the metrics inside the server, a strategic choice was made to ship the information to a best-of-breed service. Prometheus+Grafana is a common technology stack among operations teams today, and the included graphs make it easy to create your own or configure thresholds for alarms.
4. TiDB handles DDL significantly better
If we ignore for a second that not all data definition language (DDL) changes in MySQL are online, a larger challenge when running a distributed MySQL system is externalizing schema changes on all nodes at the same time. Think about a scenario where you have 10 shards and add a column, but each shard takes a different length of time to complete the modification. This challenge still exists without sharding, since replicas will process DDL after a master.
TiDB implements online DDL using the protocol introduced by the Google F1 paper. In short, DDL changes are broken up into smaller transition stages so they can prevent data corruption scenarios, and the system tolerates an individual node being behind up to one DDL version at a time.
5. TiDB is designed for HTAP workloads
The MySQL team has traditionally focused its attention on optimizing performance for online transaction processing (OLTP) queries. That is, the MySQL team spends more time making simpler queries perform better instead of making all or complex queries perform better. There is nothing wrong with this approach since many applications only use simple queries.
TiDB is designed to perform well across hybrid transaction/analytical processing (HTAP) queries. This is a major selling point for those who want real-time analytics on their data because it eliminates the need for batch loads between their MySQL database and an analytics database.
Conclusion
These are my top five observations based on 15 years in the MySQL world and coming to TiDB. While many of them refer to internal differences, I recommend checking out the TiDB documentation on MySQL Compatibility. It describes some of the finer points about any differences that may affect your applications.

"
DevOps$cloud,"How to develop functions-as-a-service with Apache OpenWhisk
Write your functions in popular languages and build components using containers.
Apache OpenWhisk is a serverless, open source cloud platform that allows you to execute code in response to events at any scale. Apache OpenWhisk offers developers a straightforward programming model based on four concepts: actions, packages, triggers, and rules.
Actions are stateless code snippets that run on the Apache OpenWhisk platform. You can develop an action (or function) via JavaScript, Swift, Python, PHP, Java, or any binary-compatible executable, including Go programs and custom executables packaged as Linux containers. Actions can be explicitly invoked or run in response to an event. In either case, each run of an action results in an activation record that is identified by a unique activation ID. The input to an action and the result of an action are a dictionary of key-value pairs, where the key is a string and the value a valid JSON value.

Packages provide event feeds; anyone can create a new package for others to use.
Triggers associated with those feeds fire when an event occurs, and developers can map actions (or functions) to triggers using rules.
The following commands are used to create, update, delete, and list an action in Apache OpenWhisk:
Usage:
  wsk action [command]

Available Commands:
  create      create a new action
  update      update an existing action, or create an action if it does not exist
  invoke      invoke action
  get         get action
  delete      delete action
  list        list all actions in a namespace or actions contained in a package
Set up OpenWhisk
Let’s explore how that works in action. First, download Minishift to create a single-node local OKD (community distribution of Kubernetes that powers Red Hat OpenShift) cluster on your workstation:
$ minishift start --vm-driver=virtualbox --openshift-version=v3.10.0
Once Minishift is up and running, you can log in with admin /admin and create a new project (namespace). The project OpenWhisk on OpenShift provides the OpenShift templates required to deploy Apache OpenWhisk:
$ eval $(minishift oc-env) && eval $(minishift docker-env)
$ oc login $(minishift ip):8443 -u admin -p admin
$ oc new-project faas
$ oc project -q
$ oc process -f https://git.io/openwhisk-template | oc create -f -
Apache OpenWhisk is comprised of many components that must start up and sync with each other, and this process can take several minutes to stabilize. The following command will wait until the component pods are running:
$ while $(oc get pods -n faas controller-0 | grep 0/1 > /dev/null); do sleep 1; done
You can also watch the status with this:
$ while [ -z ""`oc logs controller-0 -n faas 2>&1 | grep ""invoker status changed""`"" ]; do sleep 1; done
Develop a simple Java Action
Maven archetype is a Maven project templating toolkit. In order to create a sample Java Action project, you won't refer to central Maven archetype, but you need to generate your own Maven archetype first as below:
$ git clone https://github.com/apache/incubator-openwhisk-devtools
$ cd incubator-openwhisk-devtools/java-action-archetype
$ mvn -DskipTests clean install
$ cd $PROJECT_HOM
Let’s now create a simple Java Action, deploy it to OpenWhisk, and finally, invoke it to see the result. Create the Java Action as shown below:
$ mvn archetype:generate \
  -DarchetypeGroupId=org.apache.openwhisk.java \
  -DarchetypeArtifactId=java-action-archetype \
  -DarchetypeVersion=1.0-SNAPSHOT \
  -DgroupId=com.example \
  -DartifactId=hello-openwhisk \
  -Dversion=1.0-SNAPSHOT \
  -DinteractiveMode=false
Next, build the Java application and deploy to OpenWhisk on Minishift locally:
$ cd hello-openwhisk
$ mvn clean package
$ wsk -i action create hello-openwhisk target/hello-openwhisk.jar --main com.example.FunctionApp
Having created the function hello-openwhisk, verify the function by invoking it:
$ wsk -i action invoke hello-openwhisk --result
As all the OpenWhisk actions are asynchronous, you need to add --result to get the result shown on the console. Successful execution of the command will show the following output:
{""greetings"":  ""Hello! Welcome to OpenWhisk"" }
Conclusion
With Apache OpenWhisk, you can write your functions in popular languages such as NodeJS, Swift, Java, Go, Scala, Python, PHP, and Ruby and build components using containers. It easily supports many deployment options, both locally and within cloud infrastructures such as Kubernetes and OpenShift.
"
DevOps$webdevelopment,"3 open source behavior-driven development tools
Having the right motivation is as important as choosing the right tool when implementing BDD.
Behavior-driven development (BDD) seems very easy. Tests are written in an easily readable format that allows for feedback from product owners, business sponsors, and developers. Those tests are living documentation for your team, so you don't need requirements. The tools are easy to use and allow you to automate your test suite. Reports are generated with each test run to document every step and show you where tests are failing.
Quick recap: Easily readable! Living documentation! Automation! Reports! What could go wrong, and why isn't everybody doing this?
Getting started with BDD
So, you're ready to jump in and can't wait to pick the right open source tool for your team. You want it to be easy to use, automate all your tests, and provide easily understandable reports for each test run. Great, let's get started!
Except, not so fast … First, what is your motivation for trying to implement BDD on your team? If the answer is simply to automate tests, go ahead and choose any of the tools listed below because chances are you're going to see minimal success in the long run.
My first effort
I manage a team of business analysts (BA) and quality assurance (QA) engineers, but my background is on the business analysis side. About a year ago, I attended a talk where a developer talked about the benefits of BDD. He said that he and his team had given it a try during their last project. That should have been the first red flag, but I didn't realize it at the time. You cannot simply choose to ""give BDD a try."" It takes planning, preparation, and forethought into what you want your team to accomplish.
However, you can try various parts of BDD without a large investment, and I eventually realized he and his team had written feature files and automated those tests using Cucumber. I also learned it was an experiment done solely by the team's developers, not the BA or QA staff, which defeats the purpose of understanding the end user's behavior.
During the talk we were encouraged to try BDD, so my test analyst and I went to our boss and said we were willing to give it a shot. And then, we didn't know what to do. We had no guidance, no plan in place, and a leadership team who just wanted to automate testing. I don't think I need to tell you how this story ended. Actually, there wasn't even an end, just a slow fizzle after a few initial attempts at writing behavioral scenarios.
A fresh start
Fast-forward a year, and I'm at a different company with a team of my own and BDD on the brain. I knew there was value there, but I also knew it went deeper than what I had initially been sold. I spent a lot of time thinking about how BDD could make a positive impact, not only on my team, but on our entire development team. Then I read Discovery: Explore Behaviour Using Examples by Gaspar Nagy and Seb Rose, and one of the first things I learned was that automation of tests is a benefit of BDD, but it should not be the main goal. No wonder we failed!
This book changed how I viewed BDD and helped me start to fill in the pieces I had been missing. We are now on the (hopefully correct!) path to implementing BDD on our team. It involves active involvement from our product owners, business analysts, and manual and automated testers and buy-in and support from our executive leadership. We have a plan in place for our approach and our measures of success.
We are still writing requirements (don't ever let anyone tell you that these scenarios can completely replace requirements!), but we are doing so with a more critical eye and evaluating where requirements and test scenarios overlap and how we can streamline the two.
I have told the team we cannot even try to automate these tests for at least two quarters, at which point we'll evaluate and determine whether we're ready to move forward or not. Our current priorities are defining our team's standard language, practicing writing given/when/then scenarios, learning the Gherkin syntax, determining where to store these tests, and investigating how to integrate these tests into our pipeline.
3 BDD tools to choose
At its core, BDD is a way to help the entire team understand the end user's actions and behaviors, which will lead to more clear requirements, tests, and ultimately higher-quality applications. Before you pick your tool, do your pre-work. Think about your motivation, and understand that while the different parts and pieces of BDD are fairly simple, integrating them into your team is more challenging and needs careful thought and planning. Also, think about where your people fit in.
Every organization has different roles, and BDD should not belong solely to developers nor test automation engineers. If you don't involve the business side, you're never going to gain the full benefit of this methodology. Once you have a strategy defined and are ready to move forward with automating your BDD scenarios, there are several open source tools for you to choose from.
Cucumber
Cucumber is probably the most recognized tool available that supports BDD. It is widely seen as a straightforward tool to learn and is easy to get started with. Cucumber relies on test scenarios that are written in plain text and follow the given/when/then format. Each scenario is an individual test. Scenarios are grouped into features, which is comparable to a test suite. Scenarios must be written in the Gherkin syntax for Cucumber to understand and execute the scenario's steps. The human-readable steps in the scenarios are tied to the step definitions in your code through the Cucumber framework. To successfully write and automate the scenarios, you need the right mix of business knowledge and technical ability. Identify the skill sets on your team to determine who will write and maintain the scenarios and who will automate them; most likely these should be managed by different roles. Because these tests are executed from the step definitions, reporting is very robust and can show you at which exact step your test failed. Cucumber works well with a variety of browser and API automation tools.
JBehave
JBehave is very similar to Cucumber. Scenarios are still written in the given/when/then format and are easily understandable by the entire team. JBehave supports Gherkin but also has its own JBehave syntax that can be used. Gherkin is more universal, but either option will work as long as you are consistent in your choice. JBehave has more configuration options than Cucumber, and its reports, although very detailed, need more configuration to get feedback from each step. JBehave is a powerful tool, but because it can be more customized, it is not quite as easy to get started with. Teams need to ask themselves exactly what features they need and whether or not learning the tool's various configurations is worth the time investment.
Gauge
Where Cucumber and JBehave are specifically designed to work with BDD, Gauge is not. If automation is your main goal (and not the entire BDD process), it is worth a look. Gauge tests are written in Markdown, which makes them easily readable. However, without a more standard format, such as the given/when/then BDD scenarios, tests can vary widely and, depending on the author, some tests will be much more digestible for business owners than others. Gauge works with multiple languages, so the automation team can leverage what they already use. Gauge also offers reporting with screenshots to show where the tests failed.
What are your needs?
Implementing BDD allows the team to test the users' behaviors. This can be done without automating any tests at all, but when done correctly, can result in a powerful, reusable test suite. As a team, you will need to identify exactly what your automation needs are and whether or not you are truly going to use BDD or if you would rather focus on automating tests that are written in plain text. Either way, open source tools are available for you to use and to help support your testing evolution."
Ansible$DevOps$SysAdmin,"A quickstart guide to Ansible
Many great tools have come and gone over the years. But none of them have made an impact as large as the one that Ansible has made in the IT automation space. From servers to networks to public cloud providers to serverless to Kubernetes… Ansible has a lot of use cases.
""The Ansible Automation for Sysadmins Guide"" was created to help celebrate Ansible's 7th birthday this year!
Whether you recently read the Ansible Getting Started doc and are just beginning your Ansible journey or have been going at it for quite some time, this book—much like the Ansible community—offers a little something for everyone.
"
Ansible$BigData,"Tips for success when getting started with Ansible
Key information for automating your data center with Ansible.
Ansible is an open source automation tool used to configure servers, install software, and perform a wide variety of IT tasks from one central location. It is a one-to-many agentless mechanism where all instructions are run from a control machine that communicates with remote clients over SSH, although other protocols are also supported.
While targeted for system administrators with privileged access who routinely perform tasks such as installing and configuring applications, Ansible can also be used by non-privileged users. For example, a database administrator using the mysql login ID could use Ansible to create databases, add users, and define access-level controls.
Let's go over a very simple example where a system administrator provisions 100 servers each day and must run a series of Bash commands on each one before handing it off to users.
This is a simple example, but should illustrate how easily commands can be specified in yaml files and executed on remote servers. In a heterogeneous environment, conditional statements can be added so that certain commands are only executed in certain servers (e.g., ""only execute yum commands in systems that are not Ubuntu or Debian"").
One important feature in Ansible is that a playbook describes a desired state in a computer system, so a playbook can be run multiple times against a server without impacting its state. If a certain task has already been implemented (e.g., ""user sysman already exists""), then Ansible simply ignores it and moves on.
Definitions
Tasks: A task is the smallest unit of work. It can be an action like ""Install a database,"" ""Install a web server,"" ""Create a firewall rule,"" or ""Copy this configuration file to that server.""
Plays: A play is made up of tasks. For example, the play: ""Prepare a database to be used by a web server"" is made up of tasks: 1) Install the database package; 2) Set a password for the database administrator; 3) Create a database; and 4) Set access to the database.
Playbook: A playbook is made up of plays. A playbook could be: ""Prepare my website with a database backend,"" and the plays would be 1) Set up the database server; and 2) Set up the web server.
Roles: Roles are used to save and organize playbooks and allow sharing and reuse of playbooks. Following the previous examples, if you need to fully configure a web server, you can use a role that others have written and shared to do just that. Since roles are highly configurable (if written correctly), they can be easily reused to suit any given deployment requirements.
Ansible Galaxy: Ansible Galaxy is an online repository where roles are uploaded so they can be shared with others. It is integrated with GitHub, so roles can be organized into Git repositories and then shared via Ansible Galaxy.
Please note this is just one way to organize the tasks that need to be executed. We could have split up the installation of the database and the web server into separate playbooks and into different roles. Most roles in Ansible Galaxy install and configure individual applications. You can see examples for installing mysql and installing httpd.
Tips for writing playbooks
The best source for learning Ansible is the official documentation site. And, as usual, online search is your friend. I recommend starting with simple tasks, like installing applications or creating users. Once you are ready, follow these guidelines:
When testing, use a small subset of servers so that your plays execute faster. If they are successful in one server, they will be successful in others.
Always do a dry run to make sure all commands are working (run with --check-mode flag).
Test as often as you need to without fear of breaking things. Tasks describe a desired state, so if a desired state is already achieved, it will simply be ignored.
Be sure all host names defined in /etc/ansible/hosts are resolvable.
Because communication to remote hosts is done using SSH, keys have to be accepted by the control machine, so either 1) exchange keys with remote hosts prior to starting; or 2) be ready to type in ""Yes"" to accept SSH key exchange requests for each remote host you want to manage.
Although you can combine tasks for different Linux distributions in one playbook, it's cleaner to write a separate playbook for each distro.
In the final analysis
Ansible is a great choice for implementing automation in your data center:
It's agentless, so it is simpler to install than other automation tools.
Instructions are in YAML (though JSON is also supported) so it's easier than writing shell scripts.
It's open source software, so contribute back to it and make it even better!
"
BigData,"Using data to get your next raise
Serverless ROI Tracker app shows the benefit of solving real-life problems while learning new skills.
When I want to learn something new, rather than just reading or watching a tutorial, I want to accomplish something at the same time. So when I decided I wanted to learn about serverless apps, I decided to work on a problem nearly every worker has: documenting their accomplishments during the annual review process.
What’s your ROI?
Most people dread the annual review process. It is difficult to remember everything you have done over the course of a year and summarize it in a concise, clear way. Engineers have an additional burden: translating technical achievements into a format that their business-focused colleagues can understand. Despite the difficulty, we all need to able to back up our accomplishments with numbers that show we’re providing a good return on the company’s investment, or “ROI.”
I like to use the business term “ROI” because it gets me thinking: Am I making more money for the company than it is spending on me? Am I making them a lot more? Do they know that? If not, I’d better tell them so they’ll give me some of that money.
The formula for documenting ROI—“success statements”—came out of a communication failure. A year after my manager and mentor was promoted, she was called into the vice president’s office and asked what she had done with the last year. While she knew she had grown her team and implemented improvements, she didn’t have numbers to back up her accomplishments. To the vice president, it looked like she had hardly done anything all year, when she felt like she had gone above and beyond. So she developed success statements.
How to write success statements
Success statements are a way to translate your accomplishments into the language everyone in business can understand: money.
There are three guidelines for writing success statements. First, they should be 140 characters or less, just like classic Twitter. You should be able to quickly scan through a list of them, like a resume. Second, they should be written at a high level and easy to understand for someone who might not be familiar with what you do. And third, they should be quantified in money or time. Money is the easiest, but since salary can be one of the biggest corporate costs, time can be translated into cost savings.
Having a consistent format makes a success statement easy to read and keeps you on message. The exact format is:
I “improved this thing” using “this method” by “this measurable amount.”
Here are two examples:
“I reduced storage costs by archiving data, saving $5.00 monthly.”
“I reduced manual testing by creating automated tests, saving 60 minutes weekly.”
Automating success
I decided to make an ROI Tracker app that helps people track their personal ROI throughout the year and create business reports, so when the time comes for performance reviews, they have the numbers to make their case.
Now you might be thinking, “I have learned this great communication method, so why do I need an app to track them?” First, each year, I might have a hundred examples of my ROI, and before I created this app, I had to manually comb through them, figure out which ones to include by date, and calculate the savings. It was a pain. Second, I’m an engineer, so my goal in life is to automate everything. Third, this was a great opportunity to learn about serverless apps.
Why serverless?
Serverless computing allows you to build and run your applications in the cloud, with the cloud provider managing and allocating machine resources dynamically depending on the application’s needs.
Before I started working with serverless apps, most of my projects met a dead end at the same place. I was great at designing APIs and functions but had no interest in DevOps. So when it came time to deploy my app, I would see about 20 different concepts I didn’t understand and give up. It wasn’t that I was incapable, it’s just not what excites me, so I did not make the time. Adopting a serverless architecture got me past this blocker.
Here are the services I used to create my serverless app:
Interface: API Gateway. It handles the routing, views, and schema validation.
Data storage: DynamoDB. It is a non-relational database. It is more like a dumping ground for data instead of a well-organized SQL database. However, it is easy to set up and use, so it’s great for small apps.
Algorithm: Lambda. This is the meat of the serverless app. A lambda is simply a function. That’s it. If you’ve written any script or tool before, you can transfer that into a lambda.
Home: AWS S3. The app needs to live somewhere outside your computer, and S3 is a convenient storage area.
Deployment: AWS Serverless Application Model (SAM). There are many ways to deploy a serverless app, but I found SAM to be the easiest. SAM uses a YAML or JSON template to tell AWS, “these are the services I want to use, please set them up.” There are simpler ways to do this (such as the Chalice library) and more robust (such as AWS CloudFormation), but I took the middle ground.
A happy ending
So, what happened to my manager? How did using success statements help her succeed? She was promoted and became responsible for multiple departments. And, after she taught her team members about success statements, they were also able to achieve great things. These include:
25% of employees in her department are promoted per year.
18% of employees receiving company awards are from her department.
20% of employees celebrating 15 years are from her department.
63% of her interns are hired full time.
I hope this inspires you to build your app, gather your data, and impress your boss! This will show all of your hard work, and your bosses will have no other option but to give all of you a raise. You’re welcome. Feel free to send me a percentage of your newly acquired wealth.
"
BigData,"Control your data with Syncthing: An open source synchronization tool
Decide how to store and share your personal information.
These days, some of our most important possessions—from pictures and videos of family and friends to financial and medical documents—are data. And even as cloud storage services are booming, so there are concerns about privacy and lack of control over our personal data. From the PRISM surveillance program to Google letting app developers scan your personal emails, the news is full of reports that should give us all pause regarding the security of our personal information.
Syncthing can help put your mind at ease. An open source peer-to-peer file synchronization tool that runs on Linux, Windows, Mac, Android, and others (sorry, no iOS), Syncthing uses its own protocol, called Block Exchange Protocol. In brief, Syncthing lets you synchronize your data across many devices without owning a server.

In this post, I will explain how to install and synchronize files between a Linux computer and an Android phone.

Linux
Syncthing is readily available for most popular distributions. Fedora 28 includes the latest version.
To install Syncthing in Fedora, you can either search for it in Software Center or execute the following command:
sudo dnf install syncthing syncthing-gtk
Once it’s installed, open it. You’ll be welcomed by an assistant to help configure Syncthing. Click Next until it asks to configure the WebUI. The safest option is to keep the option Listen on localhost. That will disable the web interface and keep unauthorized users away.
Close the dialog. Now that Syncthing is installed, it’s time to share a folder, connect a device, and start syncing. But first, let’s continue with your other client.
Android
Syncthing is available in Google Play and in F-Droid app stores.
Once the application is installed, you’ll be welcomed by a wizard. Grant Syncthing permissions to your storage. You might be asked to disable battery optimization for this application. It is safe to do so as we will optimize the app to synchronize only when plugged in and connected to a wireless network.
Click on the main menu icon and go to Settings, then Run Conditions. Tick Always run in the background, Run only when charging, and Run only on wifi. Now your Android client is ready to exchange files with your devices.
There are two important concepts to remember in Syncthing: folders and devices. Folders are what you want to share, but you must have a device to share with. Syncthing allows you to share individual folders with different devices. Devices are added by exchanging device IDs. A device ID is a unique, cryptographically secure identifier that is created when Syncthing starts for the first time.
Connecting devices
Now let’s connect your Linux machine and your Android client.
In your Linux computer, open Syncthing, click on the Settings icon and click Show ID. A QR code will show up.
In your Android mobile, open Syncthing. In the main screen, click the Devices tab and press the + symbol. In the first field, press the QR code symbol to open the QR scanner.
Point your mobile camera to the computer QR code. The Device ID field will be populated with your desktop client Device ID. Give it a friendly name and save. Because adding a device goes two ways, you now need to confirm on the computer client that you want to add the Android mobile. It might take a couple of minutes for your computer client to ask for confirmation. When it does, click Add.
In the New Device window, you can verify and configure some options about your new device, like the Device Name and Addresses. If you keep dynamic, it will try to auto-discover the device IP, but if you want to force one, you can add it in this field. If you already created a folder (more on this later), you can also share it with this new device.
Your computer and Android are now paired and ready to exchange files. (If you have more than one computer or mobile phone, simply repeat these steps.)
Sharing folders
Now that the devices you want to sync are already connected, it’s time to share a folder. You can share folders on your computer and the devices you add to that folder will get a copy.
To share a folder, go to Settings and click Add Shared Folder
In the next window, enter the information of the folder you want to share
You can use any label you want. Folder ID will be generated randomly and will be used to identify the folder between the clients. In Path, click Browse and locate the folder you want to share. If you want Syncthing to monitor the folder for changes (such as deletes, new files, etc.), click Monitor filesystem for changes.
Remember, when you share a folder, any change that happens on the other clients will be reflected on every single device. That means that if you share a folder containing pictures with other computers or mobile devices, changes in these other clients will be reflected everywhere. If this is not what you want, you can make your folder “Send Only” so it will send files to the clients, but the other clients’ changes won’t be synced.
When this is done, go to Share with Devices and select the hosts you want to sync with your folder:
All the devices you select will need to accept the share request; you will get a notification from the devices:
Just as when you shared the folder, you must configure the new shared folder
Again, here you can define any label, but the ID must match each client. In the folder option, select the destination for the folder and its files. Remember that any change done in this folder will be reflected with every device allowed in the folder.
These are the steps to connect devices and share folders with Syncthing. It might take a few minutes to start copying, depending on your network settings or if you are not on the same network.
Syncthing offers many more great features and options. Try it—and take control of your data.

"
BigData$AI$IoT$cloud$DevOps,"Better on the cloud: IoT, Big Data, and AI
Leading cloud providers offer business solutions that tap the power of IoT, Big Data, AI, and other emerging technologies.
Just a few years ago, companies used innovation and digital transformation mostly to differentiate themselves and to stay competitive. The dramatic growth in digital technologies and cloud computing over the last couple of years has since changed this mindset.
Today, organizations must be innovative and leverage the latest technologies simply to stay in business. Enterprises that implement online retail, banking, and other services aren’t considering these channels as just another route to increase their revenue. They realize that online services are fast becoming their primary revenue channel. According to data analyzed over three months for Forrester's report, The Digital Business Imperative, 84% of US banking customers used online banking for their transactions, and 43% used a mobile phone for these activities.
Organizations are working quickly to review and analyze their processes and seize opportunities for digital transformation. It is important to understand that to undergo digital transformation, companies may need to completely re-engineer their current processes to make use of technologies like the Internet of Things (IoT), Big Data analytics, artificial intelligence, and others instead of doing patchwork on existing processes to adapt to digital technologies. Furthermore, it is also important for senior IT executives to consider digital initiatives in tandem with their cloud strategy instead of treating them in isolation.

There is no doubt that the infrastructure required to support the compute power, storage, scale, and speed of these technologies is best provided by the cloud. The question you need to ask is: “Are we going to develop and host the infrastructure we need to support digital, or are we going to leverage the constantly improving robust infrastructure and services that cloud providers are offering?”
Organizations that plan for massive growth and transformation are usually the ones that invest considerably in technology to support innovative ideas. Advancements in the digital space help fuel innovation, but without a solid cloud strategy accompanied by agile development processes, ideas are likely to remain simply as ideas on paper and will take forever to get materialized into products or solutions that provide value. In today’s context, digital and cloud are almost inseparable due to the robust infrastructure, services, and tools available on the cloud to support digital initiatives. All leading cloud providers offer competing solutions and services to help organizations move forward with their digital initiatives at a rapid pace.

In my article, Transforming with the Cloud: If Not Now, When?, I touched upon some of the key areas that organizations should consider to transform their businesses with the cloud. I have structured this article in a similar fashion, focusing on the role of cloud technology to support key digital technologies such as artificial intelligence, Big Data, analytics, and the Internet of Things. The goal is to provide a high-level overview of the digital landscape and to discuss how leading cloud providers are helping enterprises with their digital initiatives.
Artificial intelligence
The digital revolution, which centers around mass production of computers and communication devices, has changed the way businesses have operated over the last several decades, resulting in constant improvements in every possible area, ranging from generating new ideas for products and services to innovative product designs to improving the customer experience. At present, the world is going through another, possibly even stronger revolution: the use of artificial intelligence to perform complex cognitive tasks to solve business problems in ways that were previously either highly complicated or extremely resource-intensive.
Most organizations deal with business propositions that yield small to medium value and require a high volume of human work, such as reviewing large numbers of documents like RFPs to understand requirements and estimate costs. AI is proving to be the best alternative to handle cases like these, which are currently handled by human beings, but the volume of work involved challenges businesses on the feasibility of continuing those without AI.
AI systems try to mimic the human brain, which uses patterns to generate perceptions, and logic to drive the structured approach of analyzing a situation from a rational perspective. AI systems process large volumes of data that come from various sources such as sensors, online applications, textual data from social media, and the like. AI processes this data using perception to analyze patterns and incorporates machine learning to utilize structured evaluation methods and rational decision-making, not only to extract pieces of meaningful information but also to assemble this information to make valuable decisions.
The cloud plays a significant role in enhancing the power of applications that incorporate AI. Almost all major players in the cloud business have developed AI services that use powerful cognitive engines to process structured data such as relational databases and unstructured data from NoSQL databases, sensors, etc., that get uploaded to the cloud. The pattern-matching algorithms and logic components built into these cognitive engines are highly sophisticated and powerful. Data and compute power are the two most critical requirements to make these engines effective. The engines predict more accurately with larger datasets. AI applications such as image recognition, video analytics, natural language processing, and speech recognition leverage machine learning using highly sophisticated neural networks that perform detection and prediction from large volumes of data. Parallel processing with the use of GPUs (graphics processing units) help these data processing and computations run faster.
Building and implementing such robust GPU-based parallel processing engines on-premise is expensive and resource-intensive. The cloud addresses this problem by providing APIs to access machine-learning services such as video analytics, speech recognition, process automation, vision detection, natural language processing, etc. Behind these APIs are complex infrastructures that combine the power of clusters of GPU-based compute engines, neural networks, and data lakes.
Big Data and analytics
With the growth of internet, cloud, and social media, we've also seen the exponential growth of data across the world. According to statistics on Big Data generation in the last five years, the average volume of data created in the world every day is about 2.3 trillion gigabytes. When the nature of the data was more structured and organized, companies relied on data warehouses and BI applications to help make important data-driven business decisions. Traditional data warehouses were built based on relational databases that could be queried using SQL; the data could be extracted, transformed, and loaded from one or more data sources via ETL jobs that ran up to several times a day.
This approach has proved to be ineffective when it comes to handling and modifying continuous streams of real-time data flowing from multiple dissimilar sources such as social media, IoT, the public web, and relational databases. Big Data analytics, which helps examine large structured and unstructured data sets, has become a major enabler for enterprises, helping them make critical business decisions by providing insight and knowledge through data mining, predictive analytics, and forecasting. The evolution of Big Data processing has led to the data lake, a centralized repository that stores structured and unstructured data as is and permits the use of various tools and approaches to address business questions.
The cloud's ability to scale vertically and horizontally makes it the ideal platform for Big Data hosting and analytics. With vertical scaling, it is possible to increase the capacity of a server by adding resources as needed by applications. Horizontal scaling allows businesses to expand hardware resources as processing requirements increase. Hadoop, which led the Big Data revolution, is designed as a distributed system so that it can scale. Parallel processing is an important part of its design, enabling the system to process multiple independent small tasks, such as serving data stores and file systems, processing streaming data, and handling queries in tandem.
Cloud-based systems offer high bandwidth, enormous amounts of memory, and scalable processing power to help Big Data applications with improved real-time processing and analysis of streaming data. The cloud is a clear choice for applications running large workloads and storing enormous volumes of data. Cloud providers offer highly scalable database services coupled with tools and services to support information management, business intelligence, and analytics.
Internet of Things
The Internet of Things (IoT) refers to the universe of connected devices such as security sensors, surveillance cameras, smartphones, wearable devices like smartwatches, and even household appliances like washing machines, refrigerators, and others that have the ability to communicate and transfer data over the network without direct human interaction. This disruptive technology has not only empowered consumers by providing greater control of household systems and appliances, but it has also enabled organizations with data that provides greater insight into critical areas of business interest, opening the doors for innovative products, solutions, and new business opportunities.
IoT will impact all industries, from manufacturing to logistics to healthcare—we are not far from a time when nearly everything in the world is connected. Business adoption of IoT is already growing exponentially as the number of connected devices continues to increase. According to a recent Gartner study, businesses could already be using as many as 3.1 billion IoT devices today, and by 2020 this number will likely increase to about 7.6 billion.
IoT plays an essential role in optimizing production, managing supply chains, tracking assets, making financial decisions, and improving the customer experience. A Forbes report on IoT’s impact on business points out a few areas where the use of IoT is becoming more dominant.
Digital transformation and IoT adoption in the fitness industry has been remarkable. Wearable devices with built-in sensors can constantly collect data on physical activities such as distance traveled and calories burned and can monitor sleep patterns to provide detailed analyses and insight for continuous healthcare. IoT applications are designed to use data from connected devices, and the sophisticated tools available in the cloud let you visualize, explore, and build complex analytics. For complex IoT applications that use several devices, it is important to understand the state of the devices and to frequently communicate with the application components that leverage those devices. It is also essential to ensure secure identity and access between devices and applications.
As with any revolution in technology, IoT also faces challenges. With more devices connected to the internet, the volume of data generated is immense. This puts significant pressure on the internet and creates a need for an infrastructure that can transmit and store this data more efficiently. With the number of connected devices constantly increasing, there is a push to create ""edge"" devices that are intelligent enough to perform some processing and send the results to servers instead of sending massive amounts of data to central servers for processing.
For example, surveillance cameras typically send videos to a central recording device, which records only when it detects motion. Imagine the network impact if hundreds of high-definition surveillance cameras sent video feeds constantly to this central server. It's also worth noting that as the physical distance between the devices and the server increases, network transmission latency increases. With edge computing, video cameras are smart enough to sense motion, and they send videos to the cloud-based central recording system only when they detect motion. This drastically reduces the volume of data that is transmitted over the network and increases efficiency. Cloud and IoT complement each other: Connected devices generate huge amounts of data and the cloud provides the infrastructure to store, process, and analyze the data.
Taking everything into account…
Technology is certainly a major driver in any business today, and companies that struggle to integrate technology effectively will eventually find it difficult to succeed or even sustain their place in the marketplace. Connected devices, social media, and massive volumes of several forms of structured and unstructured data feeds have paved the way to further leverage technology and transform businesses. Digital transformation, which includes artificial intelligence, Big Data analytics, Internet of Things, and other emerging technologies, is fast becoming a key requirement for organizations to be innovative and remain competitive. Digital has opened doors for better analytics and decision-making, leading enterprises to explore, analyze, and obtain new insights and ideas to grow their business.
Offering a mature collection of services, tools, and security, the cloud is an ideal platform for any business developing a digital transformation strategy. All leading cloud providers offer comprehensive and competitive solutions, tools, and services to address even the most complex digital transformation initiatives for their customers. Furthermore, the horizontal and vertical scaling of infrastructure offered by the cloud makes it highly suitable for the robust compute requirements digital technologies demand.
Artificial intelligence encompasses machine learning, speech recognition, speech synthesis, image recognition, image comparison, video analytics, and many other applications. It is widely implemented by enterprises to improve customer experience, implement chatbots, develop training, and more. Cloud providers offer APIs that application developers can utilize to build intelligent AI-enabled applications.
Big Data analytics takes data analytics to a different dimension, providing the means to gather better insights from structured data such as feeds from conventional relational databases and unstructured data such as data streams from social media. It enables businesses to better understand customer perceptions, for example, by analyzing comments and conversations on social media. Implementing Big Data and its associated tools on-premise requires a massive investment of time and money. A cloud platform provides all the tools and the elastic compute power needed to help businesses focus on the benefits of analytics rather than worrying about implementation, maintenance, and support.
Internet of Things (IoT) connects devices to the internet and provides individuals and organizations greater control of their homes, lives, and businesses. A rain sensor-based watering system, for example, monitors humidity, automatically waters fields, promotes better growth of crops, and cuts costs. Edge computing provides better processing ability to devices, eliminating the need to transfer large volumes of data to central servers for processing. There are plenty of solutions and tools offered by leading cloud providers for edge computing and IoT in general.
Ignoring the digital revolution will be disastrous to organizations as well as individuals. Humans and businesses generate trillions of gigabytes of data every day. Whether we realize it or not, digital is transforming our lives. It is essential for organizations to develop and implement a digital strategy that works with cloud initiatives and tools.
"
Leadership&manegment$DigitalTransformation,"Digital transformation's people problem
Digital transformation involves technologies and humans. Unfortunately, we tend to ignore the latter when leading change.
""Digital transformation,"" while not a new term, is still a burning issue for business leaders. As digital technologies become more sophisticated, as organizations continue to adapt to shifting market needs, and as social trends demand new ways of doing work, we are faced with an ever-increasing need to transform our organization and cultural norms.
Arguably, the greatest chasm we see in our organizational work today is the actual transformation before, during, or after the implementation of a digital technology—because technology invariably crosses through and impacts people, processes, and culture. What are we transforming from? What are we transforming into? These are ""people issues"" as much as they are ""technology issues,"" but we too rarely acknowledge this.
Operating our organizations on open principles promises to spark new ways of thinking that can help us address this gap. Over the course of this three-part series, we’ll take a look at how the principle foundations of open play a major role in addressing the ""people part"" of digital transformation—and closing that gap before and during
The impact of digital transformation
The meaning of the term ""digital transformation"" has changed considerably in the last decade. For example, if you look at where organizations were in 2007, you’d watch them grapple with the first iPhone. Focus here was more on search engines, data mining, and methods of virtual collaboration.
A decade later in 2017, however, we’re investing in artificial intelligence, machine learning, and the Internet of Things. Our technologies have matured—but our organizational and cultural structures have not kept pace with them.
Value Co-creation In The Organizations of the Future, a recent research report from Aalto University, states that digital transformation has created opportunities to revolutionize and change existing business models, socioeconomic structures, legal and policy measures, organizational patterns, and cultural barriers. But we can only realize this potential if we address both the technological and the organizational aspects of digital transformation.
Four critical areas of digital transformation
Let’s examine four crucial elements involved in any digital transformation effort:
change management
the needs of the ecosystem
processes
silos
Any organization must address these four elements in advance of (ideally) or in conjunction with the implementation of a new technology if that organization is going to realize success and sustainability.
Change management
Change initiatives have been clocking a 70% failure rate since the 1970s. This tells us that both our models and our approach to doing business need to change.

Change management is really about a human element: the attitudes and behaviors of internal and external stakeholders, as well and the ecosystems within which they operate. So a more inclusive (that is, ""people-focused"") approach to change management promises to increases our success rates. Embedding open organization principles in our change models can ultimately help us create empowered people who adapt quickly to business needs. Transparency and inclusivity allow for opportunities for open discourse and feedback from often underrepresented voices. Collaboration and co-creation allow us to have fresh perspective and more innovative solutions. We begin to identify gaps and roadblocks at a faster rate—which ultimately creates better processes, policies, and solutions.
Bottom line: Open principles reduce your headaches and the cost of change.
Needs of an ecosystem
What is your ecosystem? It is the living, breathing network of people and organizational frameworks that interconnect to form the system in which your organization operates. The actors in any ecosystem—employees, partners, external stakeholders, customers, vendors, etc.—are mutually dependent on each other for our business health, growth, and success. And much like the ecosystems we see in nature, the poor health of one component affects the others over time.
This mutual interdependency crucially affects everyone’s success and happiness, and organizational leaders must understand the needs of each actor’s role so they can meet the goal the organization wants to meet by introducing a new technology in the first place. Implementing advanced and emerging technologies often create an even greater need for leaders to evaluate and align to the needs of an ecosystem in advance of implementation. Every business has different needs. There is not a one-size solution. Yet the commonality here is that ecosystems are interdependent on all actors within.
Know your goals and understand how implementations will affect the whole of an ecosystem so you can create an appropriate and inclusive strategy. It may require a plan for competency training, a re-organization of work and process, creating new initiatives, and other ways of meeting the needs of your people.
Processes
Any time we use the word ""transformation,"" we must really understand what that word means. Transformation is radical change. And any change, no matter its scale, requires us to review our processes to evaluate what needs adjusting to fit within the new change.

Processes beyond the technological, however, are those we often overlook. Yet digital transformation affects the front of the house as much as the back of the house. When we add something new to, or even adjust, our workflow, we frequently forget to think about the layers upon layers of processes that already exist. We inadvertently create workflow redundancies that slow down productivity and can even cross through other connected areas of an ecosystem.
When you change a business model or implement a streamlined approach with technology, your leadership must look at what processes or policies are touched. These redundancies (or additional touches from departments) in business processes costs you in lost productivity, delayed responses, and more.
Silos
Organizations that leverage open principles remove barriers to collaboration and co-creation. They create processes that are inclusive and that allow for cross-training for their employees. Knowledge sharing and communication is transparent, accessible, and useful.
During periods of intense digital transformation, breaking down the silos that exist in our organizations is imperative to business (and, more broadly, societal) success. Leaders can leverage new ways of thinking by seeking feedback from underrepresented voices and diversity of thought in your project teams. They can create opportunities for people to learn and become more versatile in their competencies.
Your opportunity to disrupt
Any digital transformation effort presents organizational leaders with opportunities to decrease fears surrounding new technologies (like AI or automation). By taking them seriously, we can disrupt old ways of thinking and doing to create new opportunities for our people and customers. Being on the edge involves being responsible for developing the right type of change for our future, one that that goes well beyond technological matters.
"
Linux$Docker$security&privacy,"Bringing new security features to Docker
In the first of this series on Docker security, I wrote ""containers do not contain."" In this second article, I'll cover why and what we're doing about it.

Docker, Red Hat, and the open source community are working together to make Docker more secure. When I look at security containers, I am looking to protect the host from the processes within the container, and I'm also looking to protect containers from each other. With Docker we are using the layered security approach, which is ""the practice of combining multiple mitigating security controls to protect resources and data.""
Basically, we want to put in as many security barriers as possible to prevent a break out. If a privileged process can break out of one containment mechanism, we want to block them with the next. With Docker, we want to take advantage of as many security mechanisms of Linux as possible.
Luckily, with Red Hat Enterprise Linux (RHEL) 7, we get a plethora of security features.
File System Protections
Read-only mount points
Some Linux kernel file systems have to be mounted in a container environment or processes would fail to run. Fortunately, most of these filesystems can be mounted as ""read-only"".  Most apps should never need to write to these file systems.
Docker mounts these file systems into the container as ""read-only"" mount points.
. /sys 
. /proc/sys 
. /proc/sysrq-trigger 
. /proc/irq 
. /proc/bus
By mounting these file systems as read-only, privileged container processes cannot write to them. They cannot effect the host system. Of course, we also block the ability of the privileged container processes from remounting the file systems as read/write. We block the ability to mount any file systems at all within the container. I will explain how we block mounts when we get to capabilities.
Copy-on-write file systems
Docker uses copy-on-write file systems. This means containers can use the same file system image as the base for the container. When a container writes content to the image, it gets written to a container specific file system. This prevents one container from seeing the changes of another container even if they wrote to the same file system image. Just as important, one container can not change the image content to effect the processes in another container.
Capabilities
Linux capabilities are explained well on their main page:
For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero). Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process's credentials (usually: effective UID, effective GID, and supplementary group list). Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled. Capabilities are a per-thread attribute.
Removing capabilities can cause applications to break, which means we have a balancing act in Docker between functionality, usability and security. Here is the current list of capabilities that Docker uses: chown, dac_override, fowner, kill, setgid, setuid, setpcap, net_bind_service, net_raw, sys_chroot, mknod, setfcap, and audit_write.
It is continuously argued back and forth which capabilities should be allowed or denied by default. Docker allows customers to manipulate default list with the command line options for Docker run.
Capabilities removed
Docker removes several of these capabilities including the following:

Lets look closer at the last couple in the table. By removing CAP_NET_ADMIN for a container, the container processes cannot modify the systems network, meaning assigning IP addresses to network devices, setting up routing rules, modifying iptables.
All networking is setup by the Docker daemon before the container starts. You can manage the containers network interface from outside the container but not inside.
CAP_SYS_ADMIN is special capability. I believe it is the kernel catchall capability. When kernel engineers design new features into the kernel, they are supposed to pick the capability that best matches what the feature allows. Or, they were supposed to create a new capability. Problem is, there were originally only 32 capability slots available. When in doubt the kernel engineer would just fall back to using CAP_SYS_ADMIN. Here is the list of things CAP_SYS_ADMIN allows according to /usr/include/linux/capability.

The two most important features that removing CAP_SYS_ADMIN from containers does is stops processes from executing the mount syscall or modifying namespaces. You don't want to allow your container processes to mount random file systems or to remount the read-only file systems.

--cap-add --cap-drop

Docker run also has a feature where you can adjust the capabilities that your container requires. This means you can remove capabilities your container does not need. For example, if your container does not need setuid and setgid you can remove this access by executing:
docker run --cap-drop setuid --cap-drop setgid -ti rhel7 /bin/sh
You can even remove all capabilities or add them all:
docker run --cap-add all --cap-drop sys-admin -ti rhel7 /bin/sh
This command would add all capabilities except sys-admin.
Namespaces
Some of the namespaces that Docker sets up for processes to run in also provide some security.
PID namespace
The PID namespace hides all processes that are running on a system except those that are running in your current container. If you can't see the other processes, it makes it harder to attack the process. You can't easily strace or ptrace them. And, killing the pid 1 of a process namespace will automatically kill all of the processes within a container, which means an admin can easily stop a container.
Network namespace
The network namespace can be used to implement security. The admin can setup the network of a container with routing rules and iptables such that the processes within the container can only use certain networks. I can imagine people setting up three filtering containers:
One only allowed to communicate on the public Internet.
One only allowed to communicate on the private Intranet.
One connected to the other two containers, relaying messages back and forth between between the containers, but blocking inappropriate content.
cgroups
One type of attack on a system could be described as a Denial Of Service. This is where a process or group of processes use all of the resources on a system, preventing other processes from executing. cgroups can be used to mitigate this by controlling the amount of resources any Docker container can use. For example the CPU cgroup can be setup such that an administrator can still login to a system where a Docker container is trying to dominate the CPU and kill it. New cgroups are being worked on to help control processes from using too many resources like open files or number of processes. Docker will take advantage of these cgroups as they become available.
Device cgroups
Docker takes advantage of the special cgroups that allows you to specify which device nodes can be used within the container. It blocks the processes from creating and using device nodes that could be used to attack the host.
Device nodes allow processes to change the configuration of the kernel. Controlling which devices nodes are available controlls what a process is able to do on the host system.
The following device nodes are created in the container by default. 
/dev/console,/dev/null,/dev/zero,/dev/full,/dev/tty*,/dev/urandom,/dev/random,/dev/fuse
The Docker images are also mounted with nodev, which means that even if a device node was pre-created in the image, it could not be used by processes within the container to talk to the kernel.
Note: The creation of device nodes could also be blocked by removing the CAP_MKNOD capability. Docker has chosen to not do this, in order to allow processes to create a limited set of device nodes. In the futures section, I will mention the --opt command line option, which I would like to use to remove this capability.
AppArmor
Apparmor is available for Docker containers on systems that support it. But I use RHEL and Fedora, which do not support AppArmor, so you will have to investigate this security mechanism elsewhere. (Besides, I use SELinux as you well know.)
SELinux
First, a little about SELinux:
SELinux is a LABELING system
Every process has a LABEL
Every file, directory, and system object has a LABEL
Policy rules control access between labeled processes and labeled objects
The kernel enforces the rules
SELinux implements a Mandatory Access Control system. This means the owners of an object have no control or discretion over the access to an object. The kernel enforces Mandatory Access Controls. I have described how SELinux enforcement works in the visual guide to SELinux policy enforcement (and subsequent, SELinux Coloring Book).
I will use some of the cartoons from that article to describe how we use SELinux to control the access allowed to Docker container processes. We use two type of SELinux enforcement for Docker containers.
Type enforcement
The default type we use for running Docker containers is svirt_lxc_net_t. All container processes run with this type.
All content within the container is labeled with the svirt_sandbox_file_t type.
svirt_lxc_net_t is allowed to manage any content labeled with svirt_sandbox_file_t.
svirt_lxc_net_t is also able to read/execute most labels under /usr on the host.
Processes running witht he svirt_lxc_net_t are not allowed to open/write to any other labels on the system. It is not allowed to read any default labels in /var, /root, /home etc.
Basically, we want to allow the processes to read/execute system content, but we want to not allow it to use any ""data"" on the system unless it is in the container, by default.
Problem
If all container processes are run with svirt_lxc_net_t and all content is labeled with svirt_sandbox_file_t, wouldn't container processes be allowed to attack processes running in other containers and content owned by other containers? 
This is where Multi Category Security enforcement comes in, described below.
Alternate Types
Notice that we used ""net"" in the type label. We use this to indicate that this type can use full networking. I am working on a patch to Docker to allow users to specify alternate types to be used for containers. For example, you would be able to specify something like:

docker run -ti --security-opt label:type:lxc_nonet_t rhel7 /bin/sh

Then the processes inside of the container would not be allowed to use any network ports. Similarly, we could easily write an Apache policy that would only allow the container to listen on Apache ports, but not allowed to connect out on any ports. Using this type of policy you could prevent your container from becoming a spam bot even if it was cracked, and the hacker got control of the apache process within the container.
Multi Category Security enforcement
Multi Category Security is based on Multi Level Security (MLS). MCS takes advantage of the last component of the SELinux label the MLS Field. MCS enforcement protects containers from each other. When containers are launched the Docker daemon picks a random MCS label, for example s0:c1,c2, to assign to the container. The Docker daemon labels all of the content in the container with this MCS label. When the daemon launches the container process it tells the kernel to label the processes with the same MCS label. The kernel only allows container processes to read/write their own content as long as the process MCS label matches the filesystem content MCS label. The kernel blocks container processes from read/writing content labeled with a different MCS label.
A hacked container process is prevented from attacking different containers. The Docker daemon is responsible for guaranteeing that no containers use the same MCS label. This is a video I made to show what would happen if an OpenShift container was able to get on root a system. The same basic policy is used to confine a Docker container.
As I mentioned above I am working on a patch to Docker to allow the specification of different SELinux content. I will be allowing administrators to specify the label of the container.

docker run --ti --rm --label-opt level:TopSecret rhel7 /bin/sh

This would allow people to start running containers in an Multi Level Security (MLS) environment, which could be useful for environments that require MLS.
SELinux gotchas
File system support
SELinux currently will only work with the device mapper back end. SELinux does not work with BTFS. BTRFS does not support context mount labeling yet, which prevents SELinux from relabeling all content when the container starts via the mount command. Kernel engineers are working on a fix for this and potentially Overlayfs if it gets merged into the container.
Volume mounts

Since Type Enforcement only allows container processes to read/write svirt_sandbox_file_t in containers, volume mounts can be a problem.  A volume mount is just a bind mount of a directory into the container, there for the labels of the directory do not change.  In order to allow the container processes to read/write the content you need to change the type label to svirt_sandbox_file_t.
Volume mounts /var/lib/myapp
chcon -Rt svirt_sandbox_file_t /var/lib/myapp

I have written a patch for docker that has not been merged upstream to set these labels automatically. With the patch you docker would relabel the volume to either a private label ""Z"" or a shared label ""z"" automatically.

docker run -v /var/lib/myapp:/var/lib/myapp:Z ...
docker run -v /var/lib/myapp:/var/lib/myapp:z ...
Hopefully this will get merged soon.

Bottom line
We have added lots of security mechanisms to make Docker containers more secure than running applications on bare metal, but you still need to maintain good security practices as I talked about in the first article on this subject.
Only run applications from a trusted source
Run applications on a enterprise quality host
Install updates regularly
Drop privileges as quickly as possible
Run as non-root whenever possible
Watch your logs
setenforce 1
"
RaspberryPI,"How to build a WiFi picture frame with a Raspberry Pi
DIY a digital photo frame that streams photos from the cloud.
Digital picture frames are really nice because they let you enjoy your photos without having to print them out. Plus, adding and removing digital files is a lot easier than opening a traditional frame and swapping the picture inside when you want to display a new photo. Even so, it's still a bit of overhead to remove your SD card, USB stick, or other storage from a digital picture frame, plug it into your computer, and copy new pictures onto it.
An easier option is a digital picture frame that gets its pictures over WiFi, for example from a cloud service. Here's how to make one.
Gather your materials
Old TFT LCD screen
HDMI-to-DVI cable (as the TFT screen supports DVI)
Raspberry Pi 3
Micro SD card
Raspberry Pi power supply
Keyboard
Mouse (optional)
Connect the Raspberry Pi to the display using the cable and attach the power supply.
Install Raspbian

Download and flash Raspbian to the Micro SD card by following these directions. Plug the Micro SD card into the Raspberry Pi, boot it up, and configure your WiFi. My first action after a new Raspbian installation is usually running sudo raspi-config. There I change the hostname (e.g., to picframe) in Network Options and enable SSH to work remotely on the Raspberry Pi in Interfacing Options. Connect to the Raspberry Pi using (for example) ssh pi@picframe.

Build and install the cloud client
I use Nextcloud to synchronize my pictures, but you could use NFS, Dropbox, or whatever else fits your needs to upload pictures to the frame.
If you use Nextcloud, get a client for Raspbian by following these instructions. This is handy for placing new pictures on your picture frame and will give you the client application you may be familiar with on a desktop PC. When connecting the client application to your Nextcloud server, make sure to select only the folder where you'll store the images you want to be displayed on the picture frame.
Set up the slideshow
The easiest way I've found to set up the slideshow is with a lightweight slideshow project built for exactly this purpose. There are some alternatives, like configuring a screensaver, but this application appears to be the simplest to set up.
On your Raspberry Pi, download the binaries from the latest release, unpack them, and move them to an executable folder:
wget https://github.com/NautiluX/slide/releases/download/v0.9.0/slide_pi_stretch_0.9.0.tar.gz
tar xf slide_pi_stretch_0.9.0.tar.gz
mv slide_0.9.0/slide /usr/local/bin/
Install the dependencies:
sudo apt install libexif12 qt5-default
Run the slideshow by executing the command below (don't forget to modify the path to your images). If you access your Raspberry Pi via SSH, set the DISPLAY variable to start the slideshow on the display attached to the Raspberry Pi.
DISPLAY=:0.0 slide -p /home/pi/nextcloud/picframe
Autostart the slideshow
To autostart the slideshow on Raspbian Stretch, create the following folder and add an autostart file to it:
mkdir -p /home/pi/.config/lxsession/LXDE/
vi /home/pi/.config/lxsession/LXDE/autostart
Insert the following commands to autostart your slideshow. The slide command can be adjusted to your needs:
@xset s noblank 
@xset s off 
@xset -dpms
@slide -p -t 60 -o 200 -p /home/pi/nextcloud/picframe
Disable screen blanking, which the Raspberry Pi normally does after 10 minutes, by editing the following file:
vi /etc/lightdm/lightdm.conf
and adding these two lines to the end:
[SeatDefaults]
xserver-command=X -s 0 -dpms
Configure a power-on schedule
You can schedule your picture frame to turn on and off at specific times by using two simple cronjobs. For example, say you want it to turn on automatically at 7 am and turn off at 11 pm. Run crontab -e and insert the following two lines.
0 23 * * * /opt/vc/bin/tvservice -o
0 7 * * * /opt/vc/bin/tvservice -p && sudo systemctl restart display-manager
Note that this won't turn the Raspberry Pi power's on and off; it will just turn off HDMI, which will turn the screen off. The first line will power off HDMI at 11 pm. The second line will bring the display back up and restart the display manager at 7 am.
Add a final touch
By following these simple steps, you can create your own WiFi picture frame. If you want to give it a nicer look, build a wooden frame for the display.

"
RaspberryPi,"How to boot up a new Raspberry Pi
Learn how to install a Linux operating system, in the third article in our guide to getting started with Raspberry Pi.
If you've been following along in this series, you've chosen and bought your Raspberry Pi board and peripherals and now you're ready to start using it. Here, in the third article, let's look at what you need to do to boot it up.
Unlike your laptop, desktop, smartphone, or tablet, the Raspberry Pi doesn't come with built-in storage. Instead, it uses a Micro SD card to store the operating system and your files. The great thing about this is it gives you the flexibility to carry your files (even if you don't have your Raspberry Pi with you). The downside is it may also increase the risk of losing or damaging the card—and thus losing your files. Just protect your Micro SD card, and you should be fine.
You should also know that SD cards aren't as fast as mechanical or solid state drives, so booting, reading, and writing from your Pi will not be as speedy as you would expect from other devices.
How to install Raspbian

The first thing you need to do when you get a new Raspberry Pi is to install its operating system on a Micro SD card. Even though there are other operating systems (both Linux- and non-Linux-based) available for the Raspberry Pi, this series focuses on Raspbian, Raspberry Pi's official Linux version.
The easiest way to install Raspbian is with NOOBS, which stands for ""New Out Of Box Software."" Raspberry Pi offers great documentation for NOOBS, so I won't repeat the installation instructions here.
NOOBS gives you the choice of installing the following operating systems:
Raspbian
LibreELEC
OSMC
Recalbox
Lakka
RISC OS
Screenly OSE
Windows 10 IoT Core
TLXOS
Again, Raspbian is the operating system we'll use in this series, so go ahead, grab your Micro SD and follow the NOOBS documentation to install it. I'll meet you in the fourth article in this series, where we'll look at how to use Linux, including some of the main commands you'll need to know."
RaspberryPi,"Turn a Raspberry Pi 3B+ into a PriTunl VPN
PriTunl is a VPN solution for small businesses and individuals who want private access to their network.
PriTunl is a fantastic VPN terminator solution that's perfect for small businesses and individuals who want a quick and simple way to access their network privately. It's open source, and the basic free version is more than enough to get you started and cover most simple use cases. There is also a paid enterprise version with advanced features like Active Directory integration.
Special considerations on Raspberry Pi 3B+
PriTunl is generally simple to install, but this project—turning a Raspberry Pi 3B+ into a PriTunl VPN appliance—adds some complexity. For one thing, PriTunl is supplied only as AMD64 and i386 binaries, but the 3B+ uses ARM architecture. This means you must compile your own binaries from source. That's nothing to be afraid of; it can be as simple as copying and pasting a few commands and watching the terminal for a short while.
Another problem: PriTunl seems to require 64-bit architecture. I found this out when I got errors when I tried to compile PriTunl on my Raspberry Pi's 32-bit operating system. Fortunately, Ubuntu's beta version of 18.04 for ARM64 boots on the Raspberry Pi 3B+.
Also, the Raspberry Pi 3B+ uses a different bootloader from other Raspberry Pi models. This required a complicated set of steps to install and update the necessary files to get a Raspberry Pi 3B+ to boot.
Installing PriTunl
You can overcome these problems by installing a 64-bit operating system on the Raspberry Pi 3B+ before installing PriTunl. I'll assume you have basic knowledge of how to get around the Linux command line and a Raspberry Pi.
Start by opening a terminal and downloading the Ubuntu 18.04 ARM64 beta release by entering:
$ wget http://cdimage.ubuntu.com/releases/18.04/beta/ubuntu-18.04-beta-preinstalled-server-arm64+raspi3.img.xz
Unpack the download:
$ xz -d ubuntu-18.04-beta-preinstalled-server-arm64+raspi3.xz
Insert the SD card you'll use with your Raspberry Pi into your desktop or laptop computer. Your computer will assign the SD card a drive letter—something like /dev/sda or /dev/sdb. Enter the dmesg command and examine the last lines of the output to find out the card's drive assignment.
Be VERY CAREFUL with the next step! I can't stress that enough; if you get the drive assignment wrong, you could destroy your system.
Write the image to your SD card with the following command, changing <DRIVE> to your SD card's drive assignment (obtained in the previous step):
$ dd if=ubuntu-18.04-beta-preinstalled-server-arm64+raspi3.img of=<DRIVE> bs=8M
After it finishes, insert the SD card into your Pi and power it up. Make sure the Pi is connected to your network, then log in with username/password combination ubuntu/ubuntu.
Enter the following commands on your Pi to install a few things to prepare to compile PriTunl:
$ sudo apt-get -y install build-essential git bzr python python-dev python-pip net-tools openvpn bridge-utils psmisc golang-go libffi-dev mongodb
There are a few changes from the standard PriTunl source installation instructions on GitHub. Make sure you are logged into your Pi and sudo to root:
$ sudo su -
This should leave you in root's home directory. To install PriTunl version 1.29.1914.98, enter (per GitHub):
export VERSION=1.29.1914.98
tee -a ~/.bashrc << EOF
export GOPATH=\$HOME/go
export PATH=/usr/local/go/bin:\$PATH
EOF
source ~/.bashrc
mkdir pritunl && cd pritunl
go get -u github.com/pritunl/pritunl-dns
go get -u github.com/pritunl/pritunl-web
sudo ln -s ~/go/bin/pritunl-dns /usr/bin/pritunl-dns
sudo ln -s ~/go/bin/pritunl-web /usr/bin/pritunl-web
wget https://github.com/pritunl/pritunl/archive/$VERSION.tar.gz
tar -xf $VERSION.tar.gz
cd pritunl-$VERSION
python2 setup.py build
pip install -r requirements.txt
python2 setup.py install --prefix=/usr/local
Now the MongoDB and PriTunl systemd units should be ready to start up. Assuming you're still logged in as root, enter:
systemctl daemon-reload
systemctl start mongodb pritunl
systemctl enable mongodb pritunl
"
Python,"Introducing kids to computational thinking with Python
Coding program gives low-income students the skills, confidence, and knowledge to break free from economic and societal disadvantages.
When the Parkman Branch of the Detroit Public Library was flooded with bored children taking up all the computers during summer break, the library saw it not as a problem, rather an opportunity. They started a coding club, the Parkman Coders, led by Qumisha Goss, a librarian who is leveraging the power of Python to introduce disadvantaged children to computational thinking.
When she started the Parkman Coders program about four years ago, ""Q"" (as she is known) didn't know much about coding. Since then, she's become a specialist in library instruction and technology and a certified Raspberry Pi instructor.
The program began by using Scratch, but the students got bored with the block coding interface, which they regarded as ""baby stuff."" She says, ""I knew we need to make a change to something that was still beginner friendly, but that would be more challenging for them to continue to hold their attention."" At this point, she started teaching them Python.
Q first saw Python while playing a game with dungeons and skeleton monsters on Code.org. She began to learn Python by reading books like Python Programming: An Introduction to Computer Science and Python for Kids. She also recommends Automate the Boring Stuff with Python and Lauren Ipsum: A Story about Computer Science and Other Improbable Things.
Setting up a Raspberry Pi makerspace
Q decided to use Raspberry Pi computers to avoid the possibility that the students might be able to hack into the library system's computers, which weren't arranged in a way conducive to a makerspace anyway. The Pi's affordability, plus its flexibility and the included free software, lent more credibility to her decision.
While the coder program was the library's effort keep the peace and create a learning space that would engage the children, it quickly grew so popular that it ran out of space, computers, and adequate electrical outlets in a building built in 1921. They started with 10 Raspberry Pi computers shared among 20 children, but the library obtained funding from individuals, companies including Microsoft, the 4H, and the Detroit Public Library Foundation to get more equipment and expand the program.
Currently, about 40 children participate in each session and they have enough Raspberry Pi's for one device per child and some to give away. Many of the Parkman Coders come from low socio-economic backgrounds and don't have a computer at home, so the library provides them with donated Chromebooks.
Q says, ""when kids demonstrate that they have a good understanding of how to use a Raspberry Pi or a Microbit and have been coming to programs regularly, we give them equipment to take home with them. This process is very challenging, however, because [they may not] have internet access at home [or] all the peripheral things they need like monitors, keyboards, and mice.""
Learning life skills and breaking stereotypes with Python
Q says, ""I believe that the mainstays of learning computer science are learning critical thinking and problem-solving skills. My hope is that these lessons will stay with the kids as they grow and pursue futures in whatever field they choose. In addition, I'm hoping to inspire some pride in creatorship. It's a very powerful feeling to know 'I made this thing,' and once they've had these successes early, I hope they will approach new challenges with zeal.""
She also says, ""in learning to program, you have to learn to be hyper-vigilant about spelling and capitalization, and for some of our kids, reading is an issue. To make sure that the program is inclusive, we spell aloud during our lessons, and we encourage kids to speak up if they don't know a word or can't spell it correctly.""
Q also tries to give extra attention to children who need it. She says, ""if I recognize that someone has a more severe problem, we try to get them paired with a tutor at our library outside of program time, but still allow them to come to the program. We want to help them without discouraging them from participating.""
Most importantly, the Parkman Coders program seeks to help every child realize that each has a unique skill set and abilities. Most of the children are African-American and half are girls. Q says, ""we live in a world where we grow up with societal stigmas that frequently limit our own belief of what we can accomplish."" She believes that children need a nonjudgmental space where ""they can try new things, mess up, and discover.""
The environment Q and the Parkman Coders program creates helps the participants break away from economic and societal disadvantages. She says that the secret sauce is to ""make sure you have a welcoming space so anyone can come and that your space is forgiving and understanding. Let people come as they are, and be prepared to teach and to learn; when people feel comfortable and engaged, they want to stay.""

"
Python$PyCascades,"7 steps for hunting down Python code bugs
Learn some tricks to minimize the time you spend tracking down the reasons your code fails.
It is 3 pm on a Friday afternoon. Why? Because it is always 3 pm on a Friday when things go down. You get a notification that a customer has found a bug in your software. After you get over your initial disbelief, you contact DevOps to find out what is happening with the logs for your app, because you remember receiving a notification that they were being moved.
Turns out they are somewhere you can't get to, but they are in the process of being moved to a web application—so you will have this nifty application for searching and reading them, but of course, it is not finished yet. It should be up in a couple of days. I know, totally unrealistic situation, right? Unfortunately not; it seems logs or log messages often come up missing at just the wrong time. Before we track down the bug, a public service announcement: Check your logs to make sure they are where you think they are and logging what you think they should log, regularly. Amazing how these things just change when you aren't looking.
OK, so you found the logs or tried the call, and indeed, the customer has found a bug. Maybe you even think you know where the bug is.
You immediately open the file you think might be the problem and start poking around.
1. Don't touch your code yet
Go ahead and look at it, maybe even come up with a hypothesis. But before you start mucking about in the code, take that call that creates the bug and turn it into a test. This will be an integration test because although you may have suspicions, you do not yet know exactly where the problem is.
Make sure this test fails. This is important because sometimes the test you make doesn't mimic the broken call; this is especially true if you are using a web or other framework that can obfuscate the tests. Many things may be stored in variables, and it is unfortunately not always obvious, just by looking at the test, what call you are making in the test. I'm not going to say that I have created a test that passed when I was trying to imitate a broken call, but, well, I have, and I don't think that is particularly unusual. Learn from my mistakes.
2. Write a failing test
Now that you have a failing test or maybe a test with an error, it is time to troubleshoot. But before you do that, let's do a review of the stack, as this makes troubleshooting easier.
The stack consists of all of the tasks you have started but not finished. So, if you are baking a cake and adding the flour to the batter, then your stack would be:
Make cake
Make batter
Add flour

You have started making your cake, you have started making the batter, and you are adding the flour. Greasing the pan is not on the list since you already finished that, and making the frosting is not on the list because you have not started that.

If you are fuzzy on the stack, I highly recommend playing around on Python Tutor, where you can watch the stack as you execute lines of code.
Now, if something goes wrong with your Python program, the interpreter helpfully prints out the stack for you. This means that whatever the program was doing at the moment it became apparent that something went wrong is on the bottom.
3. Always check the bottom of the stack first
Not only is the bottom of the stack where you can see which error occurred, but often the last line of the stack is where you can find the issue. If the bottom doesn't help, and your code has not been linted in a while, it is amazing how helpful it can be to run. I recommend pylint or flake8. More often than not, it points right to where there is an error that I have been overlooking.
If the error is something that seems obscure, your next move might just be to Google it. You will have better luck if you don't include information that is relevant only to your code, like the name of variables, files, etc. If you are using Python 3 (which you should be), it's helpful to include the 3 in the search; otherwise, Python 2 solutions tend to dominate the top.
Once upon a time, developers had to troubleshoot without the benefit of a search engine. This was a dark time. Take advantage of all the tools available to you.
Unfortunately, sometimes the problem occurred earlier and only became apparent during the line executed on the bottom of the stack. Think about how forgetting to add the baking powder becomes obvious when the cake doesn't rise.
It is time to look up the stack. Chances are quite good that the problem is in your code, and not Python core or even third-party packages, so scan the stack looking for lines in your code first. Plus it is usually much easier to put a breakpoint in your own code. Stick the breakpoint in your code a little further up the stack and look around to see if things look like they should.
""But Maria,"" I hear you say, ""this is all helpful if I have a stack trace, but I just have a failing test. Where do I start?""
Pdb, the Python Debugger.
Find a place in your code where you know this call should hit. You should be able to find at least one place. Stick a pdb break in there.
A digression
Why not a print statement? I used to depend on print statements. They still come in handy sometimes. But once I started working with complicated code bases, and especially ones making network calls, print just became too slow. I ended up with print statements all over the place, I lost track of where they were and why, and it just got complicated. But there is a more important reason to mostly use pdb. Let's say you put a print statement in and discover that something is wrong—and must have gone wrong earlier. But looking at the function where you put the print statement, you have no idea how you got there. Looking at code is a great way to see where you are going, but it is terrible for learning where you've been. And yes, I have done a grep of my code base looking for where a function is called, but this can get tedious and doesn't narrow it down much with a popular function. Pdb can be very helpful.
You follow my advice, and put in a pdb break and run your test. And it whooshes on by and fails again, with no break at all. Leave your breakpoint in, and run a test already in your test suite that does something very similar to the broken test. If you have a decent test suite, you should be able to find a test that is hitting the same code you think your failed test should hit. Run that test, and when it gets to your breakpoint, do a w and look at the stack. If you have no idea by looking at the stack how/where the other call may have gone haywire, then go about halfway up the stack, find some code that belongs to you, and put a breakpoint in that file, one line above the one in the stack trace. Try again with the new test. Keep going back and forth, moving up the stack to figure out where your call went off the rails. If you get all the way up to the top of the trace without hitting a breakpoint, then congratulations, you have found the issue: Your app was spelled wrong. No experience here, nope, none at all.
4. Change things
If you still feel lost, try making a new test where you vary something slightly. Can you get the new test to work? What is different? What is the same? Try changing something else. Once you have your test, and maybe additional tests in place, it is safe to start changing things in the code to see if you can narrow down the problem. Remember to start troubleshooting with a fresh commit so you can easily back out changes that do not help. (This is a reference to version control, if you aren't using version control, it will change your life. Well, maybe it will just make coding easier. See ""A Visual Guide to Version Control"" for a nice introduction.)
5. Take a break
In all seriousness, when it stops feeling like a fun challenge or game and starts becoming really frustrating, your best course of action is to walk away from the problem. Take a break. I highly recommend going for a walk and trying to think about something else.
6. Write everything down
When you come back, if you aren't suddenly inspired to try something, write down any information you have about the problem. This should include:
Exactly the call that is causing the problem
Exactly what happened, including any error messages or related log messages
Exactly what you were expecting to happen
What you have done so far to find the problem and any clues that you have discovered while troubleshooting
Sometimes this is a lot of information, but trust me, it is really annoying trying to pry information out of someone piecemeal. Try to be concise, but complete.
7. Ask for help
I often find that just writing down all the information triggers a thought about something I have not tried yet. Sometimes, of course, I realize what the problem is immediately after hitting the submit button. At any rate, if you still have not thought of anything after writing everything down, try sending an email to someone. First, try colleagues or other people involved in your project, then move on to project email lists. Don't be afraid to ask for help. Most people are kind and helpful, and I have found that to be especially true in the Python community. 

"
Go,"The 7 stages of becoming a Go programmer
Whether you're new to Go or a seasoned Gopher, you may recognize these steps on the path to Go enlightenment.
One day at work, we were discussing the Go programming language in our work chatroom. At one point, I commented on a co-worker's slide, saying something along the lines of:
""I think that's like stage three in the seven stages of becoming a Go programmer.""
Naturally, my co-workers wanted to know the rest of the stages, so I briefly outlined them. Here, expanded with more context, are the seven stages of becoming a Go programmer; see if you can see yourself on this pathway.
Stage 1: You believe you can make Go do object oriented programming

After your initial run on A Tour of Go, you start thinking ""Now, how can I make this language behave more like an object oriented language...?"" After all, you are used to that stuff. You want to make robust code. You want polymorphism.

""There has to be a way!"" You say, and you find struct embedding. It allows you to cleverly delegate methods from the enclosing object to the embedded object without having to duplicate code. Great!
Of course, this is not true. Struct embedding only allows you to delegate method calls. Even if it looks like you are doing polymorphic method dispatch, the relationship is not IS-A. It's HAS-A, so the receiver of the method call is not the enclosing object: The receiver is always the embedded object to which the method call was delegated to.
You DO NOT do object oriented programming in Go. Period.
Stage 2: You believe goroutines will solve all of your problems
You were lured to Go by the promise that it will allow you to easily run concurrent code, which, it does via goroutines! All you need to do is use the go keyword, and you can make pretty much any function or method call run concurrently. It is only natural, then, that you want to maximize your code's efficiency by making as much code to run in parallel. And because you hid this fact by making your function calls to create goroutines automatically, the caller does not even need to be aware of this.
Yeah, so it might make your code a bit more complicated but look, now everything runs concurrently!
Go allows you to create millions of goroutines without sacrificing much efficiency, but you really should not use goroutines just because you can. Concurrent code is harder to maintain and debug than code that just flows in a single thread. I mean, have you given serious thought to whether your shared objects are really synchronized properly when accessed from multiple goroutines at once? Are you sure the order of execution is absolutely correct? Have you really checked if those goroutines actually exit when they're no longer needed?
Goroutines are best used only when they are necessary, and unless your requirements dictate that you do everything in memory or some such, you should never abandon the use of good old multi-process models.
And finally, try NOT to spawn goroutines behind your users' back, especially if you are writing a library. Explicit calls to use go calls usually give the user more flexibility and power.
Goroutines take you only so far. Use them only when it really makes sense.
Stage 3: You believe that instead of object oriented programming, interfaces will solve all of your problems
After being disillusioned that you cannot make your objects behave in a polymorphic manner, you suddenly realize the capabilities offered by interfaces. Interfaces allow you to describe APIs; there has to be a way to use this to write more robust code.
So now when you write libraries, you define interfaces for everything. You only export the interfaces and have private structs so that encapsulation is perrrrfect. It also should give you more flexibility on switching the underlying implementation, because now you have successfully decoupled the API from its implementation.
Interfaces do give you a lot of power, but it's not an end-all solution. It still does not provide true polymorphism in the sense of object oriented programming. You are also limited by the fact that interfaces can only define the API, and you cannot associate any data with it.
Also, although there are legitimate cases where exporting only interfaces instead of concrete structs make sense, it really should not be your default mode of operation. Interfaces are best when they are small-ish (as opposed to describing an entire list of methods defined for an object). Also, if you are not careful, you will have to either write a lot of extra code to fulfill the interface or to write code that requires a lot of type-assertions.
To make most out of using interfaces, you should only use them when you want to make certain types interchangeable.
Stage 4: You believe channels will solve all of your problems
After you spent a lot of time pondering how to bend Go to work your way, you are now looking for that missing piece that will make everything work your way. ""Wait, there's still channels!""
Channels implicitly handle concurrent access correctly. You believe you should be able to solve many of the obstacles so far by cleverly using channels to handle synchronization, returning values (a la future/promises), and flow control with select statements with various channels.
Again, channels are extremely useful, but they are only as useful as their initial purpose, which is to provide a primitive to pass values between goroutines.
I'm sure you will find great many Go idioms using channels: for timeouts, blocking I/O, synchronization tricks, etc. But again, because channels are concurency constructs, abusing them will lead to more complicated, hard to debug code.
Stage 5: You now believe Go is not as powerful as people claim it to be
""Why?! Why is it so painful to write Go code? It doesn't allow me to write code the way I have been doing.""
You are frustrated. No polymorphism. Concurrency is hard. Channels don't solve your problems. You don't even understand why Go exists. You feel like you've been stripped of all the nice tools and constructs that other languages provide.
You believe that more powerful tools to express abstract ideas are absolutely necessary. Go just doesn't cut it.
Go is decidedly opinionated. I come from a Perl background, and for a while I could not believe how limiting Go was. So yes, I understand if you become frustrated.
But is it because the language is really limiting, or is it because you were trying to make the language work the way you thought it should, without considering what the language authors intended you to do?
Stage 6: You realize that stages 1-5 were all just your imagination
At some point, you grudgingly decide to write Go code according to how most of the standard library is written. You also give up trying to be clever, and start writing straightforward code.
Then it comes to you: You just didn't want to accept the Go way.
Everything starts to make sense.
In all seriousness, learning Go does require some unlearning. I had to unlearn object oriented programming a bit, as well as embrace the fact that no matter how many useful tools the language may give you, writing concurrent code is just too hard for mere mortals. I also had to unlearn to use exceptions.
I did not check with Go's authors, so this is just my opinion, but I believe that the focus of the language is to make it harder for developers to write complex code. It gives you enough to write code that performs complex tasks, but by taking away certain key tools, the code you end up writing is simpler and harder to mess up.
Once I decided to accept the functionalities and constructs as they are, writing Go code became much easier, and definitely much more fun.
Stage 7: You are now at peace
You have accepted the Go way. You now write everything, including what you would've normally used Perl/Ruby/Python for, in Go. You realize if err != nil no longer bothers you. You only use goroutines and channels when you must.
You are one with the Gopher. You feel its glorious chi, and cry when you realize its mercy for allowing your to write code in such a majestic language.
Congratulations. Now you are a Go programmer.

Even though this sounds a bit tongue-in-cheek, these were actual issues that I felt or experienced when I was getting accustomed to Go. Maybe you agree, maybe you don't, but stage 6 was actually like that for me. I finally gave up trying to make Go work like I wanted it to and decided to write how Go was telling me to. It sounds silly, but after that, things really started to make sense.
Here's to hoping that new Gophers waste less time wondering how to bend the language and getting frustrated. "
Go,"4 tips for learning Golang
Arriving in Golang land: A senior developer's journey.
In the summer of 2014...
IBM: ""We need you to go figure out this Docker thing.""
Me: ""OK.""
IBM: ""Start contributing and just get involved.""
Me: ""OK."" (internal voice): ""This is written in Go. What's that?"" (Googles) ""Oh, a programming language. I've learned a few of those in my career. Can't be that hard.""
My university's freshman programming class was taught using VAX assembler. In data structures class, we used Pascal—loaded via diskette on tired, old PCs in the library's computer center. In one upper-level course, I had a professor that loved to show all examples in ADA. I learned a bit of C via playing with various Unix utilities' source code on our Sun workstations. At IBM we used C—and some x86 assembler—for the OS/2 source code, and we heavily used C++'s object-oriented features for a joint project with Apple. I learned shell scripting soon after, starting with csh, but moving to Bash after finding Linux in the mid-'90s. I was thrust into learning m4 (arguably more of a macro-processor than a programming language) while working on the just-in-time (JIT) compiler in IBM's custom JVM code when porting it to Linux in the late ‘90s.
Fast-forward 20 years... I'd never been nervous about learning a new programming language. But Go felt different. I was going to contribute publicly, upstream on GitHub, visible to anyone interested enough to look! I didn't want to be the laughingstock, the Go newbie as a 40-something-year-old senior developer! We all know that programmer pride that doesn't like to get bruised, no matter your experience level.

My early investigations revealed that Go seemed more committed to its ""idiomatic-ness"" than some languages. It wasn't just about getting the code to compile; I needed to be able to write code ""the Go way.""
Now that I'm four years and several hundred pull requests into my personal Go journey, I don't claim to be an expert, but I do feel a lot more comfortable contributing and writing Go code than I did in 2014. So, how do you teach an old guy new tricks—or at least a new programming language? Here are four steps that were valuable in my own journey to Golang land.
1. Don't skip the fundamentals
While you might be able to get by with copying code and hunting and pecking your way through early learnings (who has time to read the manual?!?), Go has a very readable language spec that was clearly written to be read and understood, even if you don't have a master's in language or compiler theory. Given that Go made some unique decisions about the order of the parameter:type constructs and has interesting language features like channels and goroutines, it is important to get grounded in these new concepts. Reading this document alongside Effective Go, another great resource from the Golang creators, will give you a huge boost in readiness to use the language effectively and properly.
2. Learn from the best
There are many valuable resources for digging in and taking your Go knowledge to the next level. All the talks from any recent GopherCon can be found online, like this exhaustive list from GopherCon US in 2018. Talks range in expertise and skill level, but you can easily find something you didn't know about Go by watching the talks. Francesc Campoy created a Go programming video series called JustForFunc that has an ever-increasing number of episodes to expand your Go knowledge and understanding. A quick search on ""Golang"" reveals many other video and online resources for those who want to learn more.
Want to look at code? Many of the most popular cloud-native projects on GitHub are written in Go: Docker/Moby, Kubernetes, Istio, containerd, CoreDNS, and many others. Language purists might rate some projects better than others regarding idiomatic-ness, but these are all good starting points to see how large codebases are using Go in highly active projects.
3. Use good language tools
You will learn quickly about the value of gofmt. One of the beautiful aspects of Go is that there is no arguing about code formatting guidelines per project—gofmt is built into the language runtime, and it formats Go code according to a set of stable, well-understood language rules. I don't know of any Golang-based project that doesn't insist on checking with gofmt for pull requests as part of continuous integration.
Beyond the wide, valuable array of useful tools built directly into the runtime/SDK, I strongly recommend using an editor or IDE with good Golang support features. Since I find myself much more often at a command line, I rely on Vim plus the great vim-go plugin. I also like what Microsoft has offered with VS Code, especially with its Go language plugins.
Looking for a debugger? The Delve project has been improving and maturing and is a strong contender for doing gdb-like debugging on Go binaries.
4. Jump in and write some Go!
You'll never get better at writing Go unless you start trying. Find a project that has some ""help needed"" issues flagged and make a contribution. If you are already using an open source project written in Go, find out if there are some bugs that have beginner-level solutions and make your first pull request. As with most things in life, the only real way to improve is through practice, so get going.
And, as it turns out, apparently you can teach an old senior developer new tricks—or languages at least."
Go,"Locks versus channels in concurrent Go
Compare two ways to share information with goroutines, one using synchronized shared memory and the other using channels.
Go has popularized the mantra don't communicate by sharing memory; share memory by communicating. The language does have the traditional mutex (mutual exclusion construct) to coordinate access to shared memory, but it favors the use of channels to share information among goroutines.
In this article, a short look at goroutines, threads, and race conditions sets the scene for a look at two Go programs. In the first program, goroutines communicate through synchronized shared memory, and the second uses channels for the same purpose. The code is available from my website in a .zip file with a README.
Threads and race conditions
A thread is a sequence of executable instructions, and threads within the same process share an address space: Every thread in a multi-threaded process has read/write access to the very same memory locations. A memory-based race condition occurs if two or more threads (at least one of which performs a write operation) have uncoordinated access to the same memory location.
Consider this depiction of integer variable n, whose value is 777, and two threads trying to alter its contents:
        n = n + 10  +-----+  n = n - 10
Thread1------------>| 777 |<------------Thread2
                    +-----+
                       n
On a multiprocessor machine, the two threads could execute literally at the same time. The impact on variable n is then indeterminate. It's critical to note that each attempted update consists of two machine-level operations: an arithmetic operation on n's current value (either adding or subtracting 10), and a subsequent assignment operation that sets n to a new value (either 787 or 767).
The paired operations executed in the two threads could interleave in various inappropriate ways. Consider the following scenario, with each numbered item as a single operation at the machine level. For simplicity, assume that each operation takes one tick of the system clock:
Thread1 does the addition to compute 787, which is saved in a temporary location (on the stack or in a CPU register).
Thread2 does the subtraction to compute 767, also saved in a temporary location.
Thread2 performs the assignment; the value of n is now 767.
Thread1 performs the assignment; the value of n is now 787.
By coming in last, Thread1 has won the race against Thread2. It's clear that improper interleaving has occurred. Thread1 performs an addition operation, is delayed for two ticks, and then performs the assignment. By contrast, Thread2 performs the subtraction and subsequent assignment operations without interruption. The fix is clear: The arithmetic and assignment operations should occur as if they were a single, atomic operation. A construct such as a mutex provides the required fix, and Go has the mutex.
Go programs are typically multi-threaded, although the threading occurs beneath the surface. On the surface are goroutines. A goroutine is a green thread—a thread under the Go runtime control. By contrast, a native thread is directly under OS control. But goroutines multiplex onto native threads that the OS schedules, which means that memory-based race conditions are possible in Go. The first of two sample programs illustrates this.
MiserSpendthrift1
The MiserSpendthrift1 program simulates shared access to a bank account. In addition to main, there are two other goroutines:
The miser goroutine repeatedly adds to the balance, one currency unit at a time.
The spendthrift goroutine repeatedly subtracts from the balance, also one currency unit at a time.
The number of times each goroutine performs its operation depends on a command-line argument, which should be large enough to be interesting (e.g., 100,000 to a few million). The account balance is initialized to zero and should wind up as zero because the deposits and withdrawals are for the same amount and are the same in number.
Example 1. Using a mutex to coordinate access to shared memory
package main

import (
   ""os""
   ""fmt""
   ""runtime""
   ""strconv""
   ""sync""
)

var accountBalance = 0    // balance for shared bank account
var mutex = &sync.Mutex{} // mutual-exclusion lock

// critical-section code with explicit locking/unlocking
func updateBalance(amt int) {
   mutex.Lock()
   accountBalance += amt  // two operations: update and assignment
   mutex.Unlock()
}

func reportAndExit(msg string) {
   fmt.Println(msg)
   os.Exit(-1) // all 1s in binary
}

func main() {
   if len(os.Args) < 2 {
      reportAndExit(""\nUsage: go ms1.go <number of updates per thread>"")
   }
   iterations, err := strconv.Atoi(os.Args[1])
   if err != nil {
      reportAndExit(""Bad command-line argument: "" + os.Args[1]);
   }

   var wg sync.WaitGroup  // wait group to ensure goroutine coordination

   // miser increments the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()  // invoke Done on the WaitGroup when finished
      for i := 0; i < iterations ; i++ {
         updateBalance(1)
         runtime.Gosched()  // yield to another goroutine
      }
   }()

   // spendthrift decrements the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()
      for i := 0; i < iterations; i++ {
         updateBalance(-1)
         runtime.Gosched()  // be nice--yield
      }
   }()

   wg.Wait()  // await completion of miser and spendthrift
   fmt.Println(""Final balance: "", accountBalance)  // confirm final balance is zero
}
Flow-of-control in the MiserSpendthrift1 program (see above) can be described as follows:
The program begins by attempting to read and verify a command-line argument that specifies how many times (e.g., a million) the miser and the spendthrift should each update the account balance.
The main goroutine starts two others with the call: go func() { // either the miser or the spendthrift  The first of the two started goroutines represents the miser, and the second represents the spendthrift.
The program uses a sync.WaitGroup to ensure that the main goroutine does not print the final balance until the miser and the spendthrift goroutines have finished their work and terminated.
The MiserSpendthrift1 program declares two global variables, one an integer variable to represent the shared bank account and the other a mutex to ensure coordinated goroutine access to the account:
var accountBalance = 0    // balance for shared bank account
var mutex = &sync.Mutex{} // mutual-exclusion lock
The mutex code occurs in the updateBalance function to safeguard a critical section, which is a code segment that must be executed in single-threaded fashion for the program to behave correctly:
func updateBalance(amt int) {
   mutex.Lock()
   accountBalance += amt  // critical section
   mutex.Unlock()
}
The critical section is the statement between the Lock() and Unlock() calls. Although a single line in Go source code, this statement involves two distinct operations: an arithmetic operation followed by an assignment. These two operations must be executed together, one thread at a time, which the mutex code ensures. With the locking code in place, the accountBalance is zero at the end because the number of additions by 1 and subtractions by 1 is the same.
If the mutex code is removed, then the final value of the accountBalance is unpredictable. On two sample runs with the lock code removed, the final balance was 249 on the first run and -87 on the second, thereby confirming that a memory-based race condition occurred.
The mutex code's behavior deserves a closer look:
To execute the critical section code, a goroutine must first grab the lock by executing the mutex.Lock() call. If the lock is held already, then the goroutine blocks until the lock becomes available; otherwise, the goroutine executes the mutex-protected critical section.
The mutex guarantees mutual exclusion in that only one goroutine at a time can execute the locked code segment. The mutex ensures single-threaded execution of the critical section: the arithmetic operation followed by the assignment operation.
The call to Unlock() releases a held lock so that some goroutine (perhaps the one that just released the lock) can grab the lock anew.
In the MiserSpendthrift1 program, three goroutines (the miser, the spendthrift, and main) communicate through the shared memory location named accountBalance. A mutex coordinates access to this variable by the miser and the spendthrift, and main tries to access the variable only after both the miser and the spendthrift have terminated. Even with a relatively large command-line argument (e.g., five to 10 million), the program runs relatively fast and yields the expected final value of zero for the accountBalance.
The package sync/atomic has functions such as AddInt32 with synchronization baked in. For example, if the accountBalance type were changed from int to int32, then the updateBalance function could be simplified as follows:
func updateBalance(amt int32) {          // argument must be int32 as well
   atomic.AddInt32(&accountBalance, amt) // no explicit locking required
}
The MiserSpendthrift1 program uses explicit locking to highlight the critical-section code and to underscore the need for thread synchronization to prevent a race condition. In a production-grade example, a critical section might comprise several lines of source code. In any case, a critical section should be as short as possible to keep the program as concurrent as possible.
MiserSpendthrift2
The MiserSpendthrift2 program again has a global variable accountBalance initialized to zero, and again there are miser and spendthrift goroutines contending to update the balance. However, this program does not use a mutex to prevent a race condition. Instead, there is now a banker goroutine that accesses the accountBalance in response to requests from the miser and the spendthrift. These two goroutines no longer update the accountBalance directly. Here is a sketch of the architecture:
                  requests         updates
miser/spendthrift---------->banker--------->balance
This architecture, with support from a thread-safe Go channel to serialize requests from the miser and the spendthrift, prevents a race condition on the accountBalance.
Example 2. Using a thread-safe channel to coordinate access to shared memory
package main

import (
   ""os""
   ""fmt""
   ""runtime""
   ""strconv""
   ""sync""
)

type bankOp struct { // bank operation: deposit or withdraw
   howMuch int       // amount
   confirm chan int  // confirmation channel
}

var accountBalance = 0          // shared account
var bankRequests chan *bankOp   // channel to banker

func updateBalance(amt int) int {
   update := &bankOp{howMuch: amt, confirm: make(chan int)}
   bankRequests <- update
   newBalance := <-update.confirm
   return newBalance
}

// For now a no-op, but could save balance to a file with a timestamp.
func logBalance(current int) { }

func reportAndExit(msg string) {
   fmt.Println(msg)
   os.Exit(-1) // all 1s in binary
}

func main() {
   if len(os.Args) < 2 {
      reportAndExit(""\nUsage: go ms1.go <number of updates per thread>"")
   }
   iterations, err := strconv.Atoi(os.Args[1])
   if err != nil {
      reportAndExit(""Bad command-line argument: "" + os.Args[1]);
   }

   bankRequests = make(chan *bankOp, 8) // 8 is channel buffer size

   var wg sync.WaitGroup
   // The banker: handles all requests for deposits and withdrawals through a channel.
   go func() {
      for {
         /* The select construct is non-blocking:
            -- if there's something to read from a channel, do so
            -- otherwise, fall through to the next case, if any */
         select {
         case request := <-bankRequests:
            accountBalance += request.howMuch   // update account
            request.confirm <- accountBalance   // confirm with current balance
         }
      }
   }()

   // miser increments the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()  // invoke Done on the WaitGroup when finished
      for i := 0; i < iterations ; i++ {
         newBalance := updateBalance(1)
         logBalance(newBalance)
         runtime.Gosched()  // yield to another goroutine
      }
   }()

   // spendthrift decrements the balance
   wg.Add(1)           // increment WaitGroup counter
   go func() {
      defer wg.Done()
      for i := 0; i < iterations; i++ {
         newBalance := updateBalance(-1)
         logBalance(newBalance)
         runtime.Gosched()  // be nice--yield
      }
   }()

   wg.Wait()  // await completion of miser and spendthrift
   fmt.Println(""Final balance: "", accountBalance) // confirm the balance is zero
}
The changes in the MiserSpendthrift2 program can be summarized as follows. There is a BankOp structure:
type bankOp struct { // bank operation: deposit or withdraw
   howMuch int       // amount
   confirm chan int  // confirmation channel
}
that the miser and the spendthrift goroutines use to make update requests. The howMuch field is the update amount, either 1 (miser) or -1 (spendthrift). The confirm field is a channel that the banker goroutine uses in responding to a miser or a spendthrift request; this channel carries the new balance back to the requester as confirmation. For efficiency, the address of a bankOp structure, rather than a copy of it, is sent over the bankRequests channel, which is declared as follows:
var bankRequests chan *bankOp // channel of pointers to a bankOp
Channels are synchronized—that is, thread-safe—by default.
The miser and the spendthrift again call the updateBalance function in order to change the account balance. This function no longer has any explicit thread synchronization:
func updateBalance(amt int) int {   // request structure
   update := &bankOp{howMuch: amt,
                     confirm: make(chan int)}
   bankRequests <- update           // send request
   newBalance := <-update.confirm   // await confirmation
   return newBalance                // perhaps to be logged
}
The bankRequests channel has a buffer size of eight to minimize blocking. The channel can hold up to eight unread requests before further attempts to add another bankOp pointer are blocked. In the meantime, the banker goroutine should be processing the requests as they arrive; a request is removed automatically from the channel when the banker reads it. The confirm channel is not buffered, however. The requester blocks until the confirmation message—the updated balance stored locally in the newBalanace variable—arrives from the banker.
Local variables and parameters in the updateBalance function (update, newBalance, and amt) are thereby thread-safe because every goroutine gets its own copies of them. The channels, too, are thread-safe so that the body of the updateBalance function no longer requires explicit locking. What a relief for the programmer!
The banker goroutine loops indefinitely, awaiting requests from the miser and spendthrift goroutines:
for {
   select {
   case request := <-bankRequests:      // Is there a request?
      accountBalance += request.howMuch // If so, update balance and
      request.confirm <- accountBalance // confirm to requester
   }
   // other cases could be added (e.g., golf outings)
}
While the miser and spendthrift goroutines are still active, only the banker goroutine has access to the accountBalance, which means that a race condition on this memory location cannot arise. Only after the miser and spendthrift finish their work and terminate does the main goroutine print the final value of the accountBalance and exit. When main terminates, so does the banker goroutine.
Locks or channels?
The MiserSpendthrift2 program adheres to the Go mantra by favoring channels over synchronized shared memory. To be sure, locked memory can be tricky. The mutex API is low-level and thus prone to mistakes such as locking but forgetting to unlock—with deadlock as a possible result. More subtle mistakes include locking only part of a critical section (underlocking) and locking code that does not belong to a critical section (overlocking). Thread-safe functions such as atomic.AddInt32 reduce these risks because the locking and unlocking occur automatically. Yet the challenge remains of how to reason about low-level memory locking in complicated programs.
The Go mantra brings challenges of its own. If the two miser/spendthrift programs are run with a sufficiently large command-line argument, the contrast in performance is noteworthy. The mutex may be low-level, but it performs well. Go channels are appealing because they provide built-in thread safety and encourage single-threaded access to shared critical resources such as the accountBalance in the two sample programs. Channels, however, incur a performance penalty compared to mutexes.
"
Go,"An introduction to Go arrays and slices
Learn the pros and cons of storing data in Go using arrays and slices and why one is usually better than the other.
This article is part of a Go series by Mihalis Tsoukalos:
Part 1: Creating random, secure passwords in Go
Part 2: Build a concurrent TCP server in Go,
Part 3: 3 ways to copy files in Go
In this fourth article in the series, I will explain Go arrays and slices, how to use them, and why you'll usually want to choose one over the other.
Arrays
Arrays are one of the most popular data structures among programming languages for two main reasons: They are simple and easy to understand, and they can store many different kinds of data.
You can declare a Go array named anArray that stores four integers with the following:
anArray := [4]int{-1, 2, 0, -4}
The array's size should be stated before its type, which should be defined before its elements. The len() function can help you find the length of any array. The size of the array above is 4.

If you are familiar with other programming languages, you might try to access all the elements of an array using a for loop. However, as you will see below, Go's range keyword allows you to access all the elements of an array or a slice in a more elegant way.

Last, here is how you can define an array with two dimensions:
twoD := [3][3]int{
    {1, 2, 3},
    {6, 7, 8},
    {10, 11, 12}}
The arrays.go source file explains the use of Go arrays. The most important code in arrays.go is:
for i := 0; i < len(twoD); i++ {
        k := twoD[i]
        for j := 0; j < len(k); j++ {
                fmt.Print(k[j], "" "")
        }
        fmt.Println()
}

for _, a := range twoD {
        for _, j := range a {
                fmt.Print(j, "" "")
        }
        fmt.Println()
}
This shows how you can iterate over the elements of an array using a for loop and the range keyword. The rest of the code of arrays.go shows how to pass an array into a function as a parameter.
Following is the output of arrays.go:
$ go run arrays.go
Before change(): [-1 2 0 -4]
After change(): [-1 2 0 -4]
1 2 3
6 7 8
10 11 12
1 2 3
6 7 8
10 11 12
This output demonstrates that the changes you make to an array inside a function are lost after the function exits.
Disadvantages of arrays
Go arrays have many disadvantages that will make you reconsider using them in your Go projects. First, you can't change the size of an array after you define it, which means Go arrays are not dynamic. Putting it simply, if you need to add an element to an array that doesn't have any space left, you will need to create a bigger array and copy all the elements of the old array to the new one. Second, when you pass an array to a function as a parameter, you actually pass a copy of the array, which means any changes you make to an array inside a function will be lost after the function exits. Last, passing a large array to a function can be pretty slow, mostly because Go has to create a copy of the array.
The solution to all these problems is to use Go slices.
Slices
A Go slice is similar to a Go array without the shortcomings. First, you can add an element to an existing slice using the append() function. Moreover, Go slices are implemented internally using arrays, which means Go uses an underlying array for each slice.
Slices have a capacity property and a length property, which are not always the same. The length of a slice is the same as the length of an array with the same number of elements, and it can be found using the len() function. The capacity of a slice is the room that has currently been allocated for the slice, and it can be found with the cap() function.
As slices are dynamic in size, if a slice runs out of room (which means the current length of the array is the same as its capacity while you are trying to add another element to the array), Go automatically doubles its current capacity to make room for more elements and adds the requested element to the array.
Additionally, slices are passed by reference to functions, which means what is actually passed to a function is the memory address of the slice variable, and any modifications you make to a slice inside a function won't get lost after the function exits. As a result, passing a big slice to a function is significantly faster than passing an array with the same number of elements to the same function. This is because Go will not have to make a copy of the slice—it will just pass the memory address of the slice variable.
Go slices are illustrated in slice.go, which contains the following code:
package main

import (
        ""fmt""
)

func negative(x []int) {
        for i, k := range x {
                x[i] = -k
        }
}

func printSlice(x []int) {
        for _, number := range x {
                fmt.Printf(""%d "", number)
        }
        fmt.Println()
}

func main() {
        s := []int{0, 14, 5, 0, 7, 19}
        printSlice(s)
        negative(s)
        printSlice(s)

        fmt.Printf(""Before. Cap: %d, length: %d\n"", cap(s), len(s))
        s = append(s, -100)
        fmt.Printf(""After. Cap: %d, length: %d\n"", cap(s), len(s))
        printSlice(s)

        anotherSlice := make([]int, 4)
        fmt.Printf(""A new slice with 4 elements: "")
        printSlice(anotherSlice)
}
The biggest difference between a slice definition and an array definition is that you do not need to specify the size of the slice, which is determined by the number of elements you want to put in it. Additionally, the append() function allows you to add an element to an existing slice—notice that even if the capacity of a slice allows you to add an element to that slice, its length will not be modified unless you call append(). The printSlice() function is a helper function used for printing the elements of its slice parameter, whereas the negative() function processes all the elements of its slice parameter.
The output of slice.go is:
$ go run slice.go
0 14 5 0 7 19
0 -14 -5 0 -7 -19
Before. Cap: 6, length: 6
After. Cap: 12, length: 7
0 -14 -5 0 -7 -19 -100
A new slice with 4 elements: 0 0 0 0
Please note that when you create a new slice and allocate memory space for a given number of elements, Go will automatically initialize all the elements to the zero value of its type, which in this case is 0.
Referencing arrays with slices
Go allows you to reference an existing array with a slice using the [:] notation. In that case, any changes you make into a slice's function will be propagated to the array—this is illustrated in refArray.go. Please remember that the [:] notation does not create a copy of the array, just a reference to it.
The most interesting part of refArray.go is:
func main() {
        anArray := [5]int{-1, 2, -3, 4, -5}
        refAnArray := anArray[:]

        fmt.Println(""Array:"", anArray)
        printSlice(refAnArray)
        negative(refAnArray)
        fmt.Println(""Array:"", anArray)
}
The output of refArray.go is:
$ go run refArray.go
Array: [-1 2 -3 4 -5]
-1 2 -3 4 -5
Array: [1 -2 3 -4 5]
So, the elements of the anArray array changed because of the slice reference to it.
Summary
Although Go supports both arrays and slices, it should be clear by now that you will most likely use slices because they are more versatile and powerful than Go arrays. There are only a few occasions where you will need to use an array instead of a slice. The most obvious one is when you are absolutely sure that you will need to store a fixed number of elements.

"
JavaScript$Tools,"Bring JavaScript to your Java enterprise with Vert.x
Refresh your enterprise toolbox with the powers of JavaScript without leaving your JVM deployments.
If you are a Java programmer, chances are that you've either used JavaScript in the past or will in the near future. Not only is it one of the most popular (and useful) programming languages, understanding some of JavaScript's features could help you build the next uber-popular web application.
JavaScript on the server
The idea to run JavaScript on the server is not new; in fact, in December 1995, soon after releasing JavaScript for browsers, Netscape introduced an implementation of the language for server-side scripting with Netscape Enterprise Server. Microsoft also adopted it on Internet Information Server as JScript, a reverse-engineered implementation of Netscape's JavaScript.
The seed was planted, but the real boom happened in 2009 when Ryan Dahl introduced Node.js. Node's success was not based on the language but on the runtime itself. It introduced a single process event loop that followed the reactive programming principles and could scale like other platforms couldn't.

The enterprise and the JVM
Many enterprises have standardized on the Java virtual machine (JVM) as the platform of choice to run their mission-critical business applications, and large investments have been made on the JVM, so it makes sense for those organizations to look for a JVM-based JavaScript runtime.
Eclipse Vert.x is a polyglot-reactive runtime that runs on the JVM. Using Eclipse Vert.x with JavaScript is not much different from what you would expect from Node.js. There are limitations, such as that the JVM JavaScript engine is not fully compatible with the ES6 standard and not all Node.js package manager (npm) modules can be used with it. But it can still do interesting things.
Why Eclipse Vert.x?
Having a large investment in the JVM and not wanting to switch to a different runtime might be reason enough for an enterprise to be interested in Eclipse Vert.x. But other benefits are that it can interact with any existing Java application and offers one of the best performances possible on the JVM.
To demonstrate, let's look at how Vert.x works with an existing business rules management system. Imagine for a moment that our fictional enterprise has a mission-critical application running inside JBoss Drools. We now need to create a new web application that can interact with this legacy app.
For the sake of simplicity, let's say our existing rules are a simple Hello World:
package drools

//list any import classes here.

//declare any global variables here

rule ""Greetings""
    when
        greetingsReferenceObject: Greeting( message == ""Hello World!"" )
    then
        greetingsReferenceObject.greet();
    end
When this engine runs, we get ""Drools Hello World!"" This is not amazing, but let's imagine this was a really complex process.
Implementing the Eclipse Vert.x JavaScript project
Like with any other JavaScript project, we'll use the standard npm commands to bootstrap a project. Here's how to bootstrap the project drools-integration and prepare it to use Vert.x:
# create an empty project directory
mkdir drools-integration
cd drools-integration

# create the initial package.json
npm init -y

# add a couple of dependencies
npm add vertx-scripts --save-dev
# You should see a tip like:
#Please add the following scripts to your 'package.json':
# ""scripts"": {
#   ""postinstall"": ""vertx-scripts init"",
#   ""test"": ""vertx-scripts launcher test -t"",
#   ""start"": ""vertx-scripts launcher run"",
#   ""package"": ""vertx-scripts package""
# }

# add
npm add @vertx/web --save-prod
We have initialized a bare-bones project so we can start writing the JavaScript code. We'll start by adding a simple HTTP server that exposes a simple API. Every time a request is made to the URL http://localhost:8080/greetings, we should see the existing Drools engine's execution result in the terminal.
Start by creating an index.js file. If you're using VisualStudio Code, it's wise to add the following two lines to the beginning of your file:
/// <reference types=""@vertx/core/runtime"" />
/// @ts-check
These lines will enable full support and check the code for syntax errors. They aren't required, but they sure help during the development phase.
Next, add the simple HTTP server. Running on the JVM is not exactly the same as running on Node, and many libraries will not be available. Think of the JVM as a headless browser, and in many cases, code that runs in a browser can run on the JVM. This does not mean we can't have a high-performance HTTP server; in fact, this is exactly what Vert.x does. Let's start writing our server:
import { Router } from '@vertx/web';

// route all request based on the request path
const app = Router.router(vertx);

app.get('/greetings').handler(function (ctx) {
    // will invoke our existing drools engine here...
});

vertx
// create a HTTP server
.createHttpServer()
// on each request pass it to our APP
.requestHandler(function (req) {
    app.accept(req);
})
// listen on port 8080
.listen(8080);
The code is not complicated and should be self-explanatory, so let's focus on the integration with existing JVM code and libraries in the form of a Drools rule. Since Drools is a Java-based tool, we should build our application with a java build tool. Fortunately, because, behind the scenes, vertx-scripts delegates the JVM bits to Apache Maven, our work is easy.
mkdir -p src/main/java/drools
mkdir -p src/main/resources/drools
Next, we add the file src/main/resources/drools/rules.drl with the following content:
package drools

//list any import classes here.

//declare any global variables here

rule ""Greetings""
    when
        greetingsReferenceObject: Greeting( message == ""Hello World!"" )
    then
        greetingsReferenceObject.greet();
    end
Then we'll add the file src/main/java/drools/Greeting.java with the following content:
package drools;

public interface Greeting {

  String getMessage();

  void greet();
}
Finally, we'll add the helper utility class src/main/java/drools/DroolsHelper.java:
package drools;

import org.drools.compiler.compiler.*;
import org.drools.core.*;
import java.io.*;

public final class DroolsHelper {

  /**
   * Simple factory to create a Drools WorkingMemory from the given `drl` file.
   */
  public static WorkingMemory load(String drl) throws IOException, DroolsParserException {
    PackageBuilder packageBuilder = new PackageBuilder();
    packageBuilder.addPackageFromDrl(new StringReader(drl));
    RuleBase ruleBase = RuleBaseFactory.newRuleBase();
    ruleBase.addPackage(packageBuilder.getPackage());
    return ruleBase.newStatefulSession();
  }

  /**
   * Simple factory to create a Greeting objects.
   */
  public static Greeting createGreeting(String message, Runnable andThen) {
    return new Greeting() {
      @Override
      public String getMessage() {
        return message;
      }

      @Override
      public void greet() {
        andThen.run();
      }
    };
  }
}
We cannot use the file directly; we need to have drools. To do this, we add a custom property to our package.json named mvnDependencies (following the usual pattern):
{
    ""mvnDependencies"": {
        ""org.drools:drools-compiler"": ""6.0.1.Final""
    }
}
Of course, since we updated the project file, we should update npm:
npm install
We are now entering the final step of this project, where we mix Java and JavaScript. We had a placeholder before, so let's fill in the gaps. We first use the helper Java class to create an engine (you can now see the power of Vert.x, a truly polyglot runtime), then invoke our engine whenever an HTTP request arrives.
// get a reference from Java to the JavaScript runtime
const DroolsHelper = Java.type('drools.DroolsHelper');
// get a drools engine instance
const engine = DroolsHelper.load(vertx.fileSystem().readFileBlocking(""drools/rules.drl""));

app.get('/greetings').handler(function (ctx) {
  // create a greetings message
  var greeting = DroolsHelper.createGreeting('Hello World!', function () {
    // when a match happens you should see this message
    console.log('Greetings from Drools!');
  });

  // run the engine
  engine.insert(greeting);
  engine.fireAllRules();

  // complete the HTTP response
  ctx.response().end();
});
Conclusion
As this simple example shows, Vert.x allows you to be truly polyglot. The reason to choose Vert.x is not because it's another JavaScript runtime, rather it's a runtime that allows you to reuse what you already have and quickly build new code using the tools and language that run the internet. We didn't touch on performance here (as it is a topic on its own), but I encourage you to look at independent benchmarks such as TechEmpower to explore that topic.

"
Tools$Linux,"3 tools for viewing files at the command line
Take a look at less, Antiword, and odt2txt, three utilities for viewing files in the terminal.
I always say you don't need to use the command line to use Linux effectively—I know many Linux users who never crack open a terminal window and are quite happy. However, even though I don't consider myself a techie, I spend about 20% of my computing time at the command line, manipulating files, processing text, and using utilities.
One thing I often do in a terminal window is viewing files, whether text or word processor files. Sometimes it's just easier to use a command line utility than to fire up a text editor or a word processor.
Here are three of the utilities I use to view files at the command line.
less
The beauty of less is that it's easy to use and it breaks the files you're viewing down into discrete chunks (or pages), which makes them easier to read. You use it to view text files at the command line, such as a README, an HTML file, a LaTeX file, or anything else in plaintext. I took a look at less in a previous article.
To use less, just type:
less file_name
Scroll down through the file by pressing the spacebar or PgDn key on your keyboard. You can move up through a file by pressing the PgUp key. To stop viewing the file, press the Q key on your keyboard.
Antiword
Antiword is great little utility that you can use to that can convert Word documents to plaintext. If you want, you can also convert them to PostScript or PDF. For this article, let's just stick with the conversion to text.
Antiword can read and convert files created with versions of Word from 2.0 to 2003. It doesn't read DOCX files—if you try, Antiword displays an error message that what you're trying to read is a ZIP file. That's technically correct, but it's still frustrating.
To view a Word document using Antiword, type the following command:
antiword file_name.doc
Antiword converts the document to text and displays it in the terminal window. Unfortunately, it doesn't break the document into pages in the terminal. You can, though, redirect Antiword's output to a utility like less or more to paginate it. Do that by typing the following command:
antiword file_name.doc | less
If you're new to the command line, the | is called a pipe. That's what does the redirection.
odt2txt
Being a good open source citizen, you'll want to use as many open formats as possible. For your word processing needs, you might deal with ODT files (used by such word processors as LibreOffice Writer and AbiWord) instead of Word files. Even if you don't, you might run into ODT files. And they're easy to view at the command line, even if you don't have Writer or AbiWord installed on your computer.
How? With a little utility called odt2txt. As you've probably guessed, odt2txt converts an ODT file to plaintext. To use it, run the command:
odt2txt file_name.odt
Like Antiword, odt2txt converts the document to text and displays it in the terminal window. And, like Antiword, it doesn't page the document. Once again, though, you can pipe the output from odt2txt to a utility like less or more using the following command:
odt2txt file_name.odt | more

"
Cat$Linux,"Getting started with the Linux cat command
The Linux cat and zcat commands are more useful than you may realize.
Cat is a fairly simple tool designed to concatenate and write file(s) to your screen, which is known as standard output (stdout). It is part of the GNU Core Utils released under the GPLv3+ license. You can expect to find it in just about any Linux distribution or other Unix operating environment, such as FreeBSD or Solaris. The simplest use of cat is to show the contents of a file. Here is an example with a file named hello.world:
$ ls
hello.world
$ cat hello.world
Hello World!

$

The most common way I use the cat command is for viewing configuration files, such as those in the /etc directory. The cat command will display a file without risking damage to it. If I open a critical configuration file using an editor such as Vi or Nano, I could inadvertently make unwanted changes to the file. The cat command is not an editor and therefore poses no risk of making changes to a file's content.

If I need to view a longer file, I can use a pipe with the more command:
$ cat <somelongfile> | more
Cat can display multiple files at the same time. If we want to see two files—hello.world and goodbye.world—we would include both filenames as arguments in the command:
$ cat hello.world goodbye.world
Hello World!

Good Bye World!

$
Cat can also number a file's lines during output. There are two commands to do this, as shown in the help documentation:
-b, --number-nonblank    number nonempty output lines, overrides -n
-n, --number             number all output lines
If I use the -b command with the hello.world file, the output will be numbered like this:
$ cat -b hello.world
     1  Hello World!

$
In the example above, there is an empty line. We can determine why this empty line appears by using the -n argument:
$ cat -n hello.world
     1  Hello World!
     2
$
Now we see that there is an extra empty line. These two arguments are operating on the final output rather than the file contents, so if we were to use the -n option with both files, numbering will count lines as follows:
$ cat -n hello.world goodbye.world
     1  Hello World!
     2  
     3  Good Bye World!
     4
$
One other option that can be useful is -s for squeeze-blank. This argument tells cat to reduce repeated empty line output down to one line. This is helpful when reviewing files that have a lot of empty lines, because it effectively fits more text on the screen. Suppose I have a file with three lines that are spaced apart by several empty lines, such as in this example, greetings.world:
$ cat greetings.world
Greetings World!




Take me to your Leader!




We Come in Peace!
$
Using the -s option saves screen space:
$ cat -s greetings.world
Greetings World!

Take me to your Leader!

We Come in Peace!
$
Cat is often used to copy contents of one file to another file. You may be asking, ""Why not just use cp?"" Here is how I could create a new file, called both.files, that contains the contents of the hello and goodbye files:
$ cat hello.world goodbye.world > both.files
$ cat both.files
Hello World!

Good Bye World!

$
zcat
There is another variation on the cat command known as zcat. This command is capable of displaying files that have been compressed with Gzip without needing to uncompress the files with the gunzip command. As an aside, this also preserves disk space, which is the entire reason files are compressed!
The zcat command is a bit more exciting because it can be a huge time saver for system administrators who spend a lot of time reviewing system log files. Where can we find compressed log files? Take a look at /var/log on most Linux systems. On my system, /var/log contains several files, such as syslog.2.gz and syslog.3.gz. These files are the result of the log management system, which rotates and compresses log files to save disk space and prevent logs from growing to unmanageable file sizes. Without zcat, I would have to uncompress these files with the gunzip command before viewing them. Thankfully, I can use zcat:
$ cd /var/log
$ ls *.gz
syslog.2.gz  syslog.3.gz
$
$ zcat syslog.2.gz |more
Jan 30 00:02:26 workstation systemd[1850]: Starting GNOME Terminal Server...
Jan 30 00:02:26 workstation dbus-daemon[1920]: [session uid=2112 pid=1920] Successful
ly activated service 'org.gnome.Terminal'
Jan 30 00:02:26 workstation systemd[1850]: Started GNOME Terminal Server.
Jan 30 00:02:26 workstation org.gnome.Terminal.desktop[2059]: # watch_fast: ""/org/gno
me/terminal/legacy/"" (establishing: 0, active: 0)
Jan 30 00:02:26 workstation org.gnome.Terminal.desktop[2059]: # unwatch_fast: ""/org/g
nome/terminal/legacy/"" (active: 0, establishing: 1)
Jan 30 00:02:26 workstation org.gnome.Terminal.desktop[2059]: # watch_established: ""/
org/gnome/terminal/legacy/"" (establishing: 0)
--More--
We can also pass both files to zcat if we want to review both of them uninterrupted. Due to how log rotation works, you need to pass the filenames in reverse order to preserve the chronological order of the log contents:
$ ls -l *.gz
-rw-r----- 1 syslog adm  196383 Jan 31 00:00 syslog.2.gz
-rw-r----- 1 syslog adm 1137176 Jan 30 00:00 syslog.3.gz
$ zcat syslog.3.gz syslog.2.gz |more
The cat command seems simple but is very useful. I use it regularly. You also don't need to feed or pet it like a real cat. As always, I suggest you review the man pages (man cat) for the cat and zcat commands to learn more about how it can be used. You can also use the --help argument for a quick synopsis of command line arguments."
OpenWest$JavaScript,"When not to use a JavaScript framework
Helpful hints on where frameworks make sense, and where they don’t.
As the internet has evolved, web development has grown well beyond its intended abilities—for good and bad. To smooth over the rougher edges, web developers have invented a plethora of frameworks, both small and not so small. This has been good for developers, because browser fragmentation and standards problems abound, especially for those who want new features in APIs and more unified syntax for those features. Plus, for the most part the frameworks have been open source, which is great for everyone.
Now, seeing that those rough edges have been worn down by time and aren't as sharp as they once were, we should probably reduce our use of some of the frameworks we created. In other ways, we simply need to consider the cost of using a framework for the given task.

Things we can do ourselves
Consider the humble HTTP request, once a good 50-line function for a simple GET that worked both in Firefox and Internet Explorer. For example, here is a simple function that does a POST; we used it in production in Phone Janitor for more than a year as our main React data pump:
function postMe(name, data, callback, onError) {
    var request = new XMLHttpRequest();
    request.onreadystatechange = function() {
        if (request.readyState != 4 || request.status != 200) { return; }
        var body = JSON.parse(request.responseText);
        if (body.error) {
            onError(body.error);
        }
        else {
            callback.(body);
        }
    };
    request.open(""POST"", '/api/' + name, true);
    request.setRequestHeader(""Content-type"", ""application/json"");
    request.send(JSON.stringify(data));
}
This framework-free code could easily be rewritten to work with Promises, be adaptable for request types, or support any number of features that might be critical to your application. Is it well-engineered? Maybe not. Is it robust? It was for our needs at the time. The takeaway is less about what it is or isn't, and more about why I would use anything else.
If I don't want to write my own HTTP request engine, there is a cornucopia of options. They all come with a cost, though. How big are they? How can I include them in my code, and how does it affect my workflow? What other unnecessary things are they doing that waste time in execution? If I spent an hour (which is about what we spent on the code and testing) implementing this function to meet all my needs, would that save a lot of time compared to what it would take to integrate a library to do the same thing? Each of us will have different answers.
All things to all people
We consume services that try to be many things for a variety of use cases. This is really the crux of the issue. It is a good thing to unify APIs for the community's benefit, because some things are nuanced and hard to do alone. jQuery was invented because browsers did things wildly differently, and the JavaScript API didn't have much to it. There was a time, though, that every web dev included jQuery just so they could select document object module (DOM) elements for simple innerHTML, and it made a noticeable impact on page-load times. Consider that use case now:
// Author's note, this is mostly for example, don't manipulate DOM unless you know what that means for your app

var el1 = document.getElementById(id_Name);
var el2 = document.getElementsByClass(class_Name); // returns array
var el3 = document.querySelector(""div.user-panel.main input[name='login']"");

// Want it in a jquery style function name?

var $ = document.querySelector; // Or get fancier if you wish
var el4 = $(""div.user-panel.main input[name='login']"");
We still include jQuery for this in a lot of places  (e.g., the average WordPress theme). Don't take my word for it; feel free to look and see where it's included or not. There isn't much we can't do these days without it.
Even if we use frameworks
This is not just a matter of how and when we use frameworks, though; it's also about how we approach features and add-ons. For instance, consider Google Visualization integration into the Angular framework. At MobilSense, we rely heavily on charts for reporting to management teams—but we also use Angular 1.5. So how do we integrate updating functionality into our app's charts?
Our options were using angular-google-chart or developing our own solution. Although angular-google-chart is a fantastic library—I have used it elsewhere and I appreciate the author supporting his project for free—we decided to do our own for some now obvious reasons. Here's how they stacked up:
Our own solution does not handle all the cases the library does. We don't need those cases, and if we did, we could probably add them pretty easily and in a way that is portable to our workflows and other frameworks. This is the type of tradeoff we each need to decide based on our own specific needs. There is no shame in either choice.
When we should and shouldn't use frameworks
I strongly advocate for knowing the purpose in coding a given tool. If our goal is a quickly cobbled-together thing that exists temporarily, engineering it well probably doesn't matter. If we're looking for longevity, I think using framework tools is something we need to weigh more heavily. A framework is hard to transition away from, especially if we add in libraries that further tie us into that framework.
If it will take just a day or two to write your own solution, I would lean toward that. If it will take a week or more, the considerations may change.
Another good reason to write your own would be if it would be costly to couple yourself to a framework that may not be around for the life of your project. However, if it is a really complicated thing, such as integrating PDF support, you probably don't want to consider writing your own, as in that way lies madness.
As with any type of software engineering, consider your work like construction. If you're building a doghouse, whatever you do is probably fine. If you're building a skyscraper, you've got to do more planning. Where do we draw the line between them? The role of frameworks is the same as the role of the materials and styles of construction you are building. Does it fit the environment, and can we get replacement materials later on when we need them? Although your decisions are your own, I hope this information and these examples will help guide you."
JavaScript,"Getting started with React Native animations
Here are the tools you need to overcome performance challenges when implementing React Native animations.
React Native animation is a popular topic for workshops and classes, perhaps because many developers find it challenging to work with. While many online blogs and resources focus on the performance aspects of React Native, few take you through the basics. In this article, I will discuss the fundamentals of how to implement React Native animations.
First, let's review some background and history.
History and evolution
To work with cross-platform JavaScript code, native components of your phone must communicate information via an element called a bridge. The bridge is asynchronous, which causes JavaScript-based React Native apps to lag because asynchronous requests over the bridge clog the path of JavaScript code interacting with the native parts.
To achieve high performance, animations must be rendered on a native UI thread. Since data needs to be serialized over the bridge, this often blocks the JavaScript thread, causing a drop in frames. This problem has been prevalent since 2015 when animations posed one of the biggest limitations in React Native.
Fortunately, the situation has improved since then thanks to overwhelming support from community members. Achieving 60 frames per second (FPS) is now common for React Native animations. Declarative APIs such as Animated have eased the process of implementing interactive animations.
Using the Animated API to improve performance
Still, it is not unusual for developers to encounter performance issues, especially when they are working on complex animations.
As mentioned above, performance bottlenecks in React Native animations are caused by heavy workloads on JavaScript threads, which reduces the frame rate and causes a sluggish user experience. To overcome this problem, you need to maintain a frame rate of 60 frames per second.
Using the Animated API is the best solution to this as it optimizes the required serialization/deserialization time. It does this by using a declarative API to describe animations. The idea behind this is to declare the entire animation at once in advance so that the declaration in JavaScript can be serialized and sent to the bridge. A driver then executes the animations frame by frame.
How to implement animations in React Native
There are several ways to implement animations in React Native. Here are some that I find most useful.
Animated values
Animated values tops my list as the building block for animations in any React Native app. These generally point to a real value, which is converted back to a real number when passed with an animated component.
Let’s look at an example:
Animated.timing(this.valueToAnimate, {
    toValue: 42;
    duration: 1000;
}).start()
In the above example, I have declared value.ToAnimate as 42, which will be executed after 1000 milliseconds.
You can also use Animated values to declare properties such as opacity or position. Here’s an example implementation of opacity with Animated values:
<Animated.View style={{ opacity: myAnimatedOpacity }} />  
Animation drivers: Animated.timing, Animated.event, Animated.decay
Think of drivers like nodes in a graph that changes an Animated value with each frame. For instance, Animated.timing will increase a value, while Animated.decay will reduce a value over each frame.
Let’s look at an example:
Animated.decay(this.valueToAnimate, {
   velocity: 2.0,
   deceleration: 0.9
}).start();
This example launches the animation at a particular velocity and gradually decelerates at a particular time. I like to do this in a cross-platform app when docs on the material design initially surface. It feels pretty great, and there are many ways to make the animation deliver a memorable experience.
You can also use Animated.event to drive a value as your user scroll:
<ScrollView onScroll={Animated.event(
  [{nativeEvent: {contentOffset: {y: this.state.scrollY}}}]
)}
>
</ScrollView>
In the above example, Animated.event returns a function that sets the scrollView's nativeEvent.contentOffset.y to your current scrollY state.
All in all, animated drivers can be used in association with Animated values or other Animated drivers.
As a side note, keep in mind that when a driver updates each frame, the new value will instantly update the View property. So be careful while declaring the variables and mindful of their scope while using them.
Transform methods
Transform methods enable you to convert an Animated value to a new Animated value.
You can use methods such as Animated.add(), Animated.multiply(), or Animated.interpolate() to implement transform operations. You can execute a transform operation on any node in the Animated graph with this:
newAnimated.Value(55).interpolate(.....) // Transformation operation using Animated.interpolate() method
Animated props
Animated props are special nodes that map an Animated value to a prop on a component. It is generated when you render an Animated.view and assign it properties.
Let’s look at the following code snippet:
Var opacity = new Animated.Value(0.7);
<Animated.View style={{ opacity }} />
Here, I’ve added an Animated prop that converts the value 0.7 to a property. If a method updates the value, the change will be reflected in the View’s property.
The methods described above work in conjunction with, and play a crucial role in, animating objects in React Native.
The animated value for every frame of the animation is changed by the animation driver (Animated.Timing, Animated.Event, or Animated.Decay). The result is then passed along to any transformation operation, where it gets stored as the prop of the view (opacity or transform value).
The result is then handed over to the native realm by JavaScript, where the view gets updated while calling setNativeProps. Finally, it is passed over to iOS or Android, where UIView or Android.View gets updated.
Implementing animations using the Animated API and Native Driver
Since the inception of the React Native Animated API, a JavaScript driver has been used for frame execution, but it resulted in frame drop as the business logic directly falls on the JavaScript thread.
To address frame drops, the latest version of the driver was made purely native, and it is now capable of executing animations frame by frame in native realm.
The Native Driver, when used alongside the Animated API, allows the native animated module to update views directly without the need to calculate the value in JavaScript.
In order to use Native Driver, you must specify useNativeDriver to be true while configuring your animations:
useNativeDriver: true
Using PanResponder for handling gestures in React Native

The React Native Animated API can do most of the ""grunt work"" for you while implementing React Native animations. However, it has a key limitation when it comes to implementing gestures in animations: It is unable to respond to gestures beyond the scope of a ScrollView.

While you can do many things with a simple ScrollView component, you will likely agree that mobile is incomplete without gestures, which are the actions users perform with animations, such as scroll or pan.
In React Native, gestures can be handled seamlessly by using PanResponder with the Animated API.
PanResponder combines various touches into a specific gesture. It makes a single touch responsive to extra touches so that gestures function smoothly.
By default, PanResponder consists of an InteractionManager handle, which blocks the events running on the JS thread from interrupting the gestures.
Improving uptime for slow Navigator transitions
Any React Native animation that involves moving from one app screen to another should usually be done using navigation components. Navigation components such as React Navigation are generally used for navigation transitions.
In React Native, navigation transitions usually happen in JavaScript threads, which can result in slow transitions in for low-end/low-memory devices (typically Androids, as iOS devices handle these more effectively). Slow navigation transitions usually happen when React Native is trying to render a new screen while an animation is still executing in the background.
To avoid such situations, InteractionManager allows long-running tasks to be scheduled after an animation or interaction has executed in the JavaScript thread.
Layout animations
LayoutAnimation is a simple API that automatically animates the view to the next consecutive position when the next layout happens. It works on the UI thread, which makes it highly performant.
Animations configured using LayoutAnimation will apply on all the components once it is called, in contrast to Animated, in which you control the specific values to animate. LayoutAnimation is capable of animating everything that changes on the next rendering, so you should call it before calling setState.
Configuring a layout animation before calling setState will ensure smooth animations in native thread and prevent your animations from being affected if code in another setState diff is triggered (under normal conditions, this would compromise your app's animation).
Another way of using LayoutAnimation is to call it inside the component WillReceiveProps method. Simply call LayoutAnimation.configureNext by passing the appropriate parameters for animation configuration, as shown below:
LayoutAnimation.configureNext(animationConfiguration, callbackCompletionMethod); 
this.setState({ stateToChange: newStateValue });
LayoutAnimation supports only two properties: opacity and scalability.
It identifies views by using their unique keys and computing their expected position. Moreover, it animates the frame changes as long as the view keeps the same key between changes of states.
Animations implemented using LayoutAnimation happen natively, which is good from a performance perspective, but it can be challenging if all properties need to be animated between states.
Closing thoughts
This article only scratches the surface of React Native animations. A rule of thumb when working in React Native is to use Animated API wherever possible. For gestures, use PanResponder alongside Animated API.
Leveraging Native Driver along with Animated API will eliminate most of the issues you may encounter—but if performance still lags, stick to LayoutAnimation.

"
3Dprinting$OpenSCAD$FreeCAD$Blender,"
Take a free course on OpenSCAD, FreeCAD, and Blender
Expand your job skills with Wikiversity's Open Source 3D Printing course.
Demand for 3D printing skills is soaring, with many engineering job listings from a variety of fields, including biomedical, software, and transportation, requiring familiarity with 3D printing.
Wikiversity is trying to fill this skills gap with its Open Source 3D Printing (OS3DP) course for undergraduate and graduate university students. Following in the tradition that started with the first open source RepRap (self replicating rapid prototyper), everything in this class—which will teach you what you need to know about 3D printing—is free.
Schools can use the course as a foundation to help their students create the future. Educators can adapt all or any part of the course, including all of the course videos, and anyone who wants to learn independently can walk through it at their own pace.
The difference between the course's undergraduate and graduate versions is that the graduate students must do everything the undergraduates do, but also make a significant improvement on the RepRap printers and publish their mods. This has resulted in an explosion of creativity, with past students adding heated beds, multi-material capability, solar-power, engraving and laser cutting functions, climate control, touchscreens, and error alarms.
Getting 3D printers
It's hard to learn about 3D printing without access to a 3D printer, so schools are using a variety of ways to make them available. For example, Michigan Tech charges a $500 course fee to cover the cost of a MOST Delta RepRap 2 kit, which students build and use in the course and keep afterwards. Despite the fee, the course is extremely popular and consistently overbooked with a long waiting list.
Other universities have either outfitted a 3D printer lab with commercial RepRaps like the Lulzbot Taz or loaned 3D printers to students. Another option is to have students build a JellyBox RepRap, which they assemble with zip ties and then disassemble at the end of the course to prepare the kit for the next class.
Although it is better from a pedagogical standpoint if the students can build and hack their own systems, this may be economically prohibitive for your students. In this case, the other methods are good substitutes.
What you will learn
The course is built around a selection of progressively more challenging exercises that teach students three open source design packages (OpenSCAD, FreeCAD, and Blender) so they can solve just about any 3D printing design challenge. 
Rock wall teaches the basics of OpenSCAD and is an easy way to ensure students can use the 3D printers.
Customizer teaches a more advanced version of OpenSCAD to make it easy for novices to adapt students' designs. It uses the open source customizer to help students learn to solve all the problems in a category rather than just their own problems.
Viking mashup teaches students to take an open source design from the web and make a challenging adaptation to it by revising and modding others' designs to meet their functional or aesthetic needs. This feeds into the open source evolutionary design for all 3D printable products.
Adaptive aid is a virtual service learning project meant to demonstrate to students how open source sharing can help real people. Last year's students designed open source adaptive aids that saved more than 94%, on average, compared to commercial aids.
OSH Science is a service learning project and a mini-version of Wikiversity's 3D Printing of Open Source Hardware for Science course that helps students get to know research problems at their schools and how to design for high-tech environments following the Open Source Lab model.
OSAT is a virtual service learning project to demonstrate how open source sharing can help real people in resource-constrained contexts.
Big Money is the final project for the undergrads. It combines everything they've learned and asks them to demonstrate how they can create high-value products using a low-cost open source RepRap 3D printer.
OS3DP is one of 250+ courses that Wikiversity hosts to help ""set learning free."" Have a look and see what other fun subjects you can learn or contribute to help others learn.


"
Arduino$3Dprinting,"CNC milling with open source software
Create your favorite designs with a DIY CNC milling machine based on open source software and an Arduino controller.
I'm always looking for new projects to create with my 3D printer. When I recently saw a new design for a computer numeric code (CNC) milling machine that mostly uses 3D printed parts, I was intrigued. When I saw that the machine works with open source software and the controller is an Arduino running open source software, I knew I had to make one.
CNC milling machines are precision cutting tools used in creating dies, engravings, and models. Unlike other milling tools, CNC machines can move on three axes: the Z axis moves vertically, the X axis moves horizontally, and the Y axis moves backward and forward.
DIY your CNC
While many of this CNC machine's components are 3D printed, several parts must be ordered to make it work. Its creator, Nikodem Bartnik, has a list of parts on the project's Thingiverse page along with links to download the 3D printed parts' STL files.
I ordered the necessary parts and started the wait. They shipped from overseas, so some of them took about a month to arrive. In the meantime, I 3D-printed the other parts.
The CNC machine is built on an Arduino controller running the open source GRBL motion-control software. The GRBL controller receives CNC G-code (a text-based list of instructions for the CNC mill) and translates it into motion by driving the stepper motors. The machine has one stepper for the Z axis (up and down), one stepper for the X axis (left and right), and two steppers for the Y axis (backward and forward). The GRBL website has documentation for loading GRBL onto the Arduino board.
I won't go into details on the CNC hardware assembly, as Nikodem has a series of videos that explain it in detail. Instead, I'll focus on the open source software that can be used to create designs for and run the CNC machine. Along with Fedora as my base operating system, the other pieces of open source software I used are GRBL, Inkscape, jscut, and CNCjs.
For this tutorial, I'll explain how to create a CNC-milled, wooden version of Larry Ewing's Linux Tux logo.
Make your files
The first step toward making a wooden Tux is downloading a black-and-white version of the logo in PNG format from Wikimedia Commons.
So the Tux logo can work on the CNC machine, convert the Tux PNG file to an SVG file with Inkscape by dragging and dropping the PNG file into the Inkscape window. Then use Inkscape's Trace Bitmap option to convert the image to SVG paths. This creates duplicate copies of the image—one is paths, the other is a bitmap—delete the bitmap copy. Use the Break Apart option and remove the Fill property for the outline of Tux and Tux's mouth; you'll have an SVG image that looks like the inverse of the original image.
Next, convert the SVG file to CNC G-code, which is a list of instructions that tell the CNC machine how to create the desired design. I used the open source jscut software, which is a web-based computer-aided manufacturing (CAM) program. You can download the software and run it on your local machine or use the web-based version at jscut.org, which is what I did.
Open the SVG file on jscut.org using the Open SVG drop-down menu. Next, click on one or more parts of the SVG image, then click Create Operation. Choose the type of operation (engrave, outside, pocket, etc.) and the depth of the cut, and click Generate. The different types of operations define where the cut is made; you can see the effects of different operations by going to the Simulate GCODE tab, which shows a preview of what the cut will look like. For Tux, I created several operations for various parts of the design.
Set the properties of your cutting bit under the Tool section; these include specifying the diameter of the bit and how fast the cut should be made. One limitation of jscut is you can't specify different bit sizes for different operations. To create Tux, I needed to use two different bits: a smaller bit to engrave details like the eyes, nose, and mouth, and a larger bit to cut Tux's outline all the way through the wood board. I used jscut twice to generate two different G-code files: one to make the engravings with the smaller bit and another to cut out Tux's outline with the larger bit.
After you create the two G-code files, the next task is to get CNC controller software. I used the open source CNCjs. CNCjs is web-based, easy to use, and supports interfacing with GRBL controllers. After it's installed, access CNCjs in a web browser and connect it to your Arduino GRBL controller.
Click Upload G-Code and specify the file you created with jscut. First, I loaded the G-code file that makes the engravings on top of Tux.
Get ready to cut
You're almost ready to start cutting! Since you will be cutting all the way through the wood, it's smart to use a ""waste board""—a board that sits on top of that CNC machine that you don't mind cutting into. This is because when the machine cuts Tux out, it will go all the way through and into the board under it. The waste board can be replaced as needed.
Securely attach the waste board to your CNC machine to prevent any movement, then attach the piece of wood you will use for Tux. There are many ways to attach the wood—I use heavy-duty double-sided carpet tape.
The final step before you cut is to align the bit on the spindle to the piece of wood you are cutting. Use CNCjs's Axes buttons to physically move the spindle so that the X, Y, and Z coordinates of the bit are aligned with the top-left corner of your wood. The Z axis should be just above the wood; you should be able to slide a piece of paper between the wood and the bit but feel the bit dragging on the paper as it slides.
Once everything is lined up, set the work position offsets for the X, Y, and Z axes to zero by clicking the Set button for each.
Press the Z+ button in CNCjs to raise the bit a little so the spindle doesn't make a mark in the wood when it's turned on.
To start cutting, put on your safety glasses, hearing protection, and a dust mask. Next, turn on the spindle, and click the Play button in CNCjs.
Closely monitor the machine during cutting and have a plan to quickly disconnect the power to the CNC machine and spindle if needed. Be ready with a vacuum cleaner as well, because it will create a lot of sawdust.
After the first G-code file completes the engravings on Tux, turn off the spindle and press the X0Y0 button in CNCjs to return the spindle to the home X and Y axis work position. Then change to the larger bit, upload the G-code file that will cut out Tux's outline, and follow the same process to run it.
Sand Tux to clean things up and apply some wood finish to protect it. Here's my outcome: a wooden Tux made with 100% open source software."
AI$RaspberryPi$Arduino$3Dprinting,"How to use an Arduino and Raspberry Pi to turn a fiber optic neural network into wall art
Learn how a machine learning algorithm can produce a beautiful wall decoration.
Hollywood has made many big promises about artificial intelligence (AI): how it will destroy us, how it will save us, and how it will pass us butter. One of the less memorable promises is how cool it will look.
There's a great example of amazing AI visualization in Avengers: Age of Ultron when Tony Stark's AI butler Jarvis interacts with Ultron and we see an organic floating network of light morphing and pulsing. I wanted to make something similar to fill blank space on my apartment wall (to improve upon the usual Ikea art). Obviously, I couldn't create anything as amazing as Jarvis as a floating orb of light; however, I could use a machine learning algorithm that looks interesting with quirky data visualization: a neural network! It employs biologically inspired elements that were meant to replicate how (we thought) the human brain works.
Neural networks in machine learning are inspired by biological neurons in the brain. Each node in the network is connected to other nodes in the network. As a signal flows through it, the connections between the nodes alter the signal until it reaches the end of the network. The specific way that the connections adjust the signal is learned during the algorithm's training phase.
I set about recreating a neural network with light. For the nodes, I printed custom shapes on a 3D printer in a semi-transparent plastic. For the connections between nodes, I used what's known as side-glow fiber optic, which is normal fiber optic tubing that emits light along the length of the tubing (it looks very sci-fi).
The light source was an RGB LED strip that was laboriously cut, soldered, and glued into each node. (Each node took about 30 minutes to fit with the electronics.)
The component that tells the LEDs what to do is an Arduino board, and that it is told what to do by a little Raspberry Pi Zero, which makes the whole thing WiFi connected.
Now, how does it work? The light network is a glorified WiFi lamp. The real AI is running on a laptop connected to the same WiFi network. I wrote a handy bit of code that plugs into my standard neural network programming environment, and while I train my algorithm, it sends the connection values (green = high value, red = low value, blue = near zero) to the light network in real time. Then I get to see my neural network learning.
It will now live happily on my wall as an illuminating symbol of AI!
This graph shows the data pattern that the neural network is trying to replicate. Based on just the X and Y values (feature 1 and feature 2 in the graph), the neural network will try to output the correct classification (the color in the graph). Below is another animation that shows the light network learning alongside its prediction of the data. As you can see, the colors of the data points evolve until they closely match the original data. This is learning!
"
Hardware$3Dprinting,"How a university's 3D-printed prosthetics club provides devices for amputees
Duke University's eNable chapter collaborates with sponsors and other innovators to create and deliver 3D-printed prosthetic devices to amputees
In the spring of 2016, Duke University eNable started out as six engineering students with a passion for innovation and design, supported by a small stipend from the Innovation Co-Lab and a grant from OSPRI (Open Source Pedagogy, Research and Innovation), a project supported by Red Hat.
Since then we have established ourselves as a presence on campus, grown into a large interdisciplinary team, and connected with multiple recipients—including a young boy in Milot, Haiti. The resources offered through Duke and the sponsorship we've received allow us to continuously transform our ideas into things we can share with open source enthusiasts, makers, and dreamers alike.
Local recipients
Over the past year, our club has been working hard with three local recipients. Kaylyn, our first connection, was introduced in the article last year. She is a cake decorator from Wake Forest for whom we are building a custom modular device with multiple attachments that each serve a specific function in her bakery. This fall, we are working on the finishing touches on Kaylyn’s device and hope to deliver it within the next couple months.
Later in the year we connected with Brooke, a recent pharmacy school graduate who requested a hand that could pinch skin while she delivered subcutaneous injections. For Brooke, we started off with an open source design called the K-1 Hand; we then made modifications to adapt the shape of the thumb, create a more secure fastening system, and optimize the grip between the thumb and pointer finger. Brooke’s hand is now completed and ready to be delivered upon her return to North Carolina.
For our third recipient, an 11-year-old boy named Nathan, we are using another open source file called the Unlimbited Arm. Our goal is to help Nathan transition to a medical grade prosthesis; we are also adding some fun attachments such as a slingshot and a bike handle grip.
Working with recipients around Duke helped us improve our design process and better understand the benefits of leveraging open source with original ideas. These experiences helped drive our success with our fourth recipient, a boy named Chris.
Prosthetics in process
I met Chris in the summer of 2016, while volunteering in Milot with members of my home church. Chris, who was ten years old when I met him, has a congenital upper limb amputation—he was born with half an arm on his left side. With the goals of eNable fresh in my mind, I immediately wanted to connect with him and see if there was a way for our club to help. I asked our host to translate as I explained to Chris and his parents that I am in a group that endeavors to build devices for amputees, and they expressed interest in the idea and gratitude for the desire to help.

Back on campus in the fall, I pitched the idea to our club and began forming a small team to see it through. Meanwhile, we needed to acquire funding and gain support from the university. By networking with alumni from the Pratt School of Engineering, we connected with the Given Limb Foundation, which agreed to support the project. Later, we obtained additional funding through a combined grant from the Lord Foundation and the Engineering Alumni Council.

We began the design process at the start of the summer. Using pictures of Chris next to a tape measure, we extracted measurements to get a rough estimate of dimensions—these were used to design a socket for Chris’s upper arm and a forearm piece. For the hand itself, we turned to an open source design called the Gripper Thumb Hand. This design could grant Chris a basic gripping capability without requiring mechanical activation through his shoulder.
At the end of June, two of us traveled to Haiti with three different-sized prototypes, extra parts and materials, and equipment to perform a 3D structure scan and produce a plaster cast. We used this trip to test the fit and function of the prototype, and acquired a variety of measurements and models to help make appropriate modifications back at Duke.
Five weeks later, another pair of us returned with a finalized device, with updates that addressed the limitations of the prototypes. After performing a few last-minute changes on site, we were able to successfully deliver the arm to Chris and provide him with spare parts and materials to use if he outgrows this one. Over the next year, we hope to come up with a plan for the next steps of our relationship with Chris—whether that entails a return trip, connecting him with other members of the open source community, or referring to a professional prosthetics company.
What's next
As we launch into the new academic year, we are excited to work on new projects and continue pushing the boundaries of 3D printing and open source solutions. Among these are the incorporation of advanced printing techniques and more robust materials, and the development of a myoelectric arm that uses muscle signals to power motors to achieve its function. Another vital goal is to continue building relationships with prosthetists and healthcare professionals to establish the niche role of 3D printing and open source designs in this field. We want to balance the excitement of the work we do with an awareness of its technical limitations—we believe this understanding is the key to successfully delivering open source solutions.
We are grateful for the widespread and constant support we have received since eNable’s inception. The Innovation Co-Lab and OSPRI program at Duke have provided us with valuable resources, mentorship, and a great workspace. The Pratt School of Engineering also helped lift us to where we are today, through connections with alumni and sponsors and opportunities to engage with more students. Most importantly, we are thankful for the students in our club for their dedication and enthusiasm towards our work.
"
Hardware$Rover,"Why NASA open sourced the Rover
Need a new project? You can build your own.
The host of the Command Line Heroes podcast, Saron Yitbarek, kicks off each episode with a sound-studded description of an event that sets the stage for the topic of the episode. Sometimes it's a speech from Al Gore, and sometimes its the Mars Curiosity Rover landing.
You should go have a listen.
In the final episode recap, Yitbarek walks us through ""how open source fuels some of humankind's grandest projects.""
She starts with the Mars Curiosity Rover robot explorer, which has it's own Twitter account with 4 million followers. The account recently tweeted an invitation to all to go build your own Rover, complete with open source instructions and code. The SGV Hack Group was the first to take up the challenge.
Wait, wait. Back up. The Rover is open source. All of the software and hardware?
Tom Soderstrom, IT Chief Technology Officer at Jet Propulsion Laboratory, explains: ""Why open source? For us, it was more difficult to make it open source. We did that so other schools could adopt it and build it. The bottom line is, when we release something as open source it's cleaner, it's tighter, it's better documented. It has to be built to be extended.""
Those are bold words from someone who should know.
""We look for, is there life out there? How did the universe originate? Where is it going? We're trying to find Earth 2.0 —earth-like planets that one day we could inhabit,"" says Tom Soderstrom. ""It's not meant to be a toy, it's meant to be something real.""
Tom goes on to explain that everything about the Rover is open source so that others will experiment with it and try new things. He's excited to see what people come up with. ""If I can think of something, someone's already doing it.""
To wrap up the episode, Yitbarek digs a little deeper into NASA's relationship with open source.
"
SysAdmin,"3 implications of serverless
Plus, when to go serverless and when to not?
If you strip away all of the modern conveniences and features that make up your internet experience today, what you're left with is the client-server model. This distributed network was what the internet was built on in the beginning, and that part hasn't changed. You could say, it is still serving us well.
So, when people talk about serverless, what does it mean? Well, it doesn't mean servers are GONE. Of course not: That ""client-server model"" is still the backbone of how things are getting done.
Serverless refers to a developer's ability to code, deploy, and create applications without having to know how to do the rest of it, like rack the servers, patch the operating system, and create container images.
Top 3 implications of serverless
People who might not have been before are becoming developers now. Why? They have to learn less of that kind of stuff and get to do more of the creative stuff.
Developers don't have the recreate the wheel. Why? They let serverless providers do what they do best: run and maintain the servers, patch the operating systems, and build containers.
Reality check: Someone on your team still has to think about the big picture, about operations. Why? Because when your server crashes or any decision at all needs to be made about the server-side of your project or product, your phone will ring and someone has to pick up. Preferably someone who knows the gameplan, your strategy for going serverless.
When to serverless and when to not?
So, serverless is great, right? But, the truth is it isn't always the right call. What are the factors that should be considered?
Cost
Scale
Time
Control
The last one, control, is where things get interesting. Projects like Apache OpenWhisk have developed processes and tools to make it possible for you, as a developer, to operate and control your serverless computing environments.
"
DevOps,"Infrastructure monitoring: Defense against surprise downtime
A strong monitoring and alert system based on open source tools prevents problems before they affect your infrastructure.
Infrastructure monitoring is an integral part of infrastructure management. It is an IT manager's first line of defense against surprise downtime. Severe issues can inject considerable downtime to live infrastructure, sometimes causing heavy loss of money and material.
Monitoring collects time-series data from your infrastructure so it can be analyzed to predict upcoming issues with the infrastructure and its underlying components. This gives the IT manager or support staff time to prepare and apply a resolution before a problem occurs.
A good monitoring system provides:
Measurement of the infrastructure's performance over time
Node-level analysis and alerts
Network-level analysis and alerts
Downtime analysis and alerts
Answers to the 5 W's of incident management and root cause analysis (RCA):
What was the actual issue?
When did it happen?
Why did it happen?
What was the downtime?
What needs to be done to avoid it in the future?
Building a strong monitoring system

There are a number of tools available that can build a viable and strong monitoring system. The only decision to make is which to use; your answer lies in what you want to achieve with monitoring as well as various financial and business factors you must consider.
While some monitoring tools are proprietary, many open source tools, either unmanaged or community-managed software, will do the job even better than the closed source options.
In this article, I will focus on open source tools and how to use them to create a strong monitoring architecture. 
Log collection and analysis
To say ""logs are helpful"" would be an understatement. Logs not only help in debugging issues; they also provide a lot of information to help you predict an upcoming issue. Logs are the first door to open when you encounter issues with software components.
Both Fluentd and Logstash can be used for log collection; the only reason I would choose Fluentd over Logstash is because of its independence from the Java process; it is written in C+ Ruby, which is widely supported by container runtimes like Docker and orchestration tools like Kubernetes.
Log analytics is the process of analyzing the log data you collect over time and producing real-time logging metrics. Elasticsearch is a powerful tool that can do just that.
Finally, you need a tool that can collect logging metrics and enable you to visualize the log trends using charts and graphs that are easy to understand. Kibana is my favorite option for that purpose.
Because logs can hold sensitive information, here are a few security pointers to remember:
Always transport logs over a secure connection.
The logging/monitoring infrastructure should be implemented inside the restricted subnet.
Access to monitoring user interfaces (e.g., Kibana and Grafana) should be restricted or authenticated only to stakeholders.
Node-level metrics
Not everything is logged!
Yes, you heard that right: Logging monitors a software or a process, not every component in the infrastructure.
Operating system disks, externally mounted data disks, Elastic Block Store, CPU, I/O, network packets, inbound and outbound connections, physical memory, virtual memory, buffer space, and queues are some of the major components that rarely appear in logs unless something fails for them.
So, how could you collect this data?
Prometheus is one answer. You just need to install software-specific exporters on the virtual machine nodes and configure Prometheus to collect time-based data from those unattended components. Grafana uses the data Prometheus collects to provide a live visual representation of your node's current status.
If you are looking for a simpler solution to collect time-series metrics, consider Metricbeat, Elastic.io's in-house open source tool, which can be used with Kibana to replace Prometheus and Grafana.
Alerts and notifications
You can't take advantage of monitoring without alerts and notifications. Unless stakeholders—no matter where they are in this big, big world—receive a notification about an issue, there's no way they can analyze and fix the issue, prevent the customer from being impacted, and avoid it in the future.
Prometheus, with predefined alerting rules using its in-house Alertmanager and Grafana, can send alerts based on configured rules. Sensu and Nagios are other open source tools that offer alerting and monitoring services.
The only problem people have with open source alerting tools is that the configuration time and the process sometimes seem hard, but once they are set up, these tools function better than proprietary alternatives.
However, open source tools' biggest advantage is that we have control over their behavior.
Monitoring workflow and architecture
A good monitoring architecture is the backbone of a strong and stable monitoring system. It might look something like this diagram.
In the end, you must choose a tool based on your needs and infrastructure. The open source tools discussed in this article are used by many organizations for monitoring their infrastructure and blessing it with high uptime.

"
DevOps$Agile,"Introducing the Small Scale Scrum framework
This agile approach is designed for small teams whose members play multiple roles.
Scrum is a leading candidate for the implementation of Small Scale Agile for many reasons, including its popularity, developers’ preferences, high success rates for scrum adoption and project deliveries, and strong principles and values including focus, courage, openness, commitment, and respect.
Small Scale Scrum can be best described as “a people-first framework defined by and for small teams (a maximum of three people) and supporting planning, developing, and delivering production-quality software solutions.” The proposed framework centers around the concept of team members occupying multiple roles on any project.
Small Scale Scrum is valuable due to its strong support for the small, distributed teams found in organizations all over the world. Small teams need new ways to meet customers’ continuously growing expectations for rapid delivery and high quality, and Small Scale Scrum’s guidelines and principles help address this challenge.
The Project backlog is a list of everything known to be required in the project. It is created before any development work begins and is maintained by a development team. The project backlog contains development tasks and software requirements. It replaces the traditional product and sprint backlogs.
The sprint planning meeting is time-boxed (i.e., set in advance for a specified amount of time) and focused on planning work for the upcoming sprint. The meeting is run by the development team and advance planning is required to keep it concise. Sprint planning is value-based. At this point, requirements that will be worked on in the upcoming sprint should be clear and contain approved acceptance criteria. Capacity, typically measured as velocity, is not used; instead, the team has to take an educated guess. Velocity emerges only after three or more sprints, which is usually after the Small Scale Scrum projects are finished.
The three-people team is comprised of the development team and the project manager (most of the time). The development team is expected to be involved in gathering and clarifying business requirements, doing development work, performing quality testing, and releasing software to the customer.
The proof of concept/demo is a realization of the small work completed in the sprint to showcase progress in development. Any deviations or incomplete work are discussed during the POC/demo.
The sprint review meeting is organized and run by the development team and project manager (if involved in the project) and attended by the customer or customer team. The sprint review is time-boxed and contains demonstrations of sprint work and a short outline of work completed/not completed, bugs raised, and known and/or descoped issues (if any). Customer feedback is gathered at the end of the demonstration and incorporated into future sprints. Very little (if any) preparation is required to keep the meeting short and structured.
The sprint retrospective meeting is organized and run by the development team and project manager (if involved) and may be attended by the customer and/or customer team. The sprint retrospective is time-boxed and requires no advance preparation. Due to the small size of the development team and the sprint builds (with a smaller number of features delivered), this meeting is relatively short. The sprint retrospective takes place after the sprint review and before the next sprint planning meeting.
The sprint review, sprint retrospective, and sprint planning may be combined into a single meeting which we term the sprint termination, lasting approximately 90 minutes. These meetings are concise, structured (thanks to advance preparation), and must not include any unnecessary/unrelated discussions.
The first/final release is the project’s end result. The development team runs the tests, verifies the completed work, confirms fixes, and signs off on the release build.
The Scrum Guide’s recommended time-box guidelines per ceremony should be considered valid for small scale-scrum ceremonies.
A related version of this article was originally published on Medium and is republished with permission.
"
DevOps$GitLab,"Getting started as a GitLab contributor
GitLab's open culture makes it very welcoming to contributors. Here's what I learned when I decided to become one.
GitLab's open culture is one of its strongest assets and the main reason I use GitLab in DevOps transformations. The community edition's code is open source and the paid version makes its source code available for contributions. These are valuable factors rooted in the company culture its CEO has diligently maintained over the years. It doesn't hurt that its tools are great, too.
I believe GitLab's sales and marketing team is the best of any company out there. They have included me as a user, customer, and friend over the last few years, and they are genuine and caring people. This was underscored last year when I wanted to contribute a feature, and GitLab's team went to extraordinary lengths to help me succeed. Here's the story of making my first contribution to GitLab.
How it started
During a review of our application code, the National Association of Insurance Commissioners (NAIC) found that many applications either didn't include our license or the license was no longer current. We needed to make it easy for everyone in the company to access our proprietary code license and provide an auditing function to ensure they all stayed current.
So, I went spelunking through GitLab issues in search of something related. I found a related issue that was a couple of years old and didn't look like it would be worked on anytime soon. I mentioned to Joe Drumtra, our GitLab account manager, that I'd like to contribute the feature, and he got very excited.
Joe immediately scheduled time with me to speak to the product owner, James Ramsay, and the lead engineer, Douwe Maan, on that team. He also added me to a channel in their Slack for better collaboration. We had the first meeting a week after I mentioned we'd like to contribute.
Our first meeting
Aaron Blythe from the NAIC joined me on the first call with James and Douwe from GitLab. They recorded our first conversation on Zoom and shared it publicly afterward. They also posted a follow-up in the Slack channel detailing what we discussed.


During that first call, James walked us through how licenses worked in the version we'd be modifying, then we discussed the vision for the feature we'd be implementing. This was to ensure we were all on the same page and everyone understood what was happening. This was really helpful for Aaron and me so we could look back as we went through coding the new feature.
They also brought in a designer, Sarah Vesselov, to ensure the UX would be right. They were migrating to a new look where we'd be working, so our timing would be an issue. Sarah also made some great mockups for the result and posted them on the GitLab issue.
During the call, Douwe navigated Aaron and me through the Ruby on Rails code and made some of the initial changes. He served as an experienced guide to make us more familiar with the code, show us the parts we would likely need to change, and help us avoid some pitfalls. Thanks to his thorough explanation, we were able to complete a lot in just an hour. 
After looking through the code, I felt a little nervous. Just a few years ago, I was writing Ruby all day every day, and these weren't simple scripts. I was contributing to complex systems with race conditions and functionality, like creating a directed acyclic graph of tasks and executing them. However, none of that was in Ruby on Rails, and Ruby on Rails looked barely like the Ruby I remember.
Programming (really) in the open
Aaron and I decided to live stream our endeavor on our DevOps KC YouTube channel. We did two live streams, two hours each, with the two of us pair programming. We did a little bit of prep before the first live stream, mostly just looking around and trying to understand the code.
We began the first live stream with a brief refresher of the issue. I struggled a bit to figure out how to use Google Hangouts. (In fact, the whole live stream might be proof that I need to remove Ruby from the languages I say I know, and I might not even understand technology all that well.) Mostly we stumbled around testing out different solutions and hoping one would work. Most people just call this ""programming.""
During this session, Aaron taught me a lot I didn't know about Ruby on Rails and reminded me of some things I'd forgotten about Ruby language idioms. I believe Aaron also learned some things from me. During the first live stream, we created several pieces of the feature and knew where we needed to find a solution.
We learned even more in the second live stream. For example, we discovered that managing environments and dependencies is hard, but there are tools that can help. Also, it can be hard to manage projects when you have a directory structure that includes separate projects within projects. Context matters a lot! We also found out that dropdowns are really hard to work with.
The finishing touches
The feature wasn't done after the live streams, but we were close. I decided to finish it up on my own because Aaron needed to focus on some other work. I also struggled to find time to complete it, so I was thankful when someone from GitLab reached out and asked if I needed help. I finished what I could and submitted a merge request. (If you want to learn a lot very quickly, it's worth reading through the merge request discussions.)
Douwe and Nick Thomas from GitLab quickly hopped on and gave me a lot of guidance to finish some of it. We figured out there was a pretty obscure problem with the dropdown code that was causing most of our problems with it. Without Nick's help, I don't think I ever would have found the problem or the code that needed to be updated.
Nick put in a lot of changes to get it ready to merge, and I helped out where I could to finish it up over the next couple of weeks.
The feature was finally completed and merged back into the Master branch for GitLab's 11.3 release. However, this wasn't the only feature related to our work that was completed. Nick also implemented other file types like .gitlab-ci.yml, .gitignore, and Dockerfile as instance-level templates.
Why did I do it?
So, why did the chief architect of a nonprofit organization decide to contribute to GitLab? There are three main reasons:
First, it gave me a chance to show that I don't know everything and I'm fallible. We are building a culture of blamelessness and servant leadership at NAIC, and this was the fastest way I could think of to walk the walk.
Second, we wanted to make it part of our culture to contribute back to open source projects. It's hard for someone to say, ""you aren't allowed to contribute,"" when the chief architect does it live on YouTube and tells everyone else they can. Also, we are funded by consumers, so one of my goals is for NAIC to operate as lean as possible and contribute back to the community as much as we can.
Finally, I wanted to contribute this specific feature because it's something our legal department needed. Making it easier to stay in compliance helps our legal team, which helps us build a strong relationship of trust and mutual benefit that's important as we move into new areas with legal ambiguity. Also, I knew no one would work on this issue in the near future, a lot of customers wanted it, and the other file types could be implemented quickly once we established the framework.
Getting started as a contributor
Deciding whether to contribute to GitLab and open source or source-available code might involve your legal team and company policies, so consult with them first. Even contributing to source-available code (like we did) is valuable. We changed code not only in the commercial version but also in the open source version.
If you want a feature that GitLab isn't prioritizing, I recommend contributing it back by contacting someone through the issue or connecting with your account manager. They're likely willing to talk with you to make sure you're going in the right direction, and they may even agree to collaborate as they did with us.
Before you start on the fun part of working on a contribution, check out GitLab's contributing guide. If you want to contribute, but don't have a specific feature in mind, an easy way to get started is to search GitLab issues and sort by weight. The lower-weight values are expected to be the easiest. GitLab also tries to make contributing easier with some automated magic around changelog updates and other routine administrative work, but it can be more confusing if you're not aware those things exist.
"
DevOps$Agile,"4 steps to becoming an awesome agile developer
There's no magical way to do it, but these practices will put you well on your way to embracing agile in application development, testing, and debugging.
Enterprises are rushing into their DevOps journey through agile software development with cloud-native technologies such as Linux containers, Kubernetes, and serverless. Continuous integration helps enterprise developers reduce bugs, unexpected errors, and improve the quality of their code deployed in production.
However, this doesn't mean all developers in DevOps automatically embrace agile for their daily work in application development, testing, and debugging. There is no magical way to do it, but the following four practical steps and best practices will put you well on your way to becoming an awesome agile developer.
Start with design thinking agile practices

There are many opportunities to learn about using agile software development practices in your DevOps initiatives. Agile practices inspire people with new ideas and experiences for improving their daily work in application development with team collaboration. More importantly, those practices will help you discover the answers to questions such as: Why am I doing this? What kind of problems am I trying to solve? How do I measure the outcomes?
A domain-driven design approach will help you start discovery sooner and easier. For example, the Start At The End practice helps you redesign your application and explore potential business outcomes—such as, what would happen if your application fails in production? You might also be interested in Event Storming for interactive and rapid discovery or Impact Mapping for graphical and strategic design as part of domain-driven design practices.
Use a predictive approach first
In agile software development projects, enterprise developers are mainly focused on adapting to rapidly changing app development environments such as reactive runtimes, cloud-native frameworks, Linux container packaging, and the Kubernetes platform. They believe this is the best way to become an agile developer in their organization. However, this type of adaptive approach typically makes it harder for developers to understand and report what they will do in the next sprint. Developers might know the ultimate goal and, at best, the app features for a release about four months from the current sprint.
In contrast, the predictive approach places more emphasis on analyzing known risks and planning future sprints in detail. For example, predictive developers can accurately report the functions and tasks planned for the entire development process. But it's not a magical way to make your agile projects succeed all the time because the predictive team depends totally on effective early-stage analysis. If the analysis does not work very well, it may be difficult for the project to change direction once it gets started.
To mitigate this risk, I recommend that senior agile developers increase the predictive capabilities with a plan-driven method, and junior agile developers start with the adaptive methods for value-driven development.
Continuously improve code quality
Don't hesitate to engage in continuous integration (CI) practices for improving your application before deploying code into production. To adopt modern application frameworks, such as cloud-native architecture, Linux container packaging, and hybrid cloud workloads, you have to learn about automated tools to address complex CI procedures.
Jenkins is the standard CI tool for many organizations; it allows developers to build and test applications in many projects in an automated fashion. Its most important function is detecting unexpected errors during CI to prevent them from happening in production. This should increase business outcomes through better customer satisfaction.
Automated CI enables agile developers to not only improve the quality of their code but their also application development agility through learning and using open source tools and patterns such as behavior-driven development, test-driven development, automated unit testing, pair programming, code review, and design pattern.
Never stop exploring communities
Never settle, even if you already have a great reputation as an agile developer. You have to continuously take on bigger challenges to make great software in an agile way.
By participating in the very active and growing open source community, you will not only improve your skills as an agile developer, but your actions can also inspire other developers who want to learn agile practices.
How do you get involved in specific communities? It depends on your interests and what you want to learn. It might mean presenting specific topics at conferences or local meetups, writing technical blog posts, publishing practical guidebooks, committing code, or creating pull requests to open source projects' Git repositories. It's worth exploring open source communities for agile software development, as I've found it is a great way to share your expertise, knowledge, and practices with other brilliant developers and, along the way, help each other.
Get started
These practical steps can give you a shorter path to becoming an awesome agile developer. Then you can lead junior developers in your team and organization to become more flexible, valuable, and predictive using agile principles.
"
DevOps,"What does DevOps mean to you?
6 experts break down DevOps and the practices and philosophies key to making it work.
It's said if you ask 10 people about DevOps, you will get 12 answers. This is a result of the diversity in opinions and expectations around DevOps—not to mention the disparity in its practices.
To decipher the paradoxes around DevOps, we went to the people who know it the best—its top practitioners around the industry. These are people who have been around the horn, who know the ins and outs of technology, and who have practiced DevOps for years. Their viewpoints should encourage, stimulate, and provoke your thoughts around DevOps.
What does DevOps mean to you?

Let's start with the fundamentals. We're not looking for textbook answers, rather we want to know what the experts say.
In short, the experts say DevOps is about principles, practices, and tools.
Ann Marie Fred, DevOps lead for IBM Digital Business Group's Commerce Platform, says, ""to me, DevOps is a set of principles and practices designed to make teams more effective in designing, developing, delivering, and operating software.""
According to Daniel Oh, senior DevOps evangelist at Red Hat, ""in general, DevOps is compelling for enterprises to evolve current IT-based processes and tools related to app development, IT operations, and security protocol.""
Brent Reed, founder of Tactec Strategic Solutions, talks about continuous improvement for the stakeholders. ""DevOps means to me a way of working that includes a mindset that allows for continuous improvement for operational performance, maturing to organizational performance, resulting in delighted stakeholders.""
Many of the experts also emphasize culture. Ann Marie says, ""it's also about continuous improvement and learning. It's about people and culture as much as it is about tools and technology.""
To Dan Barker, chief architect and DevOps leader at the National Association of Insurance Commissioners (NAIC), ""DevOps is primarily about culture. … It has brought several independent areas together like lean, just culture, and continuous learning. And I see culture as being the most critical and the hardest to execute on.""
Chris Baynham-Hughes, head of DevOps at Atos, says, ""[DevOps] practice is adopted through the evolution of culture, process, and tooling within an organization. The key focus is culture change, and the key tenants of DevOps culture are collaboration, experimentation, fast-feedback, and continuous improvement."" 
Geoff Purdy, cloud architect, talks about agility and feedback ""shortening and amplifying feedback loops. We want teams to get feedback in minutes rather than weeks.""
But in the end, Daniel nails it by explaining how open source and open culture allow him to achieve his goals ""in easy and quick ways. In DevOps initiatives, the most important thing for me should be open culture rather than useful tools, multiple solutions.""
What DevOps practices have you found effective?
The most effective practices cited by the experts are pervasive yet disparate.
According to Ann Marie, ""some of the most powerful [practices] are agile project management; breaking down silos between cross-functional, autonomous squads; fully automated continuous delivery; green/blue deploys for zero downtime; developers setting up their own monitoring and alerting; blameless post-mortems; automating security and compliance.""
Chris says, ""particular breakthroughs have been empathetic collaboration; continuous improvement; open leadership; reducing distance to the business; shifting from vertical silos to horizontal, cross-functional product teams; work visualization; impact mapping; Mobius loop; shortening of feedback loops; automation (from environments to CI/CD).""
Brent supports ""evolving a learning culture that includes TDD [test-driven development] and BDD [behavior-driven development] capturing of a story and automating the sequences of events that move from design, build, and test through implementation and production with continuous integration and delivery pipelines. A fail-first approach to testing, the ability to automate integration and delivery processes and include fast feedback throughout the lifecycle.""
Geoff highlights automated provisioning. ""Picking one, automated provisioning has been hugely effective for my team. More specifically, automated provisioning from a versioned Infrastructure-as-Code codebase.""
Dan uses fun. ""We do a lot of different things to create a DevOps culture. We hold 'lunch and learns' with free food to encourage everyone to come and learn together; we buy books and study in groups.""
How do you motivate your team to achieve DevOps goals?

Daniel emphasizes ""automation that matters. In order to minimize objection from multiple teams in a DevOps initiative, you should encourage your team to increase the automation capability of development, testing, and IT operations along with new processes and procedures. For example, a Linux container is the key tool to achieve the automation capability of DevOps.""
Geoff agrees, saying, ""automate the toil. Are there tasks you hate doing? Great. Engineer them out of existence if possible. Otherwise, automate them. It keeps the job from becoming boring and routine because the job constantly evolves.""
Dan, Ann Marie, and Brent stress team motivation.
Dan says, ""at the NAIC, we have a great awards system for encouraging specific behaviors. We have multiple tiers of awards, and two of them can be given to anyone by anyone. We also give awards to teams after they complete something significant, but we often award individual contributors.""
According to Ann Marie, ""the biggest motivator for teams in my area is seeing the success of others. We have a weekly playback for each other, and part of that is sharing what we've learned from trying out new tools or practices. When teams are enthusiastic about something they're doing and willing to help others get started, more teams will quickly get on board.""
Brent agrees. ""Getting everyone educated and on the same baseline of knowledge is essential ... assessing what helps the team achieve [and] what it needs to deliver with the product owner and users is the first place I like to start.""
Chris recommends a two-pronged approach. ""Run small, weekly goals that are achievable and agreed by the team as being important and [where] they can see progress outside of the feature work they are doing. Celebrate wins and visualize the progress made.""
How do DevOps and agile work together?
This is an important question because both DevOps and agile are cornerstones of modern software development.
DevOps is a process of software development focusing on communication and collaboration to facilitate rapid application and product deployment, whereas agile is a development methodology involving continuous development, continuous iteration, and continuous testing to achieve predictable and quality deliverables.
So, how do they relate? Let's ask the experts.
In Brent's view, ""DevOps != Agile, second Agile != Scrum. … Agile tools and ways of working—that support DevOps strategies and goals—are how they mesh together.""
Chris says, ""agile is a fundamental component of DevOps for me. Sure, we could talk about how we adopt DevOps culture in a non-agile environment, but ultimately, improving agility in the way software is engineered is a key indicator as to the maturity of DevOps adoption within the organization.""
Dan relates DevOps to the larger Agile Manifesto. ""I never talk about agile without referencing the Agile Manifesto in order to set the baseline. There are many implementations that don't focus on the Manifesto. When you read the Manifesto, they've really described DevOps from a development perspective. Therefore, it is very easy to fit agile into a DevOps culture, as agile is focused on communication, collaboration, flexibility to change, and getting to production quickly.""
Geoff sees ""DevOps as one of many implementations of agile. Agile is essentially a set of principles, while DevOps is a culture, process, and toolchain that embodies those principles.""
Ann Marie keeps it succinct, saying ""agile is a prerequisite for DevOps. DevOps makes agile more effective.""
Has DevOps benefited from open source? 

This question receives a fervent ""yes"" from all participants followed by an explanation of the benefits they've seen.
Ann Marie says, ""we get to stand on the shoulders of giants and build upon what's already available. The open source model of maintaining software, with pull requests and code reviews, also works very well for DevOps teams.""
Chris agrees that DevOps has ""undoubtedly"" benefited from open source. ""From the engineering and tooling side (e.g., Ansible), to the process and people side, through the sharing of stories within the industry and the open leadership community.""
A benefit Geoff cites is ""grassroots adoption. Nobody had to sign purchase requisitions for free (as in beer) software. Teams found tooling that met their needs, were free (as in freedom) to modify, [then] built on top of it, and contributed enhancements back to the larger community. Rinse, repeat.""
Open source has shown DevOps ""better ways you can adopt new changes and overcome challenges, just like open source software developers are doing it,"" says Daniel.
Brent concurs. ""DevOps has benefited in many ways from open source. One way is the ability to use the tools to understand how they can help accelerate DevOps goals and strategies. Educating the development and operations folks on crucial things like automation, virtualization and containerization, auto-scaling, and many of the qualities that are difficult to achieve without introducing technology enablers that make DevOps easier.""
Dan notes the two-way, symbiotic relationship between DevOps and open source. ""Open source done well requires a DevOps culture. Most open source projects have very open communication structures with very little obscurity. This has actually been a great learning opportunity for DevOps practitioners around what they might bring into their own organizations. Also, being able to use tools from a community that is similar to that of your own organization only encourages your own culture growth. I like to use GitLab as an example of this symbiotic relationship. When I bring [GitLab] into a company, we get a great tool, but what I'm really buying is their unique culture. That brings substantial value through our interactions with them and our ability to contribute back. Their tool also has a lot to offer for a DevOps organization, but their culture has inspired awe in the companies where I've introduced it.""
"
DevOps$Agile,"The Small Scale Agile Manifesto
These six values enhance agile methodologies to help smaller teams work more efficiently.
The “Agile Manifesto” is an umbrella term that describes and governs several lightweight and fuller agile methodologies for handling IT teams and projects. Scrum, Kanban, Lean Development, Crystal, and Extreme Programming (XP) are among the most popular and lightweight agile approaches.
While Small Scale Scrum fits into the Agile Manifesto, six of its additional values, described below, should complement and enhance agile for smaller teams.
Wide communication over narrow communication
Maintaining narrow communication with a project’s manager is essential, but wider communication offers more value. Wider communication includes all of a team’s stakeholders, including the product owner, ScrumMaster, and all team members. Excellence through the application of best principles in every team-to-team or team-to-customer communication is very important. Therefore, team members are encouraged to take time to prepare before meetings to best accommodate changes and ensure productive outcomes. Regularly using the team’s preferred communication channels quickly and effectively creates a welcoming environment.

Team feature delivery over individual responsibility
An individual team member’s responsibility for delivering single features is considered standard practice in software projects. Members of small teams, however, are expected to have much broader involvement in the project lifecycle, so they need to take mutual responsibility for delivering work items. In this approach, teamwork plays an important role, with team members working together as a single unit to ensure the project’s success from the bottom up.
This can be achieved with techniques including support for remote team members, fair workload, pair programming, code review, and shadowing. Remote team members work towards the common goal but are not geographically co-located; fair workload is to ensure that the team’s workload is divided fairly; pair programming is focused on writing and reviewing source code together in real time; code review, also known as peer review, is focused on viewing and reading source code after or as an interruption of implementation; shadowing is on-the-job learning and is commonly used for in-house training.
Quality delivery over speed of development
Rapid development and high-quality delivery are expectations in every customer engagement. While the speed of development is important, quality delivery has a much greater impact on the project’s continuous success. Investing time in quality development and testing by using quality-assurance techniques, tools, mentoring, and training can help the team continuously excel.
Multiple project responsibilities over fixed assignment
Fixed project responsibility where team members typically have one role, with the overall team containing enough skills to be self-sufficient for the roles to be filled, is a standard practice in agile, but the real value for small-scale teams comes from members taking additional roles (within reason). For example, an engineer may become the front-end developer, back-end developer, quality engineer, or user interface (UI) designer.
The idea behind this approach is to ensure that the small team is as self-sufficient as possible. For small teams to adopt multiple responsibilities, the workload must be fair and the project’s processes must be streamlined. This can be achieved by continuously reviewing and improving work conditions and simplifying processes to help the team focus on delivery. Coaching should be offered to give team members guidelines to help them improve their skills.
Accelerating innovation over marginal request-driven thinking
Request-driven thinking where specific business requests are followed strictly, is important in enterprise engagements, but innovation is what customers value the most. In such engagements, the customer is the only stakeholder, and their view and voice are followed to the letter. Planting innovation is critical to get the team and customers to think outside the project’s defined requirements so they can envision the final solution with the most creative and optimal architectures, requirements, and designs.
Customer growth over customer engagement
A successful customer engagement is very important to a project, but it’s not sufficient for building and maintaining a strong and successful relationship with the customer. For small teams, it’s important to approach business engagement from the customer’s business-success perspective. Growth or creation of business is the customer’s highest priority for a software solution; therefore, it should be the team’s highest priority.
"
DevOps$Automation,"How to create an automated calendar with Google Apps Script with open source on top
Get this single small script that does everything Zapier does and more.
Speaking at conferences can be hard. This tutorial aims to help you better manage the ""administrivia"" of submitting to calls for proposals (CFPs) and speaking at conferences.
I primarily speak about open source tools or open organization concepts, and I think it's really important to share with others. If you're interested in speaking, check out some of the conferences mentioned in this article, read this awesome resource, and feel free to contact me if you need any tips or help. It also might help to read some of these great tips for organizers, as they may help you better formulate a proposal that can get accepted.
In addition to being a speaker, I'm on the CFP review board for DevOpsDays KC, so I know a little bit about the topic curation side of the CFP process. Pro tip: We found this past year that someone was A/B testing their titles with us, and our ratings were affected by the title. The descriptions were identical.

Managing conference submissions
I'll be submitting to a lot of conferences both in and out of the tech industry. This will be a lot to maintain mentally. I've already spent weeks creating the proposals and making them just right, but how will I manage all my submissions? It would be great if everyone used papercall.io, because I'd just track them there. However, we all know there will never be a single standard. Some conferences have a dashboard to track your submission, others send an email, and others do nothing. The last ones are the hardest to track.
So, I decided to start with a simple Google Form and autogenerated Sheet. I added the types of information I thought I'd need at different stages after the submission. I added entries for event name, location, start and end dates, CFP end date, event and CFP URLs, speaker notification date, and a list of the talks I submitted. I will fill this out as I complete each CFP.
This was going to be the end of it, but I realized it would be pretty hard to track all of this throughout the year without something like a calendar. My first thought was to create something from scratch in JavaScript, but that was just the developer in me. After I subdued him, I decided there was probably an easier way.
The googling commenced. Zapier has really good SEO in its tutorials, so I decided to use it to connect my Google Sheet to my Google Calendar. I made a couple of special calendars—one for tracking conferences where I haven't been accepted and another for the ones that have accepted my proposals. I also created a separate sheet so I could add extra columns, like the acceptance status and which talks were accepted.
The integration with Zapier was pretty easy once I got the hang of it. I configured a complex Zap that created different types of calendar events for each stage of the process, including a separate event for the CFP end date. It was working great, then I realized I was using a trial version and it would all be gone in two weeks. Not good! This isn't my job, so I don't want to pay for something I can build (and write about).
So, the googling began again. I found Google Apps Script (GAS) and some Google Sheets and Calendar API libraries. As I started to work with some of the libraries, I realized that it was going to take a decent amount of effort just to get authentication working properly, and then I'd have to run it somewhere.
Even though GAS isn't open source, I decided to use it and make everything I do on top of it completely open and free. I started by navigating to my Sheet and clicking on Tools and then Script editor This opened an IDE where I could get started.
There was quite a bit of trial and error, but I ultimately ended up with a single small script that does everything Zapier does and more. It's also completely free for my limited needs. It runs every five minutes and is completely idempotent. I also don't have to run the script—Google does that somewhere without me knowing anything else.
Building my calendar automation
Before we get started, there is a caveat to this automation. I'm not great with Google Sheets, so I couldn't figure out how to update the Results sheet when the Responses sheet is updated. This means I have to go in and copy the rule in the current cells to any new cells that need to be filled in.
Also, I recommend this Chrome extension that allows you to use a GitLab repo for your scripts. I learned about it at the end of my development then used it immediately to put my finished code into a versioned repo. You can now fork from my repo to get my script into your GAS editor from a repo you control.
Now I'll walk through the code to give you a better understanding of what's happening and why before showing the finished product.
function createTrigger() {
  ScriptApp.newTrigger('updateEvents')
      .timeBased()
      .everyMinutes(5)
      .create();
}
This first function creates a trigger that calls the updateEvents method. This script is set to run every five minutes, which is fine, as it is idempotent. Be careful not to edit the script in a way that removes the idempotency, which means you'll need to delete hundreds of events. The good news is that you can quickly delete all events on the calendar and revert to the idempotent script to return to a fully functional state in just minutes. You can also reverse the check in the deleteEvent function and the script will delete one entry per event each time the script runs. This helped me slowly walk back a mistake during development.
// Deletes an event if it exists
function deleteEvent(event) {
  if (typeof event != 'undefined') {
    Logger.log(""Deleting event %s"", event.getTitle())
    event.deleteEvent()
  }
}
Let's skip the next function and head to the third function, deleteEvent. This function takes a CalendarEvent as an argument, checks if it exists, and deletes it if it does. This is just a small helper script that cleaned up my code quite a bit.
// Get Sheet and both Calendars.
  var sheet = SpreadsheetApp.openByUrl(""https://xxxxxx"").getSheetByName(""Results"")
  var submittedCalendar = CalendarApp.getCalendarById(""xxxxxxx"")
  var acceptedCalendar = CalendarApp.getCalendarById(""xxxxxx"")
Now, let's tackle the big function, updateEvents. This performs all the logic. First, I need to establish authentication into each calendar and the sheet I'm using. This was very quick and easy, and it's handled in the first execution of the script. You can get the sheet URL by navigating to your Sheet and grabbing the URL from the address bar. You can get the calendar ID by following these instructions. I suggest not using the name for either, as you may want to change it later and forget to update references.
  // Get data from Sheet
  var data = sheet.getDataRange().getValues()
  
  var events = []
  
  // This loop parses the data from the spreadsheet and adds it to an array
  for (var i = 1; i < data.length; i++) {
    // Skips blank rows
    if (data[i][0] == """") {
      break
    }
    
    // Gets the speaker notification date if one exists or sets it to the start date of the conference
    var speakerNotificationDate = new Date(data[i][4].getTime() + 2.88e7)
    if (data[i][5] != '') {
      speakerNotificationDate = new Date(data[i][5].getTime() + 2.88e7)
    }
    
    // Uses the first row, headers, as keys with the values being assigned to each. Then the object is pushed onto the array.
    var event = {}
    event[data[0][0]] = data[i][0]
    event[data[0][1]] = data[i][1]
    event[data[0][2]] = data[i][2]
    event[data[0][3]] = data[i][3]
    event[data[0][4]] = new Date(data[i][4].getTime() + 2.88e7) // Update the time to 8 a.m.
    event[data[0][5]] = speakerNotificationDate
    event[data[0][6]] = new Date(data[i][6].getTime() + 2.88e7) // Update the time to 8 a.m.
    event[data[0][7]] = new Date(data[i][7].getTime() + 7.2e7) // Update the time to 8 p.m.
    event[data[0][8]] = data[i][8]
    event[data[0][9]] = data[i][9]
    event[data[0][10]] = data[i][10]
    events.push(event)
  }
Once it's done authenticating, I continue getting all the data from the Sheet. The script checks to ensure there's an event name; if not, it won't continue processing that row and will go to the next entry. Each row is parsed into an object with the key being the column header and the value being the value from the cell in the row being processed. I do a little cleanup on dates and empty data to ensure the rest of the script functions properly.
I experienced some issues with how my events were displaying on the calendar, and I found it best to set the start time to 8 am and the end time to 8 pm. Avoiding the hour of 0000 seemed to be ideal in solving a lot of the problems I was experiencing. The script also ensures the speaker notification date is entered or it sets the date to be the start date of the event..
if (events[i][""Accepted""] == ""Yes"") {
      if (typeof acceptedCalendar.getEvents(events[i][""Start date""], events[i][""End date""], {search: events[i][""Event Name""] + "" - "" + events[i][""Talks Accepted""]})[0] == 'undefined') {
        acceptedCalendar.createEvent(events[i][""Event Name""] + "" - "" + events[i][""Talks Accepted""], events[i][""Start date""], events[i][""End date""], {location: events[i][""Location""], description: ""Event URL: "" + events[i][""Event URL""] + ""\n\nCFP URL: "" + events[i][""CFP URL""] + ""\n\nTalks Accepted: "" + events[i][""Talks Accepted""]}).setColor(CalendarApp.EventColor.GREEN).addEmailReminder(40320).addPopupReminder(40320)
        Logger.log(""Created accepted event on accepted calendar"")
      }
      
      if (typeof submittedCalendar.getEvents(events[i][""Start date""], events[i][""End date""], {search: events[i][""Event Name""] + "" - Accepted""})[0] == 'undefined') {
        submittedCalendar.createEvent(events[i][""Event Name""] + "" - Accepted"", events[i][""Start date""], events[i][""End date""], {location: events[i][""Location""], description: ""Event URL: "" + events[i][""Event URL""] + ""\n\nCFP URL: "" + events[i][""CFP URL""] + ""\n\nTalks Accepted: "" + events[i][""Talks Accepted""]}).setColor(CalendarApp.EventColor.GREEN)
        Logger.log(""Created accepted event on submitted calendar"")
      }
      
      deleteEvent(submittedCalendar.getEvents(events[i][""Start date""], events[i][""End date""], {search: events[i][""Event Name""] + "" - Submitted""})[0])
      deleteEvent(submittedCalendar.getEvents(events[i][""Start date""], events[i][""End date""], {search: events[i][""Event Name""] + "" - Rejected""})[0])
    }
Then the script loops through each event to add it to the appropriate calendar. You can see an example of adding and removing an event above. There's an attribute on the table called Accepted that is filled in with the word Yes if the conference has accepted any of my submissions. If that is true, then two calendar events are created, one on each calendar.
Since I have two separate calendars, I have an easy view of all the conferences where I'm speaking. For the Accepted Calendar, I first do a search for the event on the calendar and return the first item from the array that Google Calendar responds with. I'm assuming that if there is an event, then it is the one I want. I'm using a very specific search that includes the accepted talks, so it should only match the event. If there's no event, then I create a new event with all the pertinent information, ensuring the color is green (since that's the color of events on the Accepted Calendar). I also have notifications set for four weeks in advance of an event. This helps me ensure my presentation is ready and all of my travel is booked.
An event is added to the Submitted Calendar in exactly the same way, however, "" - Accepted"" is added to the conference title, since it's on a calendar with a lot of other similar events. Then I delete any events with the "" - Submitted"" and "" - Rejected"" keywords in the title because they won't be needed anymore.
If the Accepted attribute is set to No, I create a rejected event on the Submitted Calendar. I also need to delete the other three possible calendar entries. Then if there is any other value or no value in the Accepted column, I can assume it has been submitted but not accepted or rejected yet. So, I create a "" - Submitted"" event and delete any possible rejected or accepted events.
  if (typeof submittedCalendar.getEvents(events[i][""CFP end date""], events[i][""Start date""], {search: events[i][""Event Name""] + "" - CFP""})[0] == 'undefined') {
      submittedCalendar.createAllDayEvent(events[i][""Event Name""] + "" - CFP"", events[i][""CFP end date""], {location: events[i][""Location""], description: ""CFP End Date: "" + events[i][""CFP end date""] + ""\n\nSpeaker Notification Date: "" + events[i][""Speaker notification date""] + ""\n\nEvent URL: "" + events[i][""Event URL""] + ""\n\nCFP URL: "" + events[i][""CFP URL""] + ""\n\nTalks Submitted: "" + events[i][""Talks Submitted""]}).setColor(CalendarApp.EventColor.YELLOW).addEmailReminder(10080).addPopupReminder(10080)
      Logger.log(""Created CFP event"")
    }
Last, I ensure there's an entry for each CFP. This is created on the published end date for the CFP, the color is changed to yellow, and notifications are set for seven days prior to the deadline. Although I've likely already submitted to the conference, sometimes something happens where I want to change a submission or I decide I want to submit another proposal. Also, I can add an event without submitting any talks (yet) so I'll be reminded to submit before the CFP closes. Currently, adding another talk to an event won't update that event unless I delete the event manually in my calendar. This is a pretty rare occurrence, so I'm not too concerned about it.
Getting all of this together shouldn't take long, and I think it's really going to help me manage my schedule better. Here's a look at my April calendar, with one accepted conference so far and several that are waiting for the CFP to close. Also, you can see that there are two colors for Submitted (an artifact of using Zapier first). This shows that the search is working correctly and only matching on the title. I could change this if I want to spend time on it, but it doesn't bother me and shouldn't affect you.
If you want to contribute back any enhancements to this project, please feel free to submit a Merge Request on GitLab.
Writing winning proposals
I've spoken at conferences for the last few years. My first year I just threw out a bunch of random proposals that were custom for each event and tried not to duplicate any. This meant I barely applied to any, and I was basically rejected by all of them. I think I spoke at one conference that year.
I decided that wasn't optimal. None of my proposals were all that good because I couldn't spend much time on any one of them. I also became aware that this is a lot like sales, in that I'm going to mostly be rejected. So, I decided to create three proposals, slightly customize them for each CFP, and customize the actual presentation after the proposal has been accepted. I change my presentations quite a bit throughout the year, based on each conference's focus and time constraints, so I basically never present the same thing twice.
I think I spoke at 10 conferences that year, including All Day DevOps, Jenkins World, All Things Open, and LISA (to name some of the bigger ones). There were also many smaller conferences with great organizers and communities that I'd recommend, like GlueCon, WeRISE Atlanta, BSidesKC, and RevolutionConf. You can see the slides or watch videos through my website.
My strategy of using just three proposals allowed me to submit to many more conferences to find the best fits for myself and the conferences' organizers. It worked pretty well, and I probably submitted to 50 conferences. Keeping track of them all was not happening. I was constantly worried I'd miss an email about getting accepted or accept to two conferences that happen at the same time. Luckily, I don't think either happened.
In 2018, I decided not to travel as much, so I only spoke at a couple of conferences, and they were all bespoke submissions because I wasn't submitting to many. I had closer to a 50% acceptance rate.
Moving into 2019, I plan to present a lot more, so I need a strategy. I've again decided to go with a small set of talks. This year it will be seven proposals, due to a lot of new knowledge I've picked up working at the NAIC and helping the organization through a large technology and culture transformation. I also hope to speak about risk in a way that includes the entire product pipeline and how it can be used to fund transformations, which is convenient as my new role is Chief Architect of RSA Archer.
"
DevOps$Kubernetes,"Create a Kubernetes cron job in OKD
Get started with OKD, a Kubernetes distribution formerly known as OpenShift Origin.
It can be daunting to get started with Kubernetes and OKD (a Kubernetes distribution formerly known as OpenShift Origin). There are a lot of concepts and components to take in and understand. This tutorial walks through creating an example Kubernetes cron job that uses a service account and a Python script to list all the pods in the current project/namespace. The job itself is relatively useless, but this tutorial introduces many parts of the Kubernetes & OKD infrastructure. Also, the Python script is a good example of using the OKD REST API for cluster-related tasks.
This tutorial covers several pieces of the Kubernetes/OKD infrastructure, including:
Service accounts and tokens
Role-based access control (RBAC)
Image streams
BuildConfigs
Source-to-image (S2I)
Jobs and cron jobs (mostly the latter)
Kubernetes' Downward API
The Python script and example OKD YAML files for this tutorial can be found on GitHub; to use them, just replace the value: https://okd.host:port line in cronJob.yml with the hostname of your OKD instance.
Prerequisites
Required software
An OKD or MiniShift cluster with the default S2I image streams installed
An integrated image registry
Optional software (for playing with the Python script)
Python3
OpenShift Python client
Kubernetes Python client
To install the modules, run:
pip3 install --user openshift kubernetes
Authentication
The Python script will use the service account API token to authenticate to OKD. The script expects an environment variable pointing to the OKD host where it will connect:
HOST: OKD host to connect to (e.g., https://okd.host:port)
OKD will also need the token to be created automatically for the service account that will run the cron job pods (see ""Set how environment variables and tokens are used"" below).
Process
Setting up this sync is a good learning experience—since you need to set up a number of Kubernetes and OKD tasks, you get a good feel for its various features.
The general process is:
Create a new project
Create a Git repository with the Python script in it
Create a service account
Grant RBAC permissions to the service account
Set how environment variables and tokens are used
Create an image stream to accept the images created by the BuildConfig
Create a BuildConfig to turn the Python script into an image/image stream
Build the image
Create the cron job
1. Create a new project
Create a new project in OKD for this exercise:
oc new-project py-cron
Depending on how your cluster is set up, you may need to ask your cluster administrator to create a new project for you.
2. Create a Git repository with the Python script
Clone or fork this repo:
https://github.com/clcollins/openshift-cronjob-example.git
You can also reference it directly in the code examples below. This will serve as the repository where the Python script will be pulled and built into the final running image.
3. Create a service account
A service account is a non-user account that can be associated with resources, permissions, etc., within OKD. For this exercise, you must create a service account to run the pod with the Python script in it, authenticate to the OKD API via token auth, and make REST API calls to list all the pods.
Since the Python script will query the OKD REST API to get a list of pods in the namespace, the service account will need permissions to list pods and namespaces. Technically, one of the default service accounts automatically created in a project—system:serviceaccount:default:deployer—already has these permissions. However, this exercise will create a new service account to explain service account creation and RBAC permissions.
Create a new service account by entering:
oc create serviceaccount py-cron
This creates a service account named, appropriately, py-cron. (Technically, it's system:serviceaccounts:py-cron:py-cron, or the ""py-cron"" service account in the ""py-cron"" namespace). The account automatically receives two secrets: an OKD API token and credentials for the OKD Container Registry. The API token will be used in the Python script to identify the service account to OKD for REST API calls.
The tokens associated with the service account can be viewed with the command:
oc describe serviceaccount py-cron
4. Grant RBAC permissions for the service account
OKD and Kubernetes use RBAC (role-based access control) to allow fine-grained control of who can do what in a complicated cluster. In RBAC:
Permissions are based on verbs and resources (e.g., create group, delete pod, etc.)
Sets of permissions are grouped into roles or ClusterRoles, the latter being, predictably, cluster-wide.
Roles and ClusterRoles are associated with (or bound to) groups and service accounts (or, if you want to do it all wrong, individual users) by creating RoleBindings or ClusterRoleBindings.
Groups, service accounts, and users can be bound to multiple roles.
For this exercise, create a role within the project, grant the role permissions to list pods and projects, and bind the py-cron service account to the role:
oc create role pod-lister --verb=list --resource=pods,namespaces
oc policy add-role-to-user pod-lister --role-namespace=py-cron system:serviceaccounts:py-cron:py-cron
Note that --role-namespace=py-cron has to be added to prevent OKD from looking for ClusterRoles.
Verify the service account has been bound to the role:
oc get rolebinding | awk 'NR==1 || /^pod-lister/'
NAME         ROLE                 USERS     GROUPS   SERVICE ACCOUNTS   SUBJECTS
pod-lister   py-cron/pod-lister                      py-cron
5. Set how environment variables and tokens are used
The tokens associated with the service account and various environment variables are referenced in the Python script as py-cron API token.
The API token automatically created for the py-cron service account is mounted by OKD into any pod the service account is running. This token is mounted to a specific path in every container in the pod:
/var/run/secrets/kubernetes.io/serviceaccount/token
The Python script reads this file and uses it to authenticate to the OKD API to manage the groups.
HOST environment variable: The HOST environment variable is specified in the cron job definition and contains the OKD API hostname in the format: https://okd.host:port.
NAMESPACE environment variable: The NAMESPACE environment variable is referenced in the cron job definition and uses the Kubernetes Downward API to dynamically populate the variable with the name of the project where the cron job pod is running.
6. Create an image stream
An image stream is a collection of images, in this case, created by the BuildConfig builds, and an abstraction layer between images and Kubernetes objects that allows them to reference the image stream rather than the image directly.
Before a newly built image can be pushed to an image stream, the stream must already exist. The easiest way to create a new, empty stream is with the oc command-line command:
oc create imagestream py-cron
7. Create a BuildConfig
The BuildConfig is the definition of the entire build process—the act of taking input parameters and code and turning them into an image.
The BuildConfig for this exercise will use the source-to-image (S2I) build strategy, using the Red Hat-provided Python S2I image, and adding the Python script to it where the requirements.txt is parsed and those modules are installed. This results in a final Python-based image with the script and the required Python modules to run it.
The important pieces of the BuildConfig are: .spec.output, .spec.source, and .spec.strategy.
.spec.output
The output section of the BuildConfig describes what to do with the build's output. In this case, the BuildConfig outputs the resulting image as an image stream tag (e.g., py-cron:1.0) that can be used in the deploymentConfig to reference the image.
These are probably self-explanatory.
spec:
  output:
    to:
      kind: ImageStreamTag
      name: py-cron:1.0
.spec.source
The source section of the BuildConfig describes where the content of the build comes from. In this case, it references the Git repository where the Python script and its supporting files are kept.
Most of these are self-explanatory as well.
spec:
  source:
    type: Git
    git:
      ref: master
      uri: https://github.com/clcollins/openshift-cronjob-example.git
.spec.strategy
The strategy section of the BuildConfig describes the build strategy to use, in this case, the source (i.e., S2I) strategy. The .spec.strategy.sourceStrategy.from section defines the public Python 3.6 image stream that exists in the default OpenShift namespace for anyone to use. This image stream contains S2I builder images that take Python code as input, install the dependencies listed in any requirements.txt files, then output a finished image with the code and requirements installed.
strategy:
  type: Source
  sourceStrategy:
    from:
      kind: ImageStreamTag
      name: python:3.6
      namespace: openshift
The complete BuildConfig for this example looks like the YAML below. Substitute your Git repo and create the BuildConfig with the oc command:
oc create -f <path.to.buildconfig.yaml>
The YAML:
---
apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  labels:
    app: py-cron
  name: py-cron
spec:
  output:
    to:
      kind: ImageStreamTag
      name: py-cron:1.0
  runPolicy: Serial
  source:
    type: Git
    git:
      ref: master
      uri: https://github.com/clcollins/openshift-cronjob-example.git
  strategy:
    type: Source
    sourceStrategy:
      from:
        kind: ImageStreamTag
        name: python:3.6
        namespace: openshift
8. Build the image
Most of the time, it would be more efficient to add a webhook trigger to the BuildConfig to allow the image to be automatically rebuilt each time code is committed and pushed to the repo. For this exercise, however, the image build will be kicked off manually whenever the image needs to be updated.
A new build can be triggered by running:
oc start-build BuildConfig/py-cron
Running this command outputs the name of a build; for example:
build.build.openshift.io/py-cron-1 started
The build's progress can be followed by watching the logs:
oc logs -f build.build.openshift.io/py-cron-1
When the build completes, the image will be pushed to the image stream listed in the .spec.output section of the BuildConfig.
9. Create the cron job
The Kubernetes cron job object defines the cron schedule and behavior as well as the Kubernetes job that is created to run the actual sync.
The important parts of the cron job definition are: .spec.concurrencyPolicy, .spec.schedule, and .spec.JobTemplate.spec.template.spec.containers.
.spec.concurrencyPolicy
The concurrencyPolicy field of the cron job spec is an optional field that specifies how to treat concurrent executions of a job that are created by this cron job. In this exercise, it will replace an existing job that may still be running if the cron job creates a new job.
Note: Other options are to allow concurrency (multiple jobs running at once) or forbid concurrency (new jobs are skipped until the running job completes).
.spec.schedule
The schedule field of the cron job spec is (unsurprisingly) a Vixie cron-format schedule. At the time(s) specified, Kubernetes will create a job, as defined in the JobTemplate spec below.
spec:
  schedule: ""*/5 * * * *""
.spec.JobTemplate.spec.template.spec.containers
The cron job spec contains a JobTemplate spec and a template spec, which in turn contains a container spec. All of these follow the standard spec for their type, i.e., the .spec.containers section is just a normal container definition you might find in any other pod definition.
The container definition for this example is a straightforward container definition that uses the environment variables discussed above.
The only important part is: 
.spec.JobTemplate.spec.template.spec.containers.ServiceAccountName
This section sets the service account created earlier, py-cron, as the account running the containers. This overrides the default deployer service account.
The complete OKD cron job for py-cron looks like the YAML below. Substitute the URL of the OKD cluster API and create the cron job by using the following oc command: 
oc create -f <path.to.cronjob.yaml>
The YAML:
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  labels:
    app: py-cron
  name: py-cron
spec:
  concurrencyPolicy: Replace
  failedJobsHistoryLimit: 1
  JobTemplate:
    metadata:
      annotations:
        alpha.image.policy.openshift.io/resolve-names: '*'
    spec:
      template:
        spec:
          containers:
          - env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: HOST
              value: https://okd.host:port
            image: py-cron/py-cron:1.0
            imagePullPolicy: Always
            name: py-cron
          ServiceAccountName: py-cron
          restartPolicy: Never
  schedule: ""*/5 * * * *""
  startingDeadlineSeconds: 600
  successfulJobsHistoryLimit: 3
  suspend: false
Looking around
Once the cron job has been created, its component can be viewed with the oc get cronjob command. This shows a brief description of the cron job, its schedule and last run, and whether it is active or paused:
oc get cronjob py-cron
NAME      SCHEDULE    SUSPEND   ACTIVE    LAST SCHEDULE   AGE
py-cron   */5 * * * *   False     0         1m              7d
As mentioned, the cron job creates Kubernetes jobs to do the work whenever the scheduled time passes. The oc get jobs command lists the jobs that have been created by the cron job, the number of desired jobs (in this case, just one job per run), and whether the job was successful:
oc get jobs
NAME                 DESIRED   SUCCESSFUL   AGE
py-cron-1544489700   1         1            10m
py-cron-1544489760   1         1            5m
py-cron-1544489820   1         1            30s
The jobs are pods, which can be seen with the oc get pods command. In this example, you can see both the job pods (named after the job that created them, such as Job ""py-cron-1544489760"" created pod ""py-cron-1544489760-xl4vt""). There are also build pods, or the pods that built the container image, as described in the BuildConfig above.
oc get pods
NAME                       READY     STATUS      RESTARTS   AGE
py-cron-1-build            0/1       Completed   0          7d
py-cron-1544489760-xl4vt   0/1       Completed   0          10m
py-cron-1544489820-zgfg8   0/1       Completed   0          5m
py-cron-1544489880-xvmsn   0/1       Completed   0          44s
py-cron-2-build            0/1       Completed   0          7d
py-cron-3-build            0/1       Completed   0          7d
Finally, because this example is just a Python script that connects to OKD's REST API to get information about pods in the project, oc get logs can be used verify that the script is working by getting the logs from the pod to see the output of the script written to standard output:
oc logs py-cron-1544489880-xvmsn
---> Running application from Python script (app.py) ...
ResourceInstance[PodList]:
  apiVersion: v1
  items:
  - metadata:
      annotations: {openshift.io/build.name: py-cron-1, openshift.io/scc: privileged}
      creationTimestamp: '2018-12-03T18:48:39Z'
      labels: {openshift.io/build.name: py-cron-1}
      name: py-cron-1-build
      namespace: py-cron
      ownerReferences:
      - {apiVersion: build.openshift.io/v1, controller: true, kind: Build, name: py-cron-1,
        uid: 0c9cf9a8-f72c-11e8-b217-005056a1038c}

<snip>
In summary
This tutorial showed how to create a Kubernetes cron job with OKD, a Kubernetes distribution formerly called OpenShift Origin. The BuildConfig described how to build the container image containing the Python script that calls the OKD REST API, and an image stream was created to describe the different versions of the image built by the builds. A service account was created to run the container, and a role was created to allow the service account to get pod information from OKD's REST API. A RoleBinding associated the role with the service account. Finally, a cron job was created to run a job—the container with the Python script—on a specified schedule.

"
containers$Buildah,"The second part of a container image is a JSON file that describes the contents of the rootfs. It contains fields like the command to run the container, the entrypoint, the environment variables required to run the container, the working directory of the container, etc. Basically this JSON file allows the developer of the container image to describe how the container image is expected to be used. The fields in this JSON file have been standardized in the OCI Image Format specification.
The rootfs and the JSON file then get tar'd together to create an image bundle that is stored in a container registry. To create a layered image, you install more software into the rootfs and modify the JSON file. Then you tar up the differences of the new and the old rootfs and store that in another image tarball. The second JSON file refers back to the first JSON file via a checksum.

Many years ago, Docker introduced Dockerfile, a simplified scripting language for building container images. Dockerfile was great and really took off, but it has many shortcomings that users have complained about. For example:

Dockerfile encourages the inclusion of tools used to build containers inside the container image. Container images do not need to include yum/dnf/apt, but most contain one of them and all their dependencies.

Each line causes a layer to be created. Because of this, secrets can mistakenly get added to container images. If you create a secret in one line of the Dockerfile and delete it in the next, the secret is still in the image.

One of my biggest complaints about the ""container revolution"" is that six years since it started, the only way to build a container image was still with Dockerfiles. Lots of tools other than docker build have appeared besides Buildah, but most still deal only with Dockerfile. So users continue hacking around the problems with Dockerfile.

Note that umoci is an alternative to docker build that allows you to build container images without Dockerfile.

Our goal with Buildah was to build a simple tool that could just create a rootfs directory on disk and allow other tools to populate the directory, then create the JSON file. Finally, Buildah would create the OCI image and push it to a container registry where it could be used by any container engine, like Docker, Podman, CRI-O, or another Buildah.

Buildah also supports Dockerfile, since we know the bulk of people building containers have created Dockerfiles.

Using Buildah directly

Lots of people use Buildah directly. A cool feature of Buildah is that you can script up the container build directly in Bash.

The example below creates a Bash script called myapp.sh, which uses Buildah to pull down the Fedora image, and then uses dnf and make on a machine to install software into the container image rootfs, $mnt. It then adds some fields to the JSON file using buildah config and commits the container to a container image myapp. Finally, it pushes the container image to a container registry, quay.io. (It could push it to any container registry.) Now this OCI image can be used by any container engine or Kubernetes.

cat myapp.sh
#!/bin/sh
ctr=$(buildah from fedora)
mnt=($buildah mount $ctr)
dnf -y install --installroot $mnt httpd
make install DESTDIR=$mnt myapp
rm -rf $mnt/var/cache $mnt/var/log/*
buildah config --command /usr/bin/myapp -env foo=bar --working-dir=/root $ctr
buildah commit $ctr myapp
buildah push myapp http://quay.io/username/myapp
To create really small images, you could replace fedora in the script above with scratch, and Buildah will build a container image that only has the requirements for the httpd package inside the container image. No need for Python or DNF.

Podman's relationship to Buildah

With Buildah, we have a low-level tool for building container images. Buildah also provides a library for other tools to build container images. Podman was designed to replace the Docker command line interface (CLI). One of the Docker CLI commands is docker build. We needed to have podman build to support building container images with Dockerfiles. Podman vendored in the Buildah library to allow it to do podman build. Any time you do a podman build, you are executing Buildah code to build your container images. If you are only going to use Dockerfiles to build container images, we recommend you only use Podman; there's no need for Buildah at all.

Other tools using the Buildah library

Podman is not the only tool to take advantage of the Buildah library. OpenShift 4 Source-to-Image (S2I) will also use Buildah to build container images. OpenShift S2I allows developers using OpenShift to use Git commands to modify source code; when they push the changes for their source code to the Git repository, OpenShift kicks off a job to compile the source changes and create a container image. It also uses Buildah under the covers to build this image.

Ansible-Bender is a new project to build container images via an Ansible playbook. For those familiar with Ansible, Ansible-Bender makes it easy to describe the contents of the container image and then uses Buildah to package up the container image and send it to a container registry.

We would love to see other tools and languages for describing and building a container image and would welcome others use Buildah to do the conversion.

Problems with rootless

Buildah works fine in rootless mode. It uses user namespace the same way Podman does. If you execute

$ buildah bud --tag myapp -f Dockerfile .
$ buildah push myapp http://quay.io/username/myapp
in your home directory, everything works great.

However, if you execute the script described above, it will fail!

The problem is that, when running the buildah mount command in rootless mode, the buildah command must put itself inside the user namespace and create a new mount namespace. Rootless users are not allowed to mount filesystems when not running in a user namespace.

When the Buildah executable exits, the user namespace and mount namespace disappear, so the mount point no longer exists. This means the commands after buildah mount that attempt to write to $mnt will fail since $mnt is no longer mounted.

How can we make the script work in rootless mode?

Buildah unshare

Buildah has a special command, buildah unshare, that allows you to enter the user namespace. If you execute it with no commands, it will launch a shell in the user namespace, and your shell will seem like it is running as root and all the contents of the home directory will seem like they are owned by root. If you look at the owner or files in /usr, it will list them as owned by nfsnobody (or nobody). This is because your user ID (UID) is now root inside the user namespace and real root (UID=0) is not mapped into the user namespace. The kernel represents all files owned by UIDs not mapped into the user namespace as the NFSNOBODY user. When you exit the shell, you will exit the user namespace, you will be back to your normal UID, and the home directory will be owned by your UID again.

If you want to execute the myapp.sh command defined above, you can execute buildah unshare myapp.sh and the script will now run correctly.

Conclusion

Building and running containers in unprivileged environments is now possible and quite useable. There is little reason for developers to develop containers as root. 

If you want to use a traditional container engine, and use Dockerfile's for builds, then you should probably just use Podman. But if you want to experiment with building container images in new ways without using Dockerfile, then you should really take a look at Buildah."
Kubernetes$datascience,"Why data scientists love Kubernetes
Kubernetes' features that streamline the software development workflow also support the data science workflow.
Let's start with an uncontroversial point: Software developers and system operators love Kubernetes as a way to deploy and manage applications in Linux containers. Linux containers provide the foundation for reproducible builds and deployments, but Kubernetes and its ecosystem provide essential features that make containers great for running real applications, like:
Continuous integration and deployment, so you can go from a Git commit to a passing test suite to new code running in production
Ubiquitous monitoring, which makes it easy to track the performance and other metrics about any component of a system and visualize them in meaningful ways
Declarative deployments, which allow you to rely on Kubernetes to recreate your production environment in a staging environment
Flexible service routing, which means you can scale services out or gradually roll updates out to production (and roll them back if necessary)

What you may not know is that Kubernetes also provides an unbeatable combination of features for working data scientists. The same features that streamline the software development workflow also support a data science workflow! To see why, let's first see what a data scientist's job looks like.

A data science project: predicting customer churn
Some people define data science broadly, including machine learning (ML), software engineering, distributed computing, data management, and statistics. Others define the field more narrowly as finding solutions to real-world problems by combining some domain expertise with machine learning or advanced statistics. We're not going to commit to an explicit definition of data scientist in this article, but we will tell you what data scientists might aim to do in a typical project and how they might do their work.
Consider a problem faced by any business with subscribing customers: Some might not renew. Customer churn detection seeks to proactively identify customers who are likely to not renew their contracts. Once these customers are identified, the business can choose to target their accounts with particular interventions (for example, sales calls or discounts) to make them less likely to leave. The overall churn-prevention problem has several parts: predicting which customers are likely to leave, identifying interventions that are likely to retain customers, and prioritizing which customers to target given a limited budget for interventions. A data scientist could work on any or all of these, but we'll use the first one as our running example.
The first part of the problem to solve is identifying an appropriate definition for ""churn"" to incorporate into a predictive model. We may have an intuitive definition of what it means to lose a customer, but a data scientist needs to formalize this definition, say, by defining the churn prediction problem as: ""Given this customer's activity in the last 18 months, how likely are they to cancel their contract in the next six?»
The data scientist then needs to decide which data about a customer's activity the model should consider—in effect, fleshing out and formalizing the first part of the churn definition. Concretely, a data scientist might consider any information available about a customer's actual use of the company's products over the historical window, the size of their account, the number of customer-service interactions they've had, or even the tone of their comments on support tickets they've filed. The measurements or data that our model considers are called features.
With a definition of churn and a set of features to consider, the data scientist can then begin exploratory analysis on historical data (that includes both the feature set and the ultimate outcome for a given customer in a given period). Exploratory analysis can include visualizing combinations of features and seeing how they correlate with whether a customer will churn. More generally, this part of the process seeks to identify structure in the historical data and whether it is possible to find a clean separation between retained customers and churning customers based on the data characterizing them.
For some problems, it won't be obvious that there's structure in the data—in these cases, the data scientist will have to go back to the drawing board and identify some new data to collect or perhaps a novel way to encode or transform the data available. However, exploratory analysis will often help a data scientist identify the features to consider while training a predictive model, as well as suggest some ways to transform those data. The data scientist's next job is feature engineering: finding a way to transform and encode the feature data—which might be in database tables, on event streams, or in data structures in a general-purpose programming language—so that it's suitable for input to the algorithm that trains a model. This generally means encoding these features as vectors of floating-point numbers. Just any encoding won't do; the data scientist needs to find an encoding that preserves the structure of the features so similar customers map to similar vectors—or else the algorithm will perform poorly.

Only now is the data scientist ready to train a predictive model. For the problem of predicting whether a customer will churn, the model-training pipeline starts with labeled historical data about customers. It then uses the techniques developed in the feature-engineering process to extract features from raw data, resulting in vectors of floating-point numbers labeled with ""true"" or ""false"" and corresponding to customers that will or will not churn in the window of interest. The model-training algorithm takes this collection of feature vectors as input and optimizes a process to separate between true and false vectors in a way that minimizes error. The predictive model will ultimately be a function that takes a feature vector and returns true or false, indicating whether the customer corresponding to that vector is likely to churn or not.
At any point in this process, the data scientist may need to revisit prior phases—perhaps to refine a feature-engineering approach, to collect different data, or even to change the metric they are trying to predict. In this way, the data science workflow is a lot like the traditional software development lifecycle: problems discovered during implementation can force an engineer to change the design of an interface or choice of a data structure. These problems can even cascade all the way back to requirements analysis, forcing a broader rethinking of the project's fundamentals. Fortunately, Kubernetes can support the data scientist's workflow in the same way it can support the software development lifecycle.
Kubernetes for data science
Data scientists have many of the same concerns that software engineers do: repeatable experiments (like repeatable builds); portable and reproducible environments (like having identical setups in development, stage, and production); credential management; tracking and monitoring metrics in production; flexible routing; and effortless scale-out. It's not hard to see some of the analogies between things application developers do with Kubernetes and things data scientists might want to do:
Repeatable batch jobs, like CI/CD pipelines, are analogous to machine learning pipelines in that multiple coordinated stages need to work together in a reproducible way to process data; extract features; and train, test, and deploy models.
Declarative configurations that describe the connections between services facilitate creating reproducible learning pipelines and models across platforms.
Microservice architectures enable simple debugging of machine learning models within the pipeline and aid collaboration between data scientists and other members of their team.
Data scientists share many of the same challenges as application developers, but they have some unique challenges related to how data scientists work and to the fact that machine learning models can be more difficult to test and monitor than conventional services. We'll focus on one problem related to workflow.
Most data scientists do their exploratory work in interactive notebooks. Notebook environments, such as those developed by Project Jupyter, provide an interactive literate programming environment in which users can mix explanatory text and code; run and change the code; and inspect its output.
These properties make notebook environments wonderfully flexible for exploratory analysis. However, they are not an ideal software artifact for collaboration or publishing—imagine if the main way software developers published their work was by posting transcripts from interactive REPLs to a pastebin service.
Sharing an interactive notebook with a colleague is akin to sharing a physical one—there's some good information in there, but they have to do some digging to find it. And due to the fragility and dependency of a notebook on its environment, a colleague may see different output when they run your notebook—or worse: it may crash.
Kubernetes for data scientists
Data scientists may not want to become Kubernetes experts—and that's fine! One of the strengths of Kubernetes is that it is a powerful framework for building higher-level tools.
One such tool is the Binder service, which takes a Git repository of Jupyter notebooks, builds a container image to serve them, then launches the image in a Kubernetes cluster with an exposed route so you can access it from the public internet. Since one of the big downsides of notebooks is that their correctness and functionality can be dependent on their environment, having a high-level tool that can build an immutable environment to serve a notebook on Kubernetes eliminates a huge source of headaches.
It's possible to use the hosted Binder service or run your own Binder instance, but if you want a little more flexibility in the process, you can also use the source-to-image (S2I) workflow and tool along with Graham Dumpleton's Jupyter S2I images to roll your own notebook service. In fact, the source-to-image workflow is a great starting point for infrastructure or packaging experts to build high-level tools that subject matter experts can use. For example, the Seldon project uses S2I to simplify publishing model services—simply provide a model object to the builder, and it will build a container exposing it as a service.
A great thing about the source-to-image workflow is that it enables arbitrary actions and transformations on a source repository before building an image. As an example of how powerful this workflow can be, we've created an S2I builder image that takes as its input a Jupyter notebook that shows how to train a model. It then processes this notebook to identify its dependencies and extract a Python script to train and serialize the model. Given these, the builder installs the necessary dependencies and runs the script in order to train the model. The ultimate output of the builder is a REST web service that serves the model built by the notebook. You can see a video of this notebook-to-model-service S2I in action. Again, this isn't the sort of tool that a data scientist would necessarily develop, but creating tools like this is a great opportunity for Kubernetes and packaging experts to collaborate with data scientists.
Kubernetes for machine learning in production
Kubernetes has a lot to offer data scientists who are developing techniques to solve business problems with machine learning, but it also has a lot to offer the teams who put those techniques in production. Sometimes machine learning represents a separate production workload—a batch or streaming job to train models and provide insights—but machine learning is increasingly put into production as an essential component of an intelligent application.
The Kubeflow project is targeted at machine learning engineers who need to stand up and maintain machine learning workloads and pipelines on Kubernetes. Kubeflow is also an excellent distribution for infrastructure-savvy data scientists. It provides templates and custom resources to deploy a range of machine learning libraries and tools on Kubernetes.
Kubeflow is an excellent way to run frameworks like TensorFlow, JupyterHub, Seldon, and PyTorch under Kubernetes and thus represents a path to truly portable workloads: a data scientist or machine learning engineer can develop a pipeline on a laptop and deploy it anywhere. This is a very fast-moving community developing some cool technology, and you should check it out!
Radanalytics.io is a community project targeted at application developers, and it focuses on the unique demands of developing intelligent applications that depend on scale-out compute in containers. The radanalytics.io project includes a containerized Apache Spark distribution to support scalable data transformation and machine learning model training, as well as a Spark operator and Spark management interface. The community also supports the entire intelligent application lifecycle by providing templates and images for Jupyter notebooks, TensorFlow training and serving, and S2I builders that can deploy an application along with the scale-out compute resources it requires. If you want to get started building intelligent applications on OpenShift or Kubernetes, a great place to start is one of the many example applications or conference talks on radanalytics.io.

"
Kubernetes$ksonnet,"How Kubeflow is evolving without ksonnet
There are big differences in how open source communities handle change compared to closed source vendors.
Many software projects depend on modules that are run as separate open source projects. When one of those modules loses support (as is inevitable), the community around the main project must determine how to proceed.
This situation is happening right now in the Kubeflow community. Kubeflow is an evolving open source platform for developing, orchestrating, deploying, and running scalable and portable machine learning workloads on Kubernetes. Recently, the primary supporter of the Kubeflow component ksonnet announced that it would no longer support the software.
When a piece of software loses support, the decision-making process (and the outcome) differs greatly depending on whether the software is open source or closed source.
A cellphone analogy
To illustrate the differences in how an open source community and a closed source/single software vendor proceed when a component loses support, let's use an example from hardware design.
Suppose you buy cellphone Model A and it stops working. When you try to get it repaired, you discover the manufacturer is out of business and no longer offering support. Since the cellphone's design is proprietary and closed, no other manufacturers can support it.
Now, suppose you buy cellphone Model B, it stops working, and its manufacturer is also out of business and no longer offering support. However, Model B's design is open, and another company is in business manufacturing, repairing and upgrading Model B cellphones.
This illustrates one difference between software written using closed and open source principles. If the vendor of a closed source software solution goes out of business, support disappears with the vendor, unless the vendor sells the software's design and intellectual property. But, if the vendor of an open source solution goes out of business, there is no intellectual property to sell. By the principles of open source, the source code is available for anyone to use and modify, under license, so another vendor can continue to maintain the software.
How Kubeflow is evolving without ksonnet
The ramification of ksonnet's backers' decision to cease development illustrates Kubeflow's open and collaborative design process. Kubeflow's designers have several options, such as replacing ksonnet, adopting and developing ksonnet, etc. Because Kubeflow is an open source project, all options are discussed in the open on the Kubeflow mailing list. Some of the community's suggestions include:
Should we look at projects that are CNCF/Apache projects e.g. helm
I would opt for back to the basics. KISS. How about plain old jsonnet + kubectl + makefile/scripts ? Thats how e.g. the coreos prometheus operator does it. It would also lower the entry barrier (no new tooling) and let vendors of k8s (gke, openshift, etc) easily build on top of that.
I vote for using a simple, programmatic context, be it manual jsonnet + kubectl, or simple Python scripts + Python K8s client, or any tool be can build on top of these.
The members of the mailing list are discussing and debating alternatives to ksonnet and will arrive at a decision to continue development. What I love about the open source way of adapting is that it's done communally. Unlike closed source software, which is often designed by one vendor, the organizations that are members of an open source project can collaboratively steer the project in the direction they best see fit. As Kubeflow evolves, it will benefit from an open, collaborative decision-making framework.

"
Kubernetes$PostgreSQL,"How to run PostgreSQL on Kubernetes
Create uniformly managed, cloud-native production deployments with the flexibility to deploy a personalized database-as-a-service.
By running a PostgreSQL database on Kubernetes, you can create uniformly managed, cloud-native production deployments with the flexibility to deploy a personalized database-as-a-service tailored to your specific needs.
Using an Operator allows you to provide additional context to Kubernetes to manage a stateful application. An Operator is also helpful when using an open source database like PostgreSQL to help with actions including provisioning, scaling, high availability, and user management.
Let's explore how to get PostgreSQL up and running on Kubernetes.
Set up the PostgreSQL operator
The first step to using PostgreSQL with Kubernetes is installing an Operator. You can get up and running with the open source Crunchy PostgreSQL Operator on any Kubernetes-based environment with the help of Crunchy's quickstart script for Linux.
The quickstart script has a few prerequisites:
The Wget utility installed
kubectl installed
A StorageClass defined on your Kubernetes cluster
Access to a Kubernetes user account with cluster-admin privileges. This is required to install the Operator RBAC rules
A namespace to hold the PostgreSQL Operator
Executing the script will give you a default PostgreSQL Operator deployment that assumes dynamic storage and a StorageClass named standard. User-provided values are allowed by the script to override these defaults.
You can download the quickstart script and set it to be executable with the following commands:
wget https://raw.githubusercontent.com/CrunchyData/postgres-operator/master/examples/quickstart.sh
chmod +x ./quickstart.sh
Then you can execute the quickstart script:
./examples/quickstart.sh
After the script prompts you for some basic information about your Kubernetes cluster, it performs the following operations:
Downloads the Operator configuration files
Sets the $HOME/.pgouser file to default settings
Deploys the Operator as a Kubernetes Deployment
Sets your .bashrc to include the Operator environmental variables
Sets your $HOME/.bash_completion file to be the pgo bash_completion file
During the quickstart's execution, you'll be prompted to set up the RBAC rules for your Kubernetes cluster. In a separate terminal, execute the command the quickstart command tells you to use.
Once the script completes, you'll get information on setting up a port forward to the PostgreSQL Operator pod. In a separate terminal, execute the port forward; this will allow you to begin executing commands to the PostgreSQL Operator! Try creating a cluster by entering:
pgo create cluster mynewcluster
You can test that your cluster is up and running with by entering:
pgo test mynewcluster
You can now manage your PostgreSQL databases in your Kubernetes environment! You can find a full reference to commands, including those for scaling, high availability, backups, and more, in the documentation. 

"
DevOps$organizationalculture,"Why every dev team should adopt a DevOps culture in 2019
Adopting a DevOps culture can be the competitive advantage your organization needs.
In the race to digitization, development teams have become the backbone of many organizations; they and the operations teams are required to do more than ever before. Often these teams drive innovation and change at the same time they are maintaining legacy code and ensuring products or services always perform. Organizations that allow dev and ops teams to be flexible, fast-moving, and opportunistic are businesses that win, which is why so many organizations are prioritizing building a DevOps culture.
DevOps culture combines the development team with the operations team in an effort to create software faster and with fewer errors. DevOps culture is a big switch from traditional IT culture; rather than having the dev and ops teams working separately, DevOps culture facilitates communication between them by having them share common goals. It requires that each side understand the objectives and can work together to make the business successful.
The right culture in your DevOps team can pay huge dividends. A study of 4,600 IT teams reported that those adopting a DevOps culture deployed software 200 times more frequently than low-performing teams. They spent half as much time fixing security issues, achieved faster recovery times, and had three times lower change-failure rates. Nearly all organizations can find some benefit in adopting a DevOps culture, but what does this culture look like?
Measure, track accountability, and be transparent
At each step, it is critical to establish clear key performance indicators (KPIs). Team members should always know what is expected of them and how it fits into the total project. This transparency not only leads to better communication, but it can also benefit team accountability.
In the past, many companies tended to isolate workers. Project elements were only shared on a ""need-to-know"" basis. The best DevOps teams have moved past that. Everyone should know how systems work and how things fit together. Collaboration tools like Microsoft Azure DevOps Server, and open source alternatives, give teams a clear picture of project stages throughout planning, development, testing, and deployment.
Make sure each team member understands the common goal. The more team members understand their role and how it fits with others, the more efficient they will be. You want team members to exhibit shared accountability across function groups.
Automate what you can
By reducing manual and repetitive tasks, you will increase your development and testing speed while cutting down on errors.
A large part of DevOps culture is to automate when possible and standardize production platforms. Standardization and automation help make deployments more predictable and less prone to human errors. Automate anything you can as part of the workflow process, including scalable infrastructures, compliance, and continuous delivery workflows. For example, tools like Docker allow development teams to automate application deployments rapidly.
By reducing manual and repetitive tasks, you will increase your development and testing speed while cutting down on errors. Additionally, by eliminating tedious, repetitive tasks, you will also free up your team to work on higher-level tasks and reduce their fatigue and burnout.
Create a culture of collaboration
Adopting DevOps culture can be an opportunity to completely break down the walls between departments and work units. Sometimes the development team and operations team have different priorities, and that's OK, but businesses should work to create a bridge of communication between them.

In other IT cultures, software development is compartmentalized to achieve speed by isolating team members to parts of projects. The development team focuses on new, innovative software, while operations works to mitigate risk and maintain the system. These priorities can create clashes. DevOps culture works to bridge these priorities through early and constant communication. By bringing the teams together from the start of software development, they can identify risks, bugs, and errors before software is sent to the operations teams. No longer will teams wait until the end to bring it all together and then identify problems that could have been fixed earlier in the development cycle.
This collaborative approach means software can be developed faster and with fewer bugs. In addition to the culture shift, teams will need new DevOps development tools, like JFrog and Go registries, which allow for both compartmentalization and collaboration. Tools like these allow better visibility and communication between team members.
Having operations and development on the same team creates collaboration. In addition to fostering a better sense of team, a DevOps culture can also help individual team members grow their skills or show other talents. By combining dev and operations, team leaders may be able to identify how people think outside their specific area of expertise. Challenging team members to think beyond just their goals and approach problems collaboratively can help individuals add additional skills or insights to the overall process. This can help leaders find today's team members that can potentially become tomorrow's team leaders.
Set realistic, transparent goals
Eliminating separate teams empowers individuals to step in and help when a milestone is at risk.
There's nothing more frustrating for DevOps teams than unrealistic goals. If they feel what they are asked to do is impossible, they likely will not give you their best efforts. By having increased communication and visibility between teams, managers will be able to set realistic goals that can reasonably be accomplished.
Additionally, by working together and side-by-side, DevOps teams also understand how the whole team is performing. They are able to see where projects get delayed or if they are moving correctly through the process. Eliminating separate teams empowers individuals to step in and help when a milestone is at risk. By seeing the whole process, they are more invested and able to create positive change when needed and ensure even more projects are delivered on time.
Create a shared learning and continuous improvement environment
The best DevOps teams are agile and constantly looking for ways to get better. Part of continuous improvement means establishing formal and informal feedback loops. This feedback is critical across teams and disciplines. By getting regular feedback from production, development, design, and management, you can cut development time. It also empowers everyone across the DevOps team to recognize their role in the overall product.
People like to be part of successful teams that consistently meet and exceed goals. Success breeds success. At the same time, team members that don't feel challenged in their work or get recognition for their contribution are more likely to leave.
DevOps culture's varied benefits
Ultimately, fast, agile, innovative teams are vital to business success, and a DevOps culture can help drive business. DevOps cultures provide stronger communication, set unified and transparent goals, and establish realistic timelines. By encouraging automation and standardization of processes, DevOps also can produce faster software development and fewer errors. Finally, it can create happy and empowered team members who can contribute in new ways. Adopting a DevOps culture can be the competitive advantage your organization needs."
DevOps$CI/CD,"Automate user acceptance testing with your DevOps pipeline
Bring your acceptance testing process into your CI/CD pipeline with open source tools.
Acceptance testing, also called user acceptance testing (UAT), determines whether a system satisfies user needs, business requirements, and authorized entity criteria. The tests are repeated every time there's a new design when the application is developed through software development lifecycle (SDLC). In many companies, the Site Reliability Engineer (SRE) automates acceptance testing by building a continuous integration/continuous development (CI/CD) pipeline within a DevOps initiative.
A large number of open source tools are needed to create CI/CD pipelines for both cloud and on-premises infrastructures, so you need to design multiple layers, such as platform, framework, and tools, to achieve productive, effective management of the pipeline.
In this article, I will walk you through a DevOps scenario that integrates automated acceptance testing tools. This example will give you a complete picture of DevOps platform integration.
Build the DevOps platform
How often does an SRE or developer build apps and do acceptance testing? Every single day? Once a week in multiple environments? Previously, you had to install, configure, and manage those environments for each project repeatedly—boring but necessary tasks. However, building a DevOps platform in a Linux container makes it fundamentally easier, quicker, and more secure to do this daily work.
Using enterprise platform-as-a-service (PaaS) solutions based on the Kubernetes project is the most powerful, suitable, and effective way to build the DevOps platform by building source code, packaging container images, and running containers in enterprise production.
To begin this example, use Minishift to create a single-node cluster on OKD (the community distribution of Kubernetes that powers Red Hat OpenShift) locally:
$ brew cask install minishift
$ minishift start --vm-driver virtualbox
$ eval $(minishift oc-env) && eval $(minishift docker-env)
$ oc login $(minishift ip):8443 -u admin -p admin
Deploy the CI framework
Using the container platform, choose one of the popular CI frameworks, such as Jenkins, Travis CI, TeamCity, CircleCI, or Bamboo and deploy it using OpenShift Template, Operator, or Helm Charts. Jenkins is the oldest and most popular open source automation server to set up CI for any programming language environment.
Deploy the containerized Jenkins master server to Minishift via OpenShift Template:
$ oc project at-cicd --> Create an acceptance testing project
$ oc new-app jenkins-ephemeral --> Create a Jenkins Master Server
Integrate acceptance testing tools
Acceptance criteria mostly focuses on application functionalities in the graphical user interface (GUI), which are as known as UI/UX requirements. In this step, developers and the QA team must spend a lot of time validating entire test cases manually, based on the user requirements.
Selenium allows you to automate function tests on webpages using WebDriver scripts that execute test cases interactively, the same way a tester does it manually. To integrate the Selenium tool with the DevOps pipeline on Jenkins, create a Jenkinsfile to define the CI/CD pipeline that includes multiple steps for build, test, and promotion.
Create a similar BuildConfig that contains multiple stages (source clone, build and compile, acceptance test) and run the pipeline resource with Minishift commands:
$ oc create -f at-selenuim-pipeline.yaml --> Create a CI/CD pipeline with AT
$ oc start-build at-selenuim-pipeline --> Start the pipeline

##### Snippet of pipeline buildconfg yaml #####
kind: ""BuildConfig""
apiVersion: ""v1""
metadata:
  name: ""at-selenuim-pipeline""
spec:
  strategy:
    jenkinsPipelineStrategy:
      jenkinsfile: |-
        pipeline {
            agent any
            def mvnHome = tool 'Maven'
            stages {
                stage ('gitclone') {
                    steps {
                        git branch: 'master', url: ""Your Code in Git Url""
                    }
                }

                stage ('build') {
                    steps {
                        sh ""${mvnHome}/bin/mvn compile""
                    }
                }

                stage ('attest') {
                    steps {
                        sh 'mvn clean test -Dwebdriver.type=remote -Dwebdriver.url=http://localhost:4444/wd/hub -Dwebdriver.cap.browserName=chrome'
                    }
                }
            }
        }
        type: JenkinsPipeline
Conclusion
These practical steps create a shorter path to integrating automated acceptance testing tools in your DevOps platform and accelerating CI/CD capabilities within your team and organization. You can further tune the automation pipeline with other open source testing tools such as JMeter, Maven, and JUnit.

"
DevOps$psychology,"The psychology behind a blameless retrospective
By understanding human behavior, you can create safe and positive environments for your agile teams to reflect on and improve their work.
A retrospective is the act of dealing with past events and activities. The word comes from Latin, and it literally means ""to look back."" In the business world, a retrospective is a practice agile teams commonly use to reflect on how their work is done to improve how they do it so they continuously become better at it.
One of the Agile Manifesto's principles suggests all teams regularly reflect on how to become more effective. The main goals of a retrospective are to promote self-improvement, improve processes, and advance team members' skills.
The Retrospective Prime Directive says:
""Regardless of what we discover, we understand and truly believe that everyone did the best job they could, given what they knew at the time, their skills and abilities, the resources available, and the situation at hand.""
At first glance, this practice looks straightforward—but reality shows us it isn't. People naturally tend to make decisions based on their instincts instead of rationally evaluating a situation. When things go wrong, it is often hard to accept responsibility and not join the blame-game.
The psychology behind blame
As humans, we uncontrollably evaluate the behavior of others to explain the cause of the behavior and the events we witness. For example, is someone late because they are not able to manage their time? Or because of an emergency situation? Over 50 years ago, Austrian psychologist Fritz Heider introduced attribution theory, which says humans tend to see cause-and-effect relationships, even in situations where they do not exist. During a retrospective, it's natural to try to understand other people's behavior by attributing one or more causes to the specific behavior.
Next, naïve realism kicks in. We firmly believe our senses provide us with direct awareness of objects as they really are; however, our perception might be playing tricks on us. This is true for natural objects, as they obey the laws of physics, whether or not we observe them. But, our memory of events is shaped by our experiences and our beliefs.
What happens in the retrospective meeting when participants believe they perceive past events ""as they were"" rather than as a subjective construction and interpretation of reality? In these situations, we believe that rational people will have similar perceptions as ours, and folks with different views must be uninformed or biased. Therefore, it is good to be aware that it is always easier to blame someone else rather than to accept responsibility. As humans, we naturally opt for the option that requires less effort. Recognizing our contributions to a bad situation is harder than looking for others who could be responsible.
To help alleviate this pressure, it is critical to create a positive environment. The prime directive for holding a retrospective says for a retrospective to be effective and successful, it needs to be safe. Norman Kerth, author of Project Retrospectives: A Handbook for Team Reviews, explains that safety means all participants must feel secure within their community.

The 3 foundations of blameless retrospectives
To ensure your blameless retrospective truly is successful, make sure to build it on these foundational pillars.
1. Nominate a facilitator
A successful retrospective needs a facilitator who can ensure the group will achieve the goals of the meeting. A facilitator can help open up discussion by asking:
What didn't work?
What did everyone do and how did we do it?
What will we do differently next time?
To create safety, the meeting facilitator needs to actively invite input from and engage with all team members. The facilitator also needs to manage the meeting to avoid interruptions.
2. Everybody has a say, everybody has a voice
This is a foundation drawn from the principles of lean manufacturing. In the Toyota Production System, any employee can pull the Andon Cord. This action immediately interrupts work for everyone so the team can gather together to perform real-time root-cause analysis and quickly apply a solution. The only way the Andon Cord can work is when everyone feels empowered to use it.
The same goes for the retrospective. Team members are not only given permission to speak but should feel an obligation to flag issues when they occur. When teams are sharing bad news, it is even more important to create a feeling of safety and empowerment.
3. Embrace enjoyment

The foundation of play is ""enjoyment."" Laughter is often recognized as a fundamental sign of a safe connection inside a team. Therefore, creating the opportunity for fun will allow everyone to signal they feel safe. This may require you to try out some unconventional techniques. If you are looking for specific recommendations, check out FunRetro, an online open source retrospective tool for distributed teams, and the Realtime Retrospective exercise.
Master the art
A lot of deliberate practice is needed to master the art of blameless retrospective. It is worth investing the time to have regular retrospectives to create a feeling of safety and enable people to discuss their work openly.
""Building safety isn't the kind of skill you can learn in a robotic, paint-by-numbers sort of way. It's a fluid, improvisational skill—sort of like learning to pass a soccer ball to a teammate during a game.""
—The Culture Code, Daniel Coyle
Retrospective participants should be able to learn from the exercise, but the most important goal of regular retrospectives is to develop safety and maintain it continuously.
"
Linux,"10 moments that shaped Linux history
Linux has come a long way since 1991. These events mark its evolution.
In August 2018, Opensource.com posted a poll with seven options asking readers: What was the most important moment in the history of Linux? I thought I would expand on the list and present 10 moments that I think have played an important part in shaping the history of Linux.
1. Linus releases Linux
Linus Torvalds initially released Linux to the world in 1991 as a hobby. It didn't remain a hobby for long!
2. Linux distributions
In 1993, several Linux distributions were founded, notably Debian, Red Hat, and Slackware. These were important because they demonstrated Linux's gains in market acceptance and development that enabled it to survive the tumultuous OS wars, browser wars, and protocol wars of the 1990s. In contrast, many established, commercial, and proprietary products did not make it past the turn of the millennium!
3. IBM's big investment in Linux
In 2000, IBM announced it would invest US$1 billion dollars in Linux. In his CNN Money article about the investment, Richard Richtmyer wrote: ""The announcement underscores Big Blue's commitment to Linux and marks significant progress in moving the alternative operating system into the mainstream commercial market.""
4. Hollywood adopts Linux
In 2002, it seemed the entire Hollywood movie industry adopted Linux. Disney, Dreamworks, and Industrial Light & Magic all began making movies with Linux that year.
5. Linux for national security
In 2003, another big moment came with the US government's acceptance of Linux. Red Hat Linux was awarded the Department of Defense Common Operating Environment (COE) certification. This is significant because the government—intelligence and military agencies in particular—have very strict requirements for computing systems to prevent attacks and support national security. This opened the door for other agencies to use Linux. Later that year, the National Weather Service announced it would replace outdated systems with new computers running Linux.

6. The systems I managed
This ""moment"" is really a collection of my personal experiences. As my career progressed in the 2000s, I discovered several types of systems and devices that I managed were all running Linux. Some of the places I found Linux were VMware ESX, F5 Big-IP, Check Point UTM Edge, Cisco ASA, and PIX. This made me realize that Linux was truly viable and here to stay.
7. Ubuntu
In 2004, Canonical was founded by Mark Shuttleworth to provide an easy-to-use Linux desktop—Ubuntu Linux—based on the Debian distribution. I think Ubuntu Linux helped to expand the desktop Linux install base. It put Linux in front of many more people, from casual home users to professional software developers.
8. Google Linux
Google released two operating systems based on the Linux kernel: the Android mobile operating system in mid-2008 and Chrome OS, running on a Chromebook, in 2011. Since then, millions of Android mobile phones and Chromebooks have been sold.
9. The cloud is Linux
In the past 10 years or so, cloud computing has gone from a grandiose vision of computing on the internet to a reinvention of how we use computers personally and professionally. The big players in the cloud space are built on Linux, including Amazon Web Services, Google Cloud Services, and Linode. Even in cases where we aren't certain, such as Microsoft Azure, running Linux workloads is well supported.
10. My car runs Linux
And so will yours! Many automakers began introducing Linux a few years ago. This led to the formation of the collaborative open source project called Automotive Grade Linux. Major car makers, such as Toyota and Subaru, have joined together to develop Linux-based automotive entertainment, navigation, and engine-management systems.
Share your favorite
This is my subjective list pulled from archives of Linux articles and events throughout my career, so there may be other more notable moments that I am overlooking. Share in the comments. Also, the Linux history poll is still open for voting if you're interested."
Linux$art&design,"The state of Linux graphic design tools in 2019
Can open source software pass a professional designer's test for work-readiness?
Before I begin this test of Linux graphic design tools, I should admit two things up front. First, I am a designer, not a software developer. Second, although I try to incorporate open source methodologies and principles wherever I can, my field pretty much demands that I use Adobe software on a sticker-emblazoned MacBook Pro. (I know, hate me if you must.) For the purposes of this research project, however, I am running Fedora 29 on a repurposed Mac Mini.
The question I want to answer with this investigation isn't just how good is open source design software, but also could I use it to do my job every day?
What I expect from professional-grade design software
Design is more craft than art. Like a craftsperson, designers have to be flexible enough to accomplish a wide range of tasks, knowledgeable enough to know which tool is appropriate for which task, and thoughtful enough to leave space and breadcrumbs for the next worker down the line to make changes and perform maintenance without too much headache. Therefore, rather than ordering this list by title, let's segment the applications by task and see what open source design software works and what doesn't.
Assignment 1: Design a logo
A good logo typically has three features: It's clean and not too visually complex, color variation is kept to a minimum so we're not mortgaging the company to print stickers later, and it's scalable enough to work just as well on a 16px favicon as it does on a 10-foot hanging banner.
Logos are almost always drawn as vectors. The clean lines and scalability of vector graphics lend themselves to the needs of logo designers so perfectly that on the few occasions where someone isn't using a vector-based application, they really should be. Thankfully, vector graphics are also very forgiving for new and non-designers. If a line doesn't quite look right, it doesn't have to be redrawn, it can simply be changed.
Inkscape
Grade: A- (92/100)
License: GPLv2
Inkscape holds a place of honor in my memory for being the very first vector software I ever used. Being able to access it for free and run it on any operating system gave me the opportunity to use it in my early days. I must say that Inkscape has matured extremely well in the past few years. It has a fairly intuitive interface and simple controls. I would like to see some improvements in the color palette system, but overall there is very little you could accomplish in proprietary software that can't be matched in Inkscape with a little bit of persistence and finesse.
LibreOffice Draw
Grade: C- (71/100)
LIcense: Mozilla Public License 2.0
I went into this assignment hopeful for a better result; maybe I didn't study enough? Maybe I skipped too many lectures? But when I sat down for the lab, I really felt a little lost. LibreOffice Draw seems to me like a clone of the LibreOffice Impress slideshow app with a few words changed here and there (the tool-tip for new layers even calls them ""slides"") and the toolbar relocated from the top of the frame to the left. This is not to say it is necessarily a bad approach for a ""does-what-you-need"" diagram-drawing application, which is precisely how I imagine most people would use it.
If your goal is to generate some easy-to-read, end-user-editable assets that you can copy and paste into other LibreOffice applications, Draw might be perfect for you. However, like most designers, I'm particular about the finer points of visual design, therefore I prefer to do all of my work in my daily-use design tools and export higher-quality products for consumption by colleagues and stakeholders.
Assignment 2: Design an ad
If logos are a designer's bread and butter, then web ads are our toast and jam. Web ads can be done in vector-based software, but more often than not, they incorporate some photographic elements that are better handled by raster graphics applications. The ability to non-destructively manipulate imagery through layered editing and use fine type controls are paramount for this kind of task.
GIMP
Grade: A (96/100)
License: GPLv2
Much like Inkscape, GIMP was a foundational part of my exploration into digital visual arts. Up until my first experience with GIMP—on Mandrake Linux in the late '90s—my only experience with raster graphics had been with Microsoft Paint. Even then, GIMP was a more powerful tool then I fully understood, and although I still am not entirely clear on what exactly Wilber is, the software has definitely kept pace with other market-leading applications. GIMP is lacking some bleeding-edge features you'd find in other raster applications (like 3D modeling), but I'm not sure how many people use or need those things on a daily basis. If 3D modeling is necessary for a web ad, it's probably over-designed.
GIMP's interface is so thoughtfully laid out that moving from other applications shouldn't present too steep a learning curve. Good color tools, tight typography controls, and a comprehensive toolbar mean that GIMP is all aces in my book.
Assignment 3: Lay out a print publication
As time marches on, print design becomes more and more of a specialization for designers that requires its own set of tools. Desktop publishing is an understatement for everything involved with print design. Sure, we can throw down some paragraphs in 12pt Times New Roman and wrap them around some square images—and for the vast majority of people that is more than adequate. Print design is about unrelenting fastidiousness across a broad range of disciplines. A successful print designer understands typography (two full semesters of my college career alone), color theory, photography, illustration, human interaction (including reading and learning disabilities), and all the technologies and methodologies used by printers to bring their designs to life.
Dear reader, do you know that coder in your company who is kinda scary smart? The one who still remembers COBOL for some unknown reason and can spot your missing semicolon at a glance? You know the one who wrote a script to tell the coffee pot, which you didn't even know had a CPU let alone a network interface, to brew a fresh cup of coffee in the amount of time it takes to walk to the breakroom? If that person had gone to art school instead of Stanford, they would be the print designer.
Scribus
Grade: A+ (98/100)
License: GPLv2 or later
This was my first experience with Scribus, so I wasn't sure what to expect, but I found myself pleasantly surprised. The self-described ""open source desktop publishing"" tool is deceptively complex and powerful. Full-featured typography controls including overflowing textboxes and image-sensitive wrapping allow for beautiful and unique type treatments. Native image cropping and shape tools make it possible to visually enhance your copy with an editorial narrative. Support for traditional CMYK profiles and spot colors gives designers total control over the outcome of the final product. The contributors to Scribus were even thoughtful enough to include tools for color-blindness testing to make sure every design can be as inclusive as possible.
I have to admit, I am seriously impressed with Scribus.
Assignment 4: Wireframe a prototype
Oof. This one really fell short for me. The world is digital now. Interactive web apps, websites, and mobile applications are such a massive part of our lives that I purchased in a PopSocket literally just so that I might stop dropping my phone on my face at night. My dentist approves, but my wife misses the comedy.
Before the introduction of wireframe and prototyping tools, mockups were usually designed in raster graphics tools like GIMP or Photoshop. And while I could probably go back to doing some of my job this way, I would quickly become less popular among my stakeholders and engineering colleagues.
I'm sure there are plenty of perfectly good games, apps, and websites that have been developed without wireframes, but today, doing something right requires careful, in-depth planning. That means UX design, UI design, and a big ol' pile of interconnected wireframes.
Wireframing and prototyping tools are necessary to create virtually kickable prototypes that are visually developed enough to not frighten project managers and interactive enough to collaborate with engineers without wasting too much of their very expensive time. In the proprietary world, this means tools like SketchApp, InVision Studio, and Adobe XD. I couldn't locate an open source application that fit this need.
The final grade: B+
The lack of available wireframing and prototyping applications really brought down the average, but I'd still call it a successful exercise. As I mentioned at the beginning, design is a craft and it relies on collaboration. All of the tools I looked at—Inkscape, LibreDraw, GIMP, and Scribus—can run just as well on Windows or MacOS as they do on any Linux distribution. The ability to create robust artwork and share editable files with stakeholders and colleagues on the platform of their choice means that a serious argument could be made that these tools are even more versatile than their proprietary counterparts.
On a personal note, not only are GIMP and Inkscape the first real design tools I got to play with unsupervised, they opened up my mind to the possibility that design was a real job I could do someday. Without these open source software applications and the community contributors behind them, I wouldn't have this job as a UX designer for Red Hat today and I never would have left Ohio. So, I offer heartfelt gratitude to all of them. Even if all of these applications aren't perfect all the time, they're enough to inspire people to make cool new things, and if you ask this designer, making cool things is what being human is all about.
Extra credit
But what about (cough) running Adobe on Wine? (cough)
Yeah, I'm sure you can do it. But this isn't about breaking end user license agreements. I'm not about that life. This is about the efficacy of real, open source, design software running available on open source platforms, right now.
"
Linux$commandline,"Using the force at the Linux command line
Like the Jedi Force, -f is powerful, potentially destructive, and very helpful when you know how to use it.
Sometime in recent history, sci-fi nerds began an annual celebration of everything Star Wars on May the 4th, a pun on the Jedi blessing, ""May the Force be with you."" Although most Linux users are probably not Jedi, they still have ways to use the force. Of course, the movie might not have been quite as exciting if Yoda simply told Luke to type man X-Wing fighter or man force. Or if he'd said, ""RTFM"" (Read the Force Manual, of course).
Many Linux commands have an -f option, which stands for, you guessed it, force! Sometimes when you execute a command, it fails or prompts you for additional input. This may be an effort to protect the files you are trying to change or inform the user that a device is busy or a file already exists.

If you don't want to be bothered by prompts or don't care about errors, use the force!
Be aware that using a command's force option to override these protections is, generally, destructive. Therefore, the user needs to pay close attention and be sure that they know what they are doing. Using the force can have consequences!
Following are four Linux commands with a force option and a brief description of how and why you might want to use it.
cp
The cp command is short for copy—it's used to copy (or duplicate) a file or directory. The man page describes the force option for cp as:
-f, --force
       if an existing destination file cannot be opened, remove it
       and try again
This example is for when you are working with read-only files:
[alan@workstation ~]$ ls -l
total 8
-rw-rw---- 1 alan alan 13 May  1 12:24 Hoth
-r--r----- 1 alan alan 14 May  1 12:23 Naboo
[alan@workstation ~]$ cat Hoth Naboo 
Icy Planet

Green Planet
If you want to copy a file called Hoth to Naboo, the cp command will not allow it since Naboo is read-only:
[alan@workstation ~]$ cp Hoth Naboo 
cp: cannot create regular file 'Naboo': Permission denied
But by using the force, cp will not prompt. The contents and permissions of Hoth will immediately be copied to Naboo:
[alan@workstation ~]$ cp -f Hoth Naboo 
[alan@workstation ~]$ cat Hoth Naboo 
Icy Planet

Icy Planet

[alan@workstation ~]$ ls -l
total 8
-rw-rw---- 1 alan alan 12 May  1 12:32 Hoth
-rw-rw---- 1 alan alan 12 May  1 12:38 Naboo
Oh no! I hope they have winter gear on Naboo.
ln
The ln command is used to make links between files. The man page describes the force option for ln as:
-f, --force
       remove existing destination files
Suppose Princess Leia is maintaining a Java application server and she has a directory where all Java versions are stored. Here is an example:
leia@workstation:/usr/lib/java$ ls -lt
total 28
lrwxrwxrwx 1 leia leia   12 Mar  5  2018 jdk -> jdk1.8.0_162
drwxr-xr-x 8 leia leia 4096 Mar  5  2018 jdk1.8.0_162
drwxr-xr-x 8 leia leia 4096 Aug 28  2017 jdk1.8.0_144
As you can see, there are several versions of the Java Development Kit (JDK) and a symbolic link pointing to the latest one. She uses a script with the following commands to install new JDK versions. However, it won't work without a force option or unless the root user runs it:
tar xvzmf jdk1.8.0_181.tar.gz -C jdk1.8.0_181/
ln -vs jdk1.8.0_181 jdk
The tar command will extract the .gz file to the specified directory, but the ln command will fail to upgrade the link because one already exists. The result will be that the link no longer points to the latest JDK:
leia@workstation:/usr/lib/java$ ln -vs jdk1.8.0_181 jdk
ln: failed to create symbolic link 'jdk/jdk1.8.0_181': File exists
leia@workstation:/usr/lib/java$ ls -lt
total 28
drwxr-x--- 2 leia leia 4096 May  1 15:44 jdk1.8.0_181
lrwxrwxrwx 1 leia leia   12 Mar  5  2018 jdk -> jdk1.8.0_162
drwxr-xr-x 8 leia leia 4096 Mar  5  2018 jdk1.8.0_162
drwxr-xr-x 8 leia leia 4096 Aug 28  2017 jdk1.8.0_144
She can force ln to update the link correctly by passing the force option and one other, -n. The -n is needed because the link points to a directory. Now, the link again points to the latest JDK:
leia@workstation:/usr/lib/java$ ln -vsnf jdk1.8.0_181 jdk
'jdk' -> 'jdk1.8.0_181'
leia@workstation:/usr/lib/java$ ls -lt
total 28
lrwxrwxrwx 1 leia leia   12 May  1 16:13 jdk -> jdk1.8.0_181
drwxr-x--- 2 leia leia 4096 May  1 15:44 jdk1.8.0_181
drwxr-xr-x 8 leia leia 4096 Mar  5  2018 jdk1.8.0_162
drwxr-xr-x 8 leia leia 4096 Aug 28  2017 jdk1.8.0_144
A Java application can be configured to find the JDK with the path /usr/lib/java/jdk instead of having to change it every time Java is updated.
rm
The rm command is short for ""remove"" (which we often call delete, since some other operating systems have a del command for this action). The man page describes the force option for rm as:
-f, --force
       ignore nonexistent files and arguments, never prompt
If you try to delete a read-only file, you will be prompted by rm:
[alan@workstation ~]$ ls -l
total 4
-r--r----- 1 alan alan 16 May  1 11:38 B-wing
[alan@workstation ~]$ rm B-wing 
rm: remove write-protected regular file 'B-wing'?
You must type either y or n to answer the prompt and allow the rm command to proceed. If you use the force option, rm will not prompt you and will immediately delete the file:
[alan@workstation ~]$ rm -f B-wing
[alan@workstation ~]$ ls -l
total 0
[alan@workstation ~]$
The most common use of force with rm is to delete a directory. The -r (recursive) option tells rm to remove a directory. When combined with the force option, it will remove the directory and all its contents without prompting.
The rm command with certain options can be disastrous. Over the years, online forums have filled with jokes and horror stories of users completely wiping their systems. This notorious usage is rm -rf *. This will immediately delete all files and directories without any prompt wherever it is used.
userdel
The userdel command is short for user delete, which will delete a user. The man page describes the force option for userdel as:
-f, --force
    This option forces the removal of the user account, even if the
    user is still logged in. It also forces userdel to remove the
    user's home directory and mail spool, even if another user uses
    the same home directory or if the mail spool is not owned by the
    specified user. If USERGROUPS_ENAB is defined to yes in
    /etc/login.defs and if a group exists with the same name as the
    deleted user, then this group will be removed, even if it is
    still the primary group of another user.

    Note: This option is dangerous and may leave your system in an
    inconsistent state.
When Obi-Wan reached the castle on Mustafar, he knew what had to be done. He had to delete Darth's user account—but Darth was still logged in.
[root@workstation ~]# ps -fu darth
UID        PID  PPID  C STIME TTY          TIME CMD
darth     7663  7655  0 13:28 pts/3    00:00:00 -bash
[root@workstation ~]# userdel darth
userdel: user darth is currently used by process 7663
Since Darth is currently logged in, Obi-Wan has to use the force option to userdel. This will delete the user account even though it's logged in.
[root@workstation ~]# userdel -f darth
userdel: user darth is currently used by process 7663
[root@workstation ~]# finger darth
finger: darth: no such user.
[root@workstation ~]# ps -fu darth
error: user name does not exist
As you can see, the finger and ps commands confirm the user Darth has been deleted.
Using force in shell scripts
Many other commands have a force option. One place force is very useful is in shell scripts. Since we use scripts in cron jobs and other automated operations, avoiding any prompts is crucial, or else these automated processes will not complete.
I hope the four examples I shared above help you understand how certain circumstances may require the use of force. You should have a strong understanding of the force option when used at the command line or in creating automation scripts. It's misuse can have devastating effects—sometimes across your infrastructure, and not only on a single machine.

"
Linux,"Inter-process communication in Linux: Using pipes and message queues
Learn how processes synchronize with each other in Linux
This is the second article in a series about interprocess communication (IPC) in Linux. The first article focused on IPC through shared storage: shared files and shared memory segments. This article turns to pipes, which are channels that connect processes for communication. A channel has a write end for writing bytes, and a read end for reading these bytes in FIFO (first in, first out) order. In typical use, one process writes to the channel, and a different process reads from this same channel. The bytes themselves might represent anything: numbers, employee records, digital movies, and so on.
Pipes come in two flavors, named and unnamed, and can be used either interactively from the command line or within programs; examples are forthcoming. This article also looks at memory queues, which have fallen out of fashion—but undeservedly so.
The code examples in the first article acknowledged the threat of race conditions (either file-based or memory-based) in IPC that uses shared storage. The question naturally arises about safe concurrency for the channel-based IPC, which will be covered in this article. The code examples for pipes and memory queues use APIs with the POSIX stamp of approval, and a core goal of the POSIX standards is thread-safety.
Consider the man pages for the mq_open function, which belongs to the memory queue API. These pages include a section on Attributes with this small table:

The value MT-Safe (with MT for multi-threaded) means that the mq_open function is thread-safe, which in turn implies process-safe: A process executes in precisely the sense that one of its threads executes, and if a race condition cannot arise among threads in the same process, such a condition cannot arise among threads in different processes. The MT-Safe attribute assures that a race condition does not arise in invocations of mq_open. In general, channel-based IPC is concurrent-safe, although a cautionary note is raised in the examples that follow.
Unnamed pipes
Let's start with a contrived command line example that shows how unnamed pipes work. On all modern systems, the vertical bar | represents an unnamed pipe at the command line. Assume % is the command line prompt, and consider this command:
% sleep 5 | echo ""Hello, world!"" ## writer to the left of |, reader to the right
The sleep and echo utilities execute as separate processes, and the unnamed pipe allows them to communicate. However, the example is contrived in that no communication occurs. The greeting Hello, world! appears on the screen; then, after about five seconds, the command line prompt returns, indicating that both the sleep and echo processes have exited. What's going on?
In the vertical-bar syntax from the command line, the process to the left (sleep) is the writer, and the process to the right (echo) is the reader. By default, the reader blocks until there are bytes to read from the channel, and the writer—after writing its bytes—finishes up by sending an end-of-stream marker. (Even if the writer terminates prematurely, an end-of-stream marker is sent to the reader.) The unnamed pipe persists until both the writer and the reader terminate.
In the contrived example, the sleep process does not write any bytes to the channel but does terminate after about five seconds, which sends an end-of-stream marker to the channel. In the meantime, the echo process immediately writes the greeting to the standard output (the screen) because this process does not read any bytes from the channel, so it does no waiting. Once the sleep and echo processes terminate, the unnamed pipe—not used at all for communication—goes away and the command line prompt returns.
Here is a more useful example using two unnamed pipes. Suppose that the file test.dat looks like this:
this
is
the
way
the
world
ends
The command:
% cat test.dat | sort | uniq
pipes the output from the cat (concatenate) process into the sort process to produce sorted output, and then pipes the sorted output into the uniq process to eliminate duplicate records (in this case, the two occurrences of the reduce to one):
ends
is
the
this
way
world
The scene now is set for a program with two processes that communicate through an unnamed pipe.
Example 1. Two processes communicating through an unnamed pipe.
#include <sys/wait.h> /* wait */
#include <stdio.h>
#include <stdlib.h>   /* exit functions */
#include <unistd.h>   /* read, write, pipe, _exit */
#include <string.h>

#define ReadEnd  0
#define WriteEnd 1

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1);    /** failure **/
}

int main() {
  int pipeFDs[2]; /* two file descriptors */
  char buf;       /* 1-byte buffer */
  const char* msg = ""Nature's first green is gold\n""; /* bytes to write */

  if (pipe(pipeFDs) < 0) report_and_exit(""pipeFD"");
  pid_t cpid = fork();                                /* fork a child process */
  if (cpid < 0) report_and_exit(""fork"");              /* check for failure */

  if (0 == cpid) {    /*** child ***/                 /* child process */
    close(pipeFDs[WriteEnd]);                         /* child reads, doesn't write */

    while (read(pipeFDs[ReadEnd], &buf, 1) > 0)       /* read until end of byte stream */
      write(STDOUT_FILENO, &buf, sizeof(buf));        /* echo to the standard output */

    close(pipeFDs[ReadEnd]);                          /* close the ReadEnd: all done */
    _exit(0);                                         /* exit and notify parent at once  */
  }
  else {              /*** parent ***/
    close(pipeFDs[ReadEnd]);                          /* parent writes, doesn't read */

    write(pipeFDs[WriteEnd], msg, strlen(msg));       /* write the bytes to the pipe */
    close(pipeFDs[WriteEnd]);                         /* done writing: generate eof */

    wait(NULL);                                       /* wait for child to exit */
    exit(0);                                          /* exit normally */
  }
  return 0;
}
The pipeUN program above uses the system function fork to create a process. Although the program has but a single source file, multi-processing occurs during (successful) execution. Here are the particulars in a quick review of how the library function fork works:
The fork function, called in the parent process, returns -1 to the parent in case of failure. In the pipeUN example, the call is: pid_t cpid = fork(); /* called in parent */  The returned value is stored, in this example, in the variable cpid of integer type pid_t. (Every process has its own process ID, a non-negative integer that identifies the process.) Forking a new process could fail for several reasons, including a full process table, a structure that the system maintains to track processes. Zombie processes, clarified shortly, can cause a process table to fill if these are not harvested.
If the fork call succeeds, it thereby spawns (creates) a new child process, returning one value to the parent but a different value to the child. Both the parent and the child process execute the same code that follows the call to fork. (The child inherits copies of all the variables declared so far in the parent.) In particular, a successful call to fork returns:
Zero to the child process
The child's process ID to the parent
An if/else or equivalent construct typically is used after a successful fork call to segregate code meant for the parent from code meant for the child. In this example, the construct is: if (0 == cpid) {    /*** child ***/
...
}
else {              /*** parent ***/
...
}  
If forking a child succeeds, the pipeUN program proceeds as follows. There is an integer array:
int pipeFDs[2]; /* two file descriptors */
to hold two file descriptors, one for writing to the pipe and another for reading from the pipe. (The array element pipeFDs[0] is the file descriptor for the read end, and the array element pipeFDs[1] is the file descriptor for the write end.) A successful call to the system pipe function, made immediately before the call to fork, populates the array with the two file descriptors:
if (pipe(pipeFDs) < 0) report_and_exit(«pipeFD"");
The parent and the child now have copies of both file descriptors, but the separation of concerns pattern means that each process requires exactly one of the descriptors. In this example, the parent does the writing and the child does the reading, although the roles could be reversed. The first statement in the child if-clause code, therefore, closes the pipe's write end:

close(pipeFDs[WriteEnd]); /* called in child code */
and the first statement in the parent else-clause code closes the pipe's read end:
close(pipeFDs[ReadEnd]);  /* called in parent code */
The parent then writes some bytes (ASCII codes) to the unnamed pipe, and the child reads these and echoes them to the standard output.
One more aspect of the program needs clarification: the call to the wait function in the parent code. Once spawned, a child process is largely independent of its parent, as even the short pipeUN program illustrates. The child can execute arbitrary code that may have nothing to do with the parent. However, the system does notify the parent through a signal—if and when the child terminates.
What if the parent terminates before the child? In this case, unless precautions are taken, the child becomes and remains a zombie process with an entry in the process table. The precautions are of two broad types. One precaution is to have the parent notify the system that the parent has no interest in the child's termination:
signal(SIGCHLD, SIG_IGN); /* in parent: ignore notification */
A second approach is to have the parent execute a wait on the child's termination, thereby ensuring that the parent outlives the child. This second approach is used in the pipeUN program, where the parent code has this call:
wait(NULL); /* called in parent */
This call to wait means wait until the termination of any child occurs, and in the pipeUN program, there is only one child process. (The NULL argument could be replaced with the address of an integer variable to hold the child's exit status.) There is a more flexible waitpid function for fine-grained control, e.g., for specifying a particular child process among several.
The pipeUN program takes another precaution. When the parent is done waiting, the parent terminates with the call to the regular exit function. By contrast, the child terminates with a call to the _exit variant, which fast-tracks notification of termination. In effect, the child is telling the system to notify the parent ASAP that the child has terminated.
If two processes write to the same unnamed pipe, can the bytes be interleaved? For example, if process P1 writes:
foo bar
to a pipe and process P2 concurrently writes:
baz baz
to the same pipe, it seems that the pipe contents might be something arbitrary, such as:
baz foo baz bar
The POSIX standard ensures that writes are not interleaved so long as no write exceeds PIPE_BUF bytes. On Linux systems, PIPE_BUF is 4,096 bytes in size. My preference with pipes is to have a single writer and a single reader, thereby sidestepping the issue.
Named pipes
An unnamed pipe has no backing file: the system maintains an in-memory buffer to transfer bytes from the writer to the reader. Once the writer and reader terminate, the buffer is reclaimed, so the unnamed pipe goes away. By contrast, a named pipe has a backing file and a distinct API.
Let's look at another command line example to get the gist of named pipes. Here are the steps:
Open two terminals. The working directory should be the same for both.
In one of the terminals, enter these two commands (the prompt again is %, and my comments start with ##): % mkfifo tester  ## creates a backing file named tester
% cat tester     ## type the pipe's contents to stdout   At the beginning, nothing should appear in the terminal because nothing has been written yet to the named pipe.
In the second terminal, enter the command: % cat > tester  ## redirect keyboard input to the pipe
hello, world!   ## then hit Return key
bye, bye        ## ditto
<Control-C>     ## terminate session with a Control-C   Whatever is typed into this terminal is echoed in the other. Once Ctrl+C is entered, the regular command line prompt returns in both terminals: the pipe has been closed.
Clean up by removing the file that implements the named pipe: % unlink tester 
As the utility's name mkfifo implies, a named pipe also is called a FIFO because the first byte in is the first byte out, and so on. There is a library function named mkfifo that creates a named pipe in programs and is used in the next example, which consists of two processes: one writes to the named pipe and the other reads from this pipe.
Example 2. The fifoWriter program
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <time.h>
#include <stdlib.h>
#include <stdio.h>

#define MaxLoops         12000   /* outer loop */
#define ChunkSize           16   /* how many written at a time */
#define IntsPerChunk         4   /* four 4-byte ints per chunk */
#define MaxZs              250   /* max microseconds to sleep */

int main() {
  const char* pipeName = ""./fifoChannel"";
  mkfifo(pipeName, 0666);                      /* read/write for user/group/others */
  int fd = open(pipeName, O_CREAT | O_WRONLY); /* open as write-only */
  if (fd < 0) return -1;                       /* can't go on */

  int i;
  for (i = 0; i < MaxLoops; i++) {          /* write MaxWrites times */
    int j;
    for (j = 0; j < ChunkSize; j++) {       /* each time, write ChunkSize bytes */
      int k;
      int chunk[IntsPerChunk];
      for (k = 0; k < IntsPerChunk; k++)
        chunk[k] = rand();
      write(fd, chunk, sizeof(chunk));
    }
    usleep((rand() % MaxZs) + 1);           /* pause a bit for realism */
  }

  close(fd);           /* close pipe: generates an end-of-stream marker */
  unlink(pipeName);    /* unlink from the implementing file */
  printf(""%i ints sent to the pipe.\n"", MaxLoops * ChunkSize * IntsPerChunk);

  return 0;
}
The fifoWriter program above can be summarized as follows:
The program creates a named pipe for writing: mkfifo(pipeName, 0666); /* read/write perms for user/group/others */
int fd = open(pipeName, O_CREAT | O_WRONLY);   where pipeName is the name of the backing file passed to mkfifo as the first argument. The named pipe then is opened with the by-now familiar call to the open function, which returns a file descriptor.
For a touch of realism, the fifoWriter does not write all the data at once, but instead writes a chunk, sleeps a random number of microseconds, and so on. In total, 768,000 4-byte integer values are written to the named pipe.
After closing the named pipe, the fifoWriter also unlinks the file: close(fd);        /* close pipe: generates end-of-stream marker */
unlink(pipeName); /* unlink from the implementing file */   The system reclaims the backing file once every process connected to the pipe has performed the unlink operation. In this example, there are only two such processes: the fifoWriter and the fifoReader, both of which do an unlink operation.
The two programs should be executed in different terminals with the same working directory. However, the fifoWriter should be started before the fifoReader, as the former creates the pipe. The fifoReader then accesses the already created named pipe.
Example 3. The fifoReader program
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <fcntl.h>
#include <unistd.h>

unsigned is_prime(unsigned n) { /* not pretty, but efficient */
  if (n <= 3) return n > 1;
  if (0 == (n % 2) || 0 == (n % 3)) return 0;

  unsigned i;
  for (i = 5; (i * i) <= n; i += 6)
    if (0 == (n % i) || 0 == (n % (i + 2))) return 0;

  return 1; /* found a prime! */
}

int main() {
  const char* file = ""./fifoChannel"";
  int fd = open(file, O_RDONLY);
  if (fd < 0) return -1; /* no point in continuing */
  unsigned count = 0, total = 0, primes_count = 0;

  while (1) {
    int next;
    int i;

    ssize_t count = read(fd, &next, sizeof(int));
    if (0 == count) break;                  /* end of stream */
    else if (count == sizeof(int)) {        /* read a 4-byte int value */
      total++;
      if (is_prime(next)) primes_count++;
    }
  }

  close(fd);       /* close pipe from read end */
  unlink(file);    /* unlink from the underlying file */
  printf(""Received ints: %u, primes: %u\n"", total, primes_count);

  return 0;
}
The fifoReader program above can be summarized as follows:
Because the fifoWriter creates the named pipe, the fifoReader needs only the standard call open to access the pipe through the backing file: const char* file = ""./fifoChannel"";
int fd = open(file, O_RDONLY);   The file opens as read-only.
The program then goes into a potentially infinite loop, trying to read a 4-byte chunk on each iteration. The read call: ssize_t count = read(fd, &next, sizeof(int));  returns 0 to indicate end-of-stream, in which case the fifoReader breaks out of the loop, closes the named pipe, and unlinks the backing file before terminating.
After reading a 4-byte integer, the fifoReader checks whether the number is a prime. This represents the business logic that a production-grade reader might perform on the received bytes. On a sample run, there were 37,682 primes among the 768,000 integers received.
On repeated sample runs, the fifoReader successfully read all of the bytes that the fifoWriter wrote. This is not surprising. The two processes execute on the same host, taking network issues out of the equation. Named pipes are a highly reliable and efficient IPC mechanism and, therefore, in wide use.
Here is the output from the two programs, each launched from a separate terminal but with the same working directory:
% ./fifoWriter
768000 ints sent to the pipe.
###
% ./fifoReader
Received ints: 768000, primes: 37682
Message queues
Pipes have strict FIFO behavior: the first byte written is the first byte read, the second byte written is the second byte read, and so forth. Message queues can behave in the same way but are flexible enough that byte chunks can be retrieved out of FIFO order.
As the name suggests, a message queue is a sequence of messages, each of which has two parts:
The payload, which is an array of bytes (char in C)
A type, given as a positive integer value; types categorize messages for flexible retrieval
Consider the following depiction of a message queue, with each message labeled with an integer type:
          +-+    +-+    +-+    +-+
sender--->|3|--->|2|--->|2|--->|1|--->receiver
          +-+    +-+    +-+    +-+
Of the four messages shown, the one labeled 1 is at the front, i.e., closest to the receiver. Next come two messages with label 2, and finally, a message labeled 3 at the back. If strict FIFO behavior were in play, then the messages would be received in the order 1-2-2-3. However, the message queue allows other retrieval orders. For example, the messages could be retrieved by the receiver in the order 3-2-1-2.
The mqueue example consists of two programs, the sender that writes to the message queue and the receiver that reads from this queue. Both programs include the header file queue.h shown below:
Example 4. The header file queue.h
#define ProjectId 123
#define PathName  ""queue.h"" /* any existing, accessible file would do */
#define MsgLen    4
#define MsgCount  6

typedef struct {
  long type;                 /* must be of type long */
  char payload[MsgLen + 1];  /* bytes in the message */
} queuedMessage;
The header file defines a structure type named queuedMessage, with payload (byte array) and type (integer) fields. This file also defines symbolic constants (the #define statements), the first two of which are used to generate a key that, in turn, is used to get a message queue ID. The ProjectId can be any positive integer value, and the PathName must be an existing, accessible file—in this case, the file queue.h. The setup statements in both the sender and the receiver programs are:
key_t key = ftok(PathName, ProjectId);   /* generate key */
int qid = msgget(key, 0666 | IPC_CREAT); /* use key to get queue id */
The ID qid is, in effect, the counterpart of a file descriptor for message queues.
Example 5. The message sender program
#include <stdio.h>
#include <sys/ipc.h>
#include <sys/msg.h>
#include <stdlib.h>
#include <string.h>
#include ""queue.h""

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1); /* EXIT_FAILURE */
}

int main() {
  key_t key = ftok(PathName, ProjectId);
  if (key < 0) report_and_exit(""couldn't get key..."");

  int qid = msgget(key, 0666 | IPC_CREAT);
  if (qid < 0) report_and_exit(""couldn't get queue id..."");

  char* payloads[] = {""msg1"", ""msg2"", ""msg3"", ""msg4"", ""msg5"", ""msg6""};
  int types[] = {1, 1, 2, 2, 3, 3}; /* each must be > 0 */
  int i;
  for (i = 0; i < MsgCount; i++) {
    /* build the message */
    queuedMessage msg;
    msg.type = types[i];
    strcpy(msg.payload, payloads[i]);

    /* send the message */
    msgsnd(qid, &msg, sizeof(msg), IPC_NOWAIT); /* don't block */
    printf(""%s sent as type %i\n"", msg.payload, (int) msg.type);
  }
  return 0;
}
The sender program above sends out six messages, two each of a specified type: the first messages are of type 1, the next two of type 2, and the last two of type 3. The sending statement:
msgsnd(qid, &msg, sizeof(msg), IPC_NOWAIT);
is configured to be non-blocking (the flag IPC_NOWAIT) because the messages are so small. The only danger is that a full queue, unlikely in this example, would result in a sending failure. The receiver program below also receives messages using the IPC_NOWAIT flag.
Example 6. The message receiver program
#include <stdio.h>
#include <sys/ipc.h>
#include <sys/msg.h>
#include <stdlib.h>
#include ""queue.h""

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1); /* EXIT_FAILURE */
}

int main() {
  key_t key= ftok(PathName, ProjectId); /* key to identify the queue */
  if (key < 0) report_and_exit(""key not gotten..."");

  int qid = msgget(key, 0666 | IPC_CREAT); /* access if created already */
  if (qid < 0) report_and_exit(""no access to queue..."");

  int types[] = {3, 1, 2, 1, 3, 2}; /* different than in sender */
  int i;
  for (i = 0; i < MsgCount; i++) {
    queuedMessage msg; /* defined in queue.h */
    if (msgrcv(qid, &msg, sizeof(msg), types[i], MSG_NOERROR | IPC_NOWAIT) < 0)
      puts(""msgrcv trouble..."");
    printf(""%s received as type %i\n"", msg.payload, (int) msg.type);
  }

  /** remove the queue **/
  if (msgctl(qid, IPC_RMID, NULL) < 0)  /* NULL = 'no flags' */
    report_and_exit(""trouble removing queue..."");

  return 0;
}
The receiver program does not create the message queue, although the API suggests as much. In the receiver, the call:
int qid = msgget(key, 0666 | IPC_CREAT);
is misleading because of the IPC_CREAT flag, but this flag really means create if needed, otherwise access. The sender program calls msgsnd to send messages, whereas the receiver calls msgrcv to retrieve them. In this example, the sender sends the messages in the order 1-1-2-2-3-3, but the receiver then retrieves them in the order 3-1-2-1-3-2, showing that message queues are not bound to strict FIFO behavior:
% ./sender
msg1 sent as type 1
msg2 sent as type 1
msg3 sent as type 2
msg4 sent as type 2
msg5 sent as type 3
msg6 sent as type 3

% ./receiver
msg5 received as type 3
msg1 received as type 1
msg3 received as type 2
msg2 received as type 1
msg6 received as type 3
msg4 received as type 2
The output above shows that the sender and the receiver can be launched from the same terminal. The output also shows that the message queue persists even after the sender process creates the queue, writes to it, and exits. The queue goes away only after the receiver process explicitly removes it with the call to msgctl:
if (msgctl(qid, IPC_RMID, NULL) < 0) /* remove queue */
Wrapping up
The pipes and message queue APIs are fundamentally unidirectional: one process writes and another reads. There are implementations of bidirectional named pipes, but my two cents is that this IPC mechanism is at its best when it is simplest. As noted earlier, message queues have fallen in popularity—but without good reason; these queues are yet another tool in the IPC toolbox. Part 3 completes this quick tour of the IPC toolbox with code examples of IPC through sockets and signals.

"
JavaScript$Rust,"Building and augmenting libraries by calling Rust from JavaScript
Explore how to use WebAssembly (Wasm) to embed Rust inside JavaScript.
In Why should you use Rust in WebAssembly?, I looked at why you might want to write WebAssembly (Wasm), and why you might choose Rust as the language to do it in. Now I'll share what that looks like by exploring ways to embed Rust inside JavaScript.
This is something that separates Rust from Go, C#, and other languages with large runtimes that can compile to Wasm. Rust has a minimal runtime (basically just an allocator), making it easy to use Rust from JavaScript libraries. C and C++ have similar stories, but what sets Rust apart is its tooling, which we'll take a look at now.
The basics
If you've never used Rust before, you'll first want to get that set up. It's pretty easy. First download Rustup, which is a way to control versions of Rust and different toolchains for cross-compilation. This will give you access to Cargo, which is the Rust build tool and package manager.
Now we have a decision to make. We can easily write Rust code that runs in the browser through WebAssembly, but if we want to do anything other than make people's CPU fans spin, we'll probably at some point want to interact with the Document Object Model (DOM) or use some JavaScript API. In other words: we need JavaScript interop (aka the JavaScript interoperability API).
The problem and the solutions

WebAssembly is an extremely simple machine language. If we want to be able to communicate with JavaScript, Wasm gives us only four data types to do it with: 32- and 64-bit floats and integers. Wasm doesn't have a concept of strings, arrays, objects, or any other rich data type. Basically, we can only pass around pointers between Rust and JavaScript. Needless to say, this is less than ideal.

The good news is there are two libraries that facilitate communication between Rust-based Wasm and JavaScript: wasm-bindgen and stdweb. The bad news, however, is these two libraries are unfortunately incompatible with each other. wasm-bindgen is lower-level than stdweb and attempts to provide full control over how JavaScript and Rust interact. In fact, there is even talk of rewriting stdweb using wasm-bindgen, which would get rid of the issue of incompatibility.
Because wasm-bindgen is the lighter-weight option (and the option officially worked on by the official Rust WebAssembly working group), we'll focus at that.
wasm-bindgen and wasm-pack
We're going to create a function that takes a string from JavaScript, makes it uppercase and prepends ""HELLO, "" to it, and returns it back to JavaScript. We'll call this function excited_greeting!
First, let's create our Rust library that will house this fabulous function:
$ cargo new my-wasm-library --lib
$ cd my-wasm-library
Now we'll want to replace the contents of src/lib.rs with our exciting logic. I think it's best to write the code out instead of copy/pasting.
// Include the `wasm_bindgen` attribute into the current namespace. 
use wasm_bindgen::prelude::wasm_bindgen;

// This attribute makes calling Rust from JavaScript possible.
// It generates code that can convert the basic types wasm understands 
// (integers and floats) into more complex types like strings and 
// vice versa. If you're interested in how this works, check this out:
// https://blog.ryanlevick.com/posts/wasm-bindgen-interop/
#[wasm_bindgen]
// This is pretty plain Rust code. If you've written Rust before this
// should look extremely familiar. If not, why wait?! Check this out:
// https://doc.rust-lang.org/book/
pub fn excited_greeting(original: &str) -> String {
  format!(""HELLO, {}"", original.to_uppercase())
}
Second, we'll have to make two changes to our Cargo.toml configuration file:
Add wasm_bindgen as a dependency.
Configure the type of library binary to be a cdylib or dynamic system library. In this case, our system is wasm, and setting this option is how we produce .wasm binary files.
[package]
name = ""my-wasm-library""
version = ""0.1.0""
authors = [""$YOUR_INFO""]
edition = ""2018""

[lib]
crate-type = [""cdylib"", ""rlib""]

[dependencies]
wasm-bindgen = ""0.2.33""
Now let's build! If we just use cargo build, we'll get a .wasm binary, but in order to make it easy to call our Rust code from JavaScript, we'd like to have some JavaScript code that converts rich JavaScript types like strings and objects to pointers and passes these pointers to the Wasm module on our behalf. Doing this manually is tedious and prone to bugs.
Luckily, in addition to being a library, wasm-bindgen also has the ability to create this ""glue"" JavaScript for us. This means in our code we can interact with our Wasm module using normal JavaScript types, and the generated code from wasm-bindgen will do the dirty work of converting these rich types into the pointer types that Wasm actually understands.
We can use the awesome wasm-pack to build our Wasm binary, invoke the wasm-bindgen CLI tool, and package all of our JavaScript (and any optional generated TypeScript types) into one nice and neat package. Let's do that now!
First we'll need to install wasm-pack:
$ cargo install wasm-pack
By default, wasm-bindgen produces ES6 modules. We'll use our code from a simple script tag, so we just want it to produce a plain old JavaScript object that gives us access to our Wasm functions. To do this, we'll pass it the --target no-modules option.
$ wasm-pack build --target no-modules
We now have a pkg directory in our project. If we look at the contents, we'll see the following:
package.json: useful if we want to package this up as an NPM module
my_wasm_library_bg.wasm: our actual Wasm code
my_wasm_library.js: the JavaScript ""glue"" code
Some TypeScript definition files
Now we can create an index.html file that will make use of our JavaScript and Wasm:
<html>
<head>
  <meta content=""text/html;charset=utf-8"" http-equiv=""Content-Type"" />
</head>
<body>
  <!-- Include our glue code --> 
  <script src='./pkg/my_wasm_library.js'></script>
  <!-- Include our glue code --> 
  <script>
    window.addEventListener('load', async () => {
      // Load the wasm file
      await wasm_bindgen('./pkg/my_wasm_library_bg.wasm');
      // Once it's loaded the `wasm_bindgen` object is populated 
      // with the functions defined in our Rust code
      const greeting = wasm_bindgen.excited_greeting(""Ryan"")
      console.log(greeting)
    });
  </script>
</body>
</html>
You may be tempted to open the HTML file in your browser, but unfortunately, this is not possible. For security reasons, Wasm files have to be served from the same domain as the HTML file. You'll need an HTTP server. If you have a favorite static HTTP server that can serve files from your filesystem, feel free to use that. I like to use basic-http-server, which you can install and run like so:
$ cargo install basic-http-server
$ basic-http-server
Now open the index.html file through the web server by going to http://localhost:4000/index.html and check your JavaScript console. You should see a very exciting greeting there!
"
JavaScript,"Getting started with React Native animations
Here are the tools you need to overcome performance challenges when implementing React Native animations
React Native animation is a popular topic for workshops and classes, perhaps because many developers find it challenging to work with. While many online blogs and resources focus on the performance aspects of React Native, few take you through the basics. In this article, I will discuss the fundamentals of how to implement React Native animations.
First, let's review some background and history.
History and evolution
To work with cross-platform JavaScript code, native components of your phone must communicate information via an element called a bridge. The bridge is asynchronous, which causes JavaScript-based React Native apps to lag because asynchronous requests over the bridge clog the path of JavaScript code interacting with the native parts.
To achieve high performance, animations must be rendered on a native UI thread. Since data needs to be serialized over the bridge, this often blocks the JavaScript thread, causing a drop in frames. This problem has been prevalent since 2015 when animations posed one of the biggest limitations in React Native.
Fortunately, the situation has improved since then thanks to overwhelming support from community members. Achieving 60 frames per second (FPS) is now common for React Native animations. Declarative APIs such as Animated have eased the process of implementing interactive animations.
Using the Animated API to improve performance
Still, it is not unusual for developers to encounter performance issues, especially when they are working on complex animations.
As mentioned above, performance bottlenecks in React Native animations are caused by heavy workloads on JavaScript threads, which reduces the frame rate and causes a sluggish user experience. To overcome this problem, you need to maintain a frame rate of 60 frames per second.
Using the Animated API is the best solution to this as it optimizes the required serialization/deserialization time. It does this by using a declarative API to describe animations. The idea behind this is to declare the entire animation at once in advance so that the declaration in JavaScript can be serialized and sent to the bridge. A driver then executes the animations frame by frame.
How to implement animations in React Native
There are several ways to implement animations in React Native. Here are some that I find most useful.
Animated values
Animated values tops my list as the building block for animations in any React Native app. These generally point to a real value, which is converted back to a real number when passed with an animated component.
Let’s look at an example:
Animated.timing(this.valueToAnimate, {
    toValue: 42;
    duration: 1000;
}).start()
In the above example, I have declared value.ToAnimate as 42, which will be executed after 1000 milliseconds.
You can also use Animated values to declare properties such as opacity or position. Here’s an example implementation of opacity with Animated values:
<Animated.View style={{ opacity: myAnimatedOpacity }} />  
Animation drivers: Animated.timing, Animated.event, Animated.decay
Think of drivers like nodes in a graph that changes an Animated value with each frame. For instance, Animated.timing will increase a value, while Animated.decay will reduce a value over each frame.
Let’s look at an example:
Animated.decay(this.valueToAnimate, {
   velocity: 2.0,
   deceleration: 0.9
}).start();
This example launches the animation at a particular velocity and gradually decelerates at a particular time. I like to do this in a cross-platform app when docs on the material design initially surface. It feels pretty great, and there are many ways to make the animation deliver a memorable experience.
You can also use Animated.event to drive a value as your user scroll:
<ScrollView onScroll={Animated.event(
  [{nativeEvent: {contentOffset: {y: this.state.scrollY}}}]
)}
>
</ScrollView>
In the above example, Animated.event returns a function that sets the scrollView's nativeEvent.contentOffset.y to your current scrollY state.
All in all, animated drivers can be used in association with Animated values or other Animated drivers.
As a side note, keep in mind that when a driver updates each frame, the new value will instantly update the View property. So be careful while declaring the variables and mindful of their scope while using them.
Transform methods
Transform methods enable you to convert an Animated value to a new Animated value.
You can use methods such as Animated.add(), Animated.multiply(), or Animated.interpolate() to implement transform operations. You can execute a transform operation on any node in the Animated graph with this:
newAnimated.Value(55).interpolate(.....) // Transformation operation using Animated.interpolate() method
Animated props
Animated props are special nodes that map an Animated value to a prop on a component. It is generated when you render an Animated.view and assign it properties.
Let’s look at the following code snippet:
Var opacity = new Animated.Value(0.7);
<Animated.View style={{ opacity }} />
Here, I’ve added an Animated prop that converts the value 0.7 to a property. If a method updates the value, the change will be reflected in the View’s property.
The methods described above work in conjunction with, and play a crucial role in, animating objects in React Native.
The animated value for every frame of the animation is changed by the animation driver (Animated.Timing, Animated.Event, or Animated.Decay). The result is then passed along to any transformation operation, where it gets stored as the prop of the view (opacity or transform value).
The result is then handed over to the native realm by JavaScript, where the view gets updated while calling setNativeProps. Finally, it is passed over to iOS or Android, where UIView or Android.View gets updated.
Implementing animations using the Animated API and Native Driver
Since the inception of the React Native Animated API, a JavaScript driver has been used for frame execution, but it resulted in frame drop as the business logic directly falls on the JavaScript thread.
To address frame drops, the latest version of the driver was made purely native, and it is now capable of executing animations frame by frame in native realm.
The Native Driver, when used alongside the Animated API, allows the native animated module to update views directly without the need to calculate the value in JavaScript.
In order to use Native Driver, you must specify useNativeDriver to be true while configuring your animations:
useNativeDriver: true
Using PanResponder for handling gestures in React Native
The React Native Animated API can do most of the ""grunt work"" for you while implementing React Native animations. However, it has a key limitation when it comes to implementing gestures in animations: It is unable to respond to gestures beyond the scope of a ScrollView.

While you can do many things with a simple ScrollView component, you will likely agree that mobile is incomplete without gestures, which are the actions users perform with animations, such as scroll or pan.
In React Native, gestures can be handled seamlessly by using PanResponder with the Animated API.
PanResponder combines various touches into a specific gesture. It makes a single touch responsive to extra touches so that gestures function smoothly.
By default, PanResponder consists of an InteractionManager handle, which blocks the events running on the JS thread from interrupting the gestures.
Improving uptime for slow Navigator transitions
Any React Native animation that involves moving from one app screen to another should usually be done using navigation components. Navigation components such as React Navigation are generally used for navigation transitions.
In React Native, navigation transitions usually happen in JavaScript threads, which can result in slow transitions in for low-end/low-memory devices (typically Androids, as iOS devices handle these more effectively). Slow navigation transitions usually happen when React Native is trying to render a new screen while an animation is still executing in the background.
To avoid such situations, InteractionManager allows long-running tasks to be scheduled after an animation or interaction has executed in the JavaScript thread.
Layout animations
LayoutAnimation is a simple API that automatically animates the view to the next consecutive position when the next layout happens. It works on the UI thread, which makes it highly performant.
Animations configured using LayoutAnimation will apply on all the components once it is called, in contrast to Animated, in which you control the specific values to animate. LayoutAnimation is capable of animating everything that changes on the next rendering, so you should call it before calling setState.
Configuring a layout animation before calling setState will ensure smooth animations in native thread and prevent your animations from being affected if code in another setState diff is triggered (under normal conditions, this would compromise your app's animation).
Another way of using LayoutAnimation is to call it inside the component WillReceiveProps method. Simply call LayoutAnimation.configureNext by passing the appropriate parameters for animation configuration, as shown below:
LayoutAnimation.configureNext(animationConfiguration, callbackCompletionMethod); 
this.setState({ stateToChange: newStateValue });
LayoutAnimation supports only two properties: opacity and scalability.
It identifies views by using their unique keys and computing their expected position. Moreover, it animates the frame changes as long as the view keeps the same key between changes of states.
Animations implemented using LayoutAnimation happen natively, which is good from a performance perspective, but it can be challenging if all properties need to be animated between states.
Closing thoughts
This article only scratches the surface of React Native animations. A rule of thumb when working in React Native is to use Animated API wherever possible. For gestures, use PanResponder alongside Animated API.
Leveraging Native Driver along with Animated API will eliminate most of the issues you may encounter—but if performance still lags, stick to LayoutAnimation.

"
JavaScript$Java,"Using Grails with jQuery and DataTables
Learn to build a Grails-based data browser that lets users visualize complex tabular data.
I’m a huge fan of Grails. Granted, I’m mostly a data person who likes to explore and analyze data using command-line tools. But even data people sometimes need to look at the data, and sometimes using data means having a great data browser. With Grails, jQuery, and the DataTables jQuery plugin, we can make really nice tabular data browsers.
The DataTables website offers a lot of decent “recipe-style” documentation that shows how to put together some fine sample applications, and it includes the necessary JavaScript, HTML, and occasional PHP to accomplish some pretty spiffy stuff. But for those who would rather use Grails as their backend, a bit of interpretation is necessary. Also, the sample application data used is a single flat table of employees of a fictional company, so the complexity of dealing with table relations serves as an exercise for the reader.
In this article, we’ll fill those two gaps by creating a Grails application with a slightly more complex data structure and a DataTables browser. In doing so, we’ll cover Grails criteria, which are Groovy-fied Java Hibernate criteria. I’ve put the code for the application on GitHub, so this article is oriented toward explaining the nuances of the code.

For prerequisites, you will need Java, Groovy, and Grails environments set up. With Grails, I tend to use a terminal window and Vim, so that’s what’s used here. To get a modern Java, I suggest downloading and installing the Open Java Development Kit (OpenJDK) provided by your Linux distro (which should be Java 8, 9, 10 or 11; at the time of writing, I’m working with Java 8). From my point of view, the best way to get up-to-date Groovy and Grails is to use SDKMAN!.
Readers who have never tried Grails will probably need to do some background reading. As a starting point, I recommend Creating Your First Grails Application.
Getting the employee browser application
As mentioned above, I’ve put the source code for this sample employee browser application on GitHub. For further explanation, the application embrow was built using the following commands in a Linux terminal window:
cd Projects
grails create-app com.nuevaconsulting.embrow
The domain classes and unit tests are created as follows:
cd embrow
grails create-domain-class com.nuevaconsulting.embrow.Position
grails create-domain-class com.nuevaconsulting.embrow.Office
grails create-domain-class com.nuevaconsulting.embrow.Employee
The domain classes built this way have no attributes, so they must be edited as follows:
The Position domain class:
package com.nuevaconsulting.embrow
  
class Position {

    String name
    int starting

    static constraints = {
        name nullable: false, blank: false
        starting nullable: false
    }
}
The Office domain class:
package com.nuevaconsulting.embrow
  
class Office {

    String name
    String address
    String city
    String country

    static constraints = {
        name nullable: false, blank: false
        address nullable: false, blank: false
        city nullable: false, blank: false
        country nullable: false, blank: false
    }
}
And the Employee domain class:
package com.nuevaconsulting.embrow
  
class Employee {

    String surname
    String givenNames
    Position position
    Office office
    int extension
    Date hired
    int salary
    static constraints = {
        surname nullable: false, blank: false
        givenNames nullable: false, blank: false
        position nullable: false
        office nullable: false
        extension nullable: false
        hired nullable: false
        salary nullable: false
    }
}
Note that whereas the Position and Office domain classes use predefined Groovy types String and int, the Employee domain class defines fields that are of type Position and Office (as well as the predefined Date). This causes the creation of the database table in which instances of Employee are stored to contain references, or foreign keys, to the tables in which instances of Position and Office are stored.
Now you can generate the controllers, views, and various other test components:
grails generate-all com.nuevaconsulting.embrow.Position
grails generate-all com.nuevaconsulting.embrow.Office
grails generate-all com.nuevaconsulting.embrow.Employee
At this point, you have a basic create-read-update-delete (CRUD) application ready to go. I’ve included some base data in the grails-app/init/com/nuevaconsulting/BootStrap.groovy to populate the tables.
If you run the application with the command:
grails run-app
you will see the following screen in the browser at http://localhost:8080/:
Clicking on the link for the OfficeController gives you a screen.
Note that this list is generated by the OfficeController index method and displayed by the view office/index.gsp.
Similarly, clicking on the EmployeeController gives a screen.
Ok, that’s pretty ugly—what’s with the Position and Office links?
Well, the views generated by the generate-all commands above create an index.gsp file that uses the Grails <f:table/> tag that by default shows the class name (com.nuevaconsulting.embrow.Position) and the persistent instance identifier (30). This behavior can be customized to yield something better looking, and there is some pretty neat stuff with the autogenerated links, the autogenerated pagination, and the autogenerated sortable columns.
But even when it's fully cleaned up, this employee browser offers limited functionality. For example, what if you want to find all employees whose position includes the text “dev”? What if you want to combine columns for sorting so that the primary sort key is a surname and the secondary sort key is an office name? Or what if you want to export a sorted subset to a spreadsheet or PDF to email to someone who doesn’t have access to the browser?
The jQuery DataTables plugin provides this kind of extra functionality and allows you to create a full-fledged tabular data browser.
Creating the employee browser view and controller methods
In order to create an employee browser based on jQuery DataTables, you must complete two tasks:
Create a Grails view that incorporates the HTML and JavaScript required to enable the DataTables
Add a method to the Grails controller to handle the new view
The employee browser view
In the directory embrow/grails-app/views/employee, start by making a copy of the index.gsp file, calling it browser.gsp:
cd Projects
cd embrow/grails-app/views/employee
cp index.gsp browser.gsp
At this point, you want to customize the new browser.gsp file to add the relevant jQuery DataTables code.
As a rule, I like to grab my JavaScript and CSS from a content provider when feasible; to do so in this case, after the line:
<title><g:message code=""default.list.label"" args=""[entityName]"" /></title>
insert the following lines:
<script src=""https://code.jquery.com/jquery-2.2.4.min.js"" integrity=""sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="" crossorigin=""anonymous""></script>
<link rel=""stylesheet"" type=""text/css"" href=""https://cdn.datatables.net/1.10.16/css/jquery.dataTables.css"">
<script type=""text/javascript"" charset=""utf8"" src=""https://cdn.datatables.net/1.10.16/js/jquery.dataTables.js""></script>
<link rel=""stylesheet"" type=""text/css"" href=""https://cdn.datatables.net/scroller/1.4.4/css/scroller.dataTables.min.css"">
<script type=""text/javascript"" charset=""utf8"" src=""https://cdn.datatables.net/scroller/1.4.4/js/dataTables.scroller.min.js""></script>
<script type=""text/javascript"" charset=""utf8"" src=""https://cdn.datatables.net/buttons/1.5.1/js/dataTables.buttons.min.js""></script>
<script type=""text/javascript"" charset=""utf8"" src=""https://cdn.datatables.net/buttons/1.5.1/js/buttons.flash.min.js""></script>
<script type=""text/javascript"" charset=""utf8"" src=""https://cdnjs.cloudflare.com/ajax/libs/jszip/3.1.3/jszip.min.js""></script>
<script type=""text/javascript"" charset=""utf8"" src=""https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.32/pdfmake.min.js""></script>
<script type=""text/javascript"" charset=""utf8"" src=""https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.32/vfs_fonts.js""></script>
<script type=""text/javascript"" charset=""utf8"" src=""https://cdn.datatables.net/buttons/1.5.1/js/buttons.html5.min.js""></script>
<script type=""text/javascript"" charset=""utf8"" src=""https://cdn.datatables.net/buttons/1.5.1/js/buttons.print.min.js ""></script>
Next, remove the code that provided the data pagination in index.gsp:
<div id=""list-employee"" class=""content scaffold-list"" role=""main"">
<h1><g:message code=""default.list.label"" args=""[entityName]"" /></h1>
<g:if test=""${flash.message}"">
<div class=""message"" role=""status"">${flash.message}</div>
</g:if>
<f:table collection=""${employeeList}"" />

<div class=""pagination"">
<g:paginate total=""${employeeCount ?: 0}"" />
</div>
</div>
and insert the code that materializes the jQuery DataTables.
The first part to insert is the HTML that creates the basic tabular structure of the browser. For the application where DataTables talks to a database backend, provide only the table headers and footers; the DataTables JavaScript takes care of the table contents.
<div id=""employee-browser"" class=""content"" role=""main"">
<h1>Employee Browser</h1>
<table id=""employee_dt"" class=""display compact"" style=""width:99%;"">
<thead>
<tr>
<th>Surname</th>
<th>Given name(s)</th>
<th>Position</th>
<th>Office</th>
<th>Extension</th>
<th>Hired</th>
<th>Salary</th>
</tr>
</thead>
<tfoot>
<tr>
<th>Surname</th>
<th>Given name(s)</th>
<th>Position</th>
<th>Office</th>
<th>Extension</th>
<th>Hired</th>
<th>Salary</th>
</tr>
</tfoot>
</table>
</div>
Next, insert a JavaScript block, which serves three primary functions: It sets the size of the text boxes shown in the footer for column filtering, it establishes the DataTables table model, and it creates a handler to do the column filtering.
<g:javascript>
$('#employee_dt tfoot th').each( function() {
The code below handles sizing the filter boxes at the bottoms of the table columns:
var title = $(this).text();
if (title == 'Extension' || title == 'Hired')
$(this).html('<input type=""text"" size=""5"" placeholder=""' + title + '?"" />');
else
$(this).html('<input type=""text"" size=""15"" placeholder=""' + title + '?"" />');
});
Next, define the table model. This is where all the table options are provided, including the scrolling, rather than paginated, nature of the interface, the cryptic decorations to be provided according to the dom string, the ability to export data to CSV and other formats, as well as where the Ajax connection to the server is established. Note that the URL is created with a Groovy GString call to the Grails createLink() method, referring to the browserLister action in the EmployeeController. Also of interest is the definition of the columns of the table. This information is sent across to the back end, which queries the database and returns the appropriate records.
var table = $('#employee_dt').DataTable( {
""scrollY"": 500,
""deferRender"": true,
""scroller"": true,
""dom"": ""Brtip"",
""buttons"": [ 'copy', 'csv', 'excel', 'pdf', 'print' ],
""processing"": true,
""serverSide"": true,
""ajax"": {
""url"": ""${createLink(controller: 'employee', action: 'browserLister')}"",
""type"": ""POST"",
},
""columns"": [
{ ""data"": ""surname"" },
{ ""data"": ""givenNames"" },
{ ""data"": ""position"" },
{ ""data"": ""office"" },
{ ""data"": ""extension"" },
{ ""data"": ""hired"" },
{ ""data"": ""salary"" }
]
});
Finally, monitor the filter columns for changes and use them to apply the filter(s).
table.columns().every(function() {
var that = this;
$('input', this.footer()).on('keyup change', function(e) {
if (that.search() != this.value && 8 < e.keyCode && e.keyCode < 32)
that.search(this.value).draw();
});
And that’s it for the JavaScript. This completes the changes to the view code.
});
</g:javascript>
Here’s a screenshot of the UI this view creates.
The employee controller browserLister action
Recall that we saw this string
""${createLink(controller: 'employee', action: 'browserLister')}""
as the URL used for the Ajax calls from the DataTables table model. createLink() is the method behind a Grails tag that is used to dynamically generate a link as the HTML is preprocessed on the Grails server. This ends up generating a link to the EmployeeController, located in
embrow/grails-app/controllers/com/nuevaconsulting/embrow/EmployeeController.groovy
and specifically to the controller method browserLister(). I’ve left some print statements in the code so that the intermediate results can be seen in the terminal window where the application is running.
    def browserLister() {
        
        // Applies filters and sorting to return a list of desired employees
First, print out the parameters passed to browserLister(). I usually start building controller methods with this code so that I’m completely clear on what my controller is receiving.
      println ""employee browserLister params $params""
        println()
Next, process those parameters to put them in a more usable shape. First, the jQuery DataTables parameters, a Groovy map called jqdtParams:
        def jqdtParams = [:]
        params.each { key, value ->
            def keyFields = key.replace(']','').split(/\[/)
            def table = jqdtParams
            for (int f = 0; f < keyFields.size() - 1; f++) {
                def keyField = keyFields[f]
                if (!table.containsKey(keyField))
                    table[keyField] = [:]
                table = table[keyField]
            }
            table[keyFields[-1]] = value
        }
        println ""employee dataTableParams $jqdtParams""
        println()
Next, the column data, a Groovy map called columnMap:
        def columnMap = jqdtParams.columns.collectEntries { k, v ->
            def whereTerm = null
            switch (v.data) {
            case 'extension':
            case 'hired':
            case 'salary':
                if (v.search.value ==~ /\d+(,\d+)*/)
                    whereTerm = v.search.value.split(',').collect { it as Integer }
                break
            default:
                if (v.search.value ==~ /[A-Za-z0-9 ]+/)
                    whereTerm = ""%${v.search.value}%"" as String
                break
            }
            [(v.data): [where: whereTerm]]
        }
        println ""employee columnMap $columnMap""
        println()
Next, a list of all column names, retrieved from columnMap, and a corresponding list of how those columns should be ordered in the view, Groovy lists called allColumnList and orderList, respectively:
        def allColumnList = columnMap.keySet() as List
        println ""employee allColumnList $allColumnList""
        def orderList = jqdtParams.order.collect { k, v -> [allColumnList[v.column as Integer], v.dir] }
        println ""employee orderList $orderList""
We’re going to use Grails’ implementation of Hibernate criteria to actually carry out the selection of elements to be displayed as well as their ordering and pagination. Criteria requires a filter closure; in most examples, this is given as part of the creation of the criteria instance itself, but here we define the filter closure beforehand. Note in this case the relatively complex interpretation of the “date hired” filter, which is treated as a year and applied to establish date ranges, and the use of createAlias to allow us to reach into related classes Position and Office:
        def filterer = {
            createAlias 'position',        'p'
            createAlias 'office',          'o'

            if (columnMap.surname.where)    ilike  'surname',     columnMap.surname.where
            if (columnMap.givenNames.where) ilike  'givenNames',  columnMap.givenNames.where
            if (columnMap.position.where)   ilike  'p.name',      columnMap.position.where
            if (columnMap.office.where)     ilike  'o.name',      columnMap.office.where
            if (columnMap.extension.where)  inList 'extension',   columnMap.extension.where
            if (columnMap.salary.where)     inList 'salary',      columnMap.salary.where
            if (columnMap.hired.where) {
                if (columnMap.hired.where.size() > 1) {
                    or {
                        columnMap.hired.where.each {
                            between 'hired', Date.parse('yyyy/MM/dd',""${it}/01/01"" as String),
                                Date.parse('yyyy/MM/dd',""${it}/12/31"" as String)
                        }
                    }
                } else {
                    between 'hired', Date.parse('yyyy/MM/dd',""${columnMap.hired.where[0]}/01/01"" as String),
                        Date.parse('yyyy/MM/dd',""${columnMap.hired.where[0]}/12/31"" as String)
                }
            }
        }
At this point, it’s time to apply the foregoing. The first step is to get a total count of all the Employee instances, required by the pagination code:
        def recordsTotal = Employee.count()
        println ""employee recordsTotal $recordsTotal""
Next, apply the filter to the Employee instances to get the count of filtered results, which will always be less than or equal to the total number (again, this is for the pagination code):
        def c = Employee.createCriteria()
        def recordsFiltered = c.count {
            filterer.delegate = delegate
            filterer()
        }
        println ""employee recordsFiltered $recordsFiltered""
Once you have those two counts, you can get the actual filtered instances using the pagination and ordering information as well.
      def orderer = Employee.withCriteria {
            filterer.delegate = delegate
            filterer()
            orderList.each { oi ->
                switch (oi[0]) {
                case 'surname':    order 'surname',    oi[1]; break
                case 'givenNames': order 'givenNames', oi[1]; break
                case 'position':   order 'p.name',     oi[1]; break
                case 'office':     order 'o.name',     oi[1]; break
                case 'extension':  order 'extension',  oi[1]; break
                case 'hired':      order 'hired',      oi[1]; break
                case 'salary':     order 'salary',     oi[1]; break
                }
            }
            maxResults (jqdtParams.length as Integer)
            firstResult (jqdtParams.start as Integer)
        }
To be completely clear, the pagination code in JTables manages three counts: the total number of records in the data set, the number resulting after the filters are applied, and the number to be displayed on the page (whether the display is scrolling or paginated). The ordering is applied to all the filtered records and the pagination is applied to chunks of those filtered records for display purposes.
Next, process the results returned by the orderer, creating links to the Employee, Position, and Office instance in each row so the user can click on these links to get all the detail on the relevant instance:
        def dollarFormatter = new DecimalFormat('$##,###.##')
        def employees = orderer.collect { employee ->
            ['surname': ""<a href='${createLink(controller: 'employee', action: 'show', id: employee.id)}'>${employee.surname}</a>"",
                'givenNames': employee.givenNames,
                'position': ""<a href='${createLink(controller: 'position', action: 'show', id: employee.position?.id)}'>${employee.position?.name}</a>"",
                'office': ""<a href='${createLink(controller: 'office', action: 'show', id: employee.office?.id)}'>${employee.office?.name}</a>"",
                'extension': employee.extension,
                'hired': employee.hired.format('yyyy/MM/dd'),
                'salary': dollarFormatter.format(employee.salary)]
        }
And finally, create the result you want to return and give it back as JSON, which is what jQuery DataTables requires.
        def result = [draw: jqdtParams.draw, recordsTotal: recordsTotal, recordsFiltered: recordsFiltered, data: employees]
        render(result as JSON)
    }
That’s it.
If you’re familiar with Grails, this probably seems like more work than you might have originally thought, but there’s no rocket science here, just a lot of moving parts. However, if you haven’t had much exposure to Grails (or to Groovy), there’s a lot of new stuff to understand—closures, delegates, and builders, among other things.
In that case, where to start? The best place is to learn about Groovy itself, especially Groovy closures and Groovy delegates and builders. Then go back to the reading suggested above on Grails and Hibernate criteria queries.
Conclusions
jQuery DataTables make awesome tabular data browsers for Grails. Coding the view isn’t too tricky, but the PHP examples provided in the DataTables documentation take you only so far. In particular, they aren’t written with Grails programmers in mind, nor do they explore the finer details of using elements that are references to other classes (essentially lookup tables).
I’ve used this approach to make a couple of data browsers that allow the user to select which columns to view and accumulate record counts, or just to browse the data. The performance is good even in million-row tables on a relatively modest VPS.
One caveat: I have stumbled upon some problems with the various Hibernate criteria mechanisms exposed in Grails (see my other GitHub repositories), so care and experimentation is required. If all else fails, the alternative approach is to build SQL strings on the fly and execute them instead. As of this writing, I prefer to work with Grails criteria, unless I get into messy subqueries, but that may just reflect my relative lack of experience with subqueries in Hibernate."
JavaScript$AI$machinelearning,"5 trending open source machine learning JavaScript frameworks
Whether you're a JavaScript developer who wants to dive into machine learning or a machine learning expert who plans to use JavaScript, these open source frameworks may intrigue you.
The tremendous growth of the machine learning field has been driven by the availability of open source tools that allow developers to build applications easily. (For example, AndreyBu, who is from Germany and has more than five years of experience in machine learning, has been utilizing various open source frameworks to build captivating machine learning projects.)
Although the Python programming language powers most of the machine learning frameworks, JavaScript hasn’t been left behind. JavaScript developers have been using various frameworks for training and deploying machine learning models in the browser.
Here are the five trending open source machine learning frameworks in JavaScript.
1. TensorFlow.js
TensorFlow.js is an open source library that allows you to run machine learning programs completely in the browser. It is the successor of Deeplearn.js, which is no longer supported. TensorFlow.js improves on the functionalities of Deeplearn.js and empowers you to make the most of the browser for a deeper machine learning experience.
With the library, you can use versatile and intuitive APIs to define, train, and deploy models from scratch right in the browser. Furthermore, it automatically offers support for WebGL and Node.js.

If you have pre-existing trained models you want to import to the browser, TensorFlow.js will allow you do that. You can also retrain existing models without leaving the browser.
2. Machine learning tools
The machine learning tools library is a compilation of resourceful open source tools for supporting widespread machine learning functionalities in the browser. The tools provide support for several machine learning algorithms, including unsupervised learning, supervised learning, data processing, artificial neural networks (ANN), math, and regression.
If you are coming from a Python background and looking for something similar to Scikit-learn for JavaScript in-browser machine learning, this suite of tools could have you covered.
3. Keras.js
Keras.js is another trending open source framework that allows you to run machine learning models in the browser. It offers GPU mode support using WebGL. If you have models in Node.js, you’ll run them only in CPU mode. Keras.js also offers support for models trained using any backend framework, such as the Microsoft Cognitive Toolkit (CNTK).
Some of the Keras models that can be deployed on the client-side browser include Inception v3 (trained on ImageNet), 50-layer Residual Network (trained on ImageNet), and Convolutional variational auto-encoder (trained on MNIST).
4. Brain.js
Machine learning concepts are very math-heavy, which may discourage people from starting. The technicalities and jargons in this field may make beginners freak out. This is where Brain.js becomes important. It is an open source, JavaScript-powered framework that simplifies the process of defining, training, and running neural networks.
If you are a JavaScript developer who is completely new to machine learning, Brain.js could reduce your learning curve. It can be used with Node.js or in the client-side browser for training machine learning models. Some of the networks that Brain.js supports include feed-forward networks, Ellman networks, and Gated Recurrent Units networks.
5. STDLib
STDLib is an open source library for powering JavaScript and Node.js applications. If you are looking for a library that emphasizes in-browser support for scientific and numerical web-based machine learning applications, STDLib could suit your needs.
The library comes with comprehensive and advanced mathematical and statistical functions to assist you in building high-performing machine learning models. You can also use its expansive utilities for building applications and other libraries. Furthermore, if you want a framework for data visualization and exploratory data analysis, you’ll find STDLib worthwhile.
Conclusion
If you are a JavaScript developer who intends to delve into the exciting world of machine learning or a machine learning expert who intends to start using JavaScript, the above open source frameworks will intrigue you.
"
Python$tox,"Automate your Python code tests with tox
Learn more about solving common Python problems in our series covering seven PyPI libraries.
Python is one of the most popular programming languages in use today—and for good reasons: it's open source, it has a wide range of uses (such as web programming, business applications, games, scientific programming, and much more), and it has a vibrant and dedicated community supporting it. This community is the reason we have such a large, diverse range of software packages available in the Python Package Index (PyPI) to extend and improve Python and solve the inevitable glitches that crop up.
In this series, we'll look at seven PyPI libraries that can help you solve common Python problems. Today, we'll examine tox, a tool for automating tests on Python code.
tox
When writing Python code, it is good to have automated checks. While you could dump the rules for running the checks directly into the continuous integration (CI) environment, that's seldom the best place for it. Among other things, it is useful to run tests locally, using the same parameters the CI runs, to save CI time.
The tox project is designed to run different checks against different versions of Python and against different versions of dependencies. Very quickly, we find the limiting factor is not the flexibility of tox but the harsh realities of the combinatorial explosions of options!
For example, a simple tox configuration can run the same tests against several versions of Python.
[tox]
envlist = py36,py37
[testenv]
deps =
    pytest
commands =
    pytest mylibrary
Tox will automatically use the right version of the interpreter, based on the version of the environment, to create the virtual environment. Tox will automatically rebuild the virtual environment if it is missing or if the dependencies change.
It is possible to explicitly indicate the Python version in an environment.
[tox]
envlist = py36,py37,docs
[testenv]
deps =
    pytest
commands =
    pytest mylibrary
[testenv:docs]
changedir = docs
deps =
    sphinx
commands =
    sphinx-build -W -b html -d {envtmpdir}/doctrees . {envtmpdir}/html
basepython = python3.7
This example uses Sphinx to build documentation for the library. One nice thing is that the Sphinx library will be installed only in the docs virtual environment. If mylibrary imports on Sphinx but forgets to indicate an explicit dependency, the tests will, correctly, fail.
We can also use tox to run the tests with different versions of the dependencies.
[tox]
envlist = {py36,py37}-{minimum,current}
[testenv]
deps =
  minimum: thirdparty==1.0
  current: thirdparty
  pytest
commands =
    pytest mylibrary
This will run four different test runs: py36-minimum, py36-current, py37-minimum, and py37-current. This is useful in the case where our library depends on thirdparty>=1.0: every test run makes sure we are still compatible with the 1.0 version while also making sure the latest version does not break us.
It is also a good idea to run a linter in tox. For example, running Black will do the right thing.
[tox]
envlist = py36,py37,py36-black
[testenv]
deps =
    pytest
commands =
    pytest mylibrary
[testenv:py36-black]
deps =
    black
commands =
    black --check --diff mylibrary
By default, tox will run all test environments. But you can run just one environment; for example, if you only want to run Black, run tox -e py36-black.
If you have a Python library you care about, add tox.ini to your workflow to keep its quality high.
In the next article in this series, we'll look at flake8, a linter and linting platform that ensures consistency in Python code.
"
Python,"Add methods retroactively in Python with singledispatch
Learn more about solving common Python problems in our series covering seven PyPI libraries.
Python is one of the most popular programming languages in use today—and for good reasons: it's open source, it has a wide range of uses (such as web programming, business applications, games, scientific programming, and much more), and it has a vibrant and dedicated community supporting it. This community is the reason we have such a large, diverse range of software packages available in the Python Package Index (PyPI) to extend and improve Python and solve the inevitable glitches that crop up.
In this series, we'll look at seven PyPI libraries that can help you solve common Python problems. Today, we'll examine singledispatch, a library that allows you to add methods to Python libraries retroactively.
singledispatch
Imagine you have a ""shapes"" library with a Circle class, a Square class, etc.
A Circle has a radius, a Square has a side, and a Rectangle has height and width. Our library already exists; we do not want to change it.
However, we do want to add an area calculation to our library. If we didn't share this library with anyone else, we could just add an area method so we could call shape.area() and not worry about what the shape is.
While it is possible to reach into a class and add a method, this is a bad idea: nobody expects their class to grow new methods, and things might break in weird ways.
Instead, the singledispatch function in functools can come to our rescue.
@singledispatch
def get_area(shape):
    raise NotImplementedError(""cannot calculate area for unknown shape"",
                              shape)
The ""base"" implementation for the get_area function fails. This makes sure that if we get a new shape, we will fail cleanly instead of returning a nonsense result.
@get_area.register(Square)
def _get_area_square(shape):
    return shape.side ** 2
@get_area.register(Circle)
def _get_area_circle(shape):
    return math.pi * (shape.radius ** 2)
One nice thing about doing things this way is that if someone writes a new shape that is intended to play well with our code, they can implement get_area themselves.
from area_calculator import get_area

@attr.s(auto_attribs=True, frozen=True)
class Ellipse:
    horizontal_axis: float
    vertical_axis: float

@get_area.register(Ellipse)
def _get_area_ellipse(shape):
    return math.pi * shape.horizontal_axis * shape.vertical_axis
Calling get_area is straightforward.
print(get_area(shape))
This means we can change a function that has a long if isintance()/elif isinstance() chain to work this way, without changing the interface. The next time you are tempted to check if isinstance, try using singledispatch!
In the next article in this series, we'll look at tox, a tool for automating tests on Python code.
"
Python$API,"API evolution the right way
Ten covenants that responsible library authors keep with their users.
Imagine you are a creator deity, designing a body for a creature. In your benevolence, you wish for the creature to evolve over time: first, because it must respond to changes in its environment, and second, because your wisdom grows and you think of better designs for the beast. It shouldn't remain in the same body forever!
The creature, however, might be relying on features of its present anatomy. You can't add wings or change its scales without warning. It needs an orderly process to adapt its lifestyle to its new body. How can you, as a responsible designer in charge of this creature's natural history, gently coax it toward ever greater improvements?

It's the same for responsible library maintainers. We keep our promises to the people who depend on our code: we release bugfixes and useful new features. We sometimes delete features if that's beneficial for the library's future. We continue to innovate, but we don't break the code of people who use our library. How can we fulfill all those goals at once?
Add useful features
Your library shouldn't stay the same for eternity: you should add features that make your library better for your users. For example, if you have a Reptile class and it would be useful to have wings for flying, go for it.
class Reptile:
    @property
    def teeth(self):
        return 'sharp fangs'

    # If wings are useful, add them!
    @property
    def wings(self):
        return 'majestic wings'
But beware, features come with risk. Consider the following feature in the Python standard library, and see what went wrong with it.
bool(datetime.time(9, 30)) == True
bool(datetime.time(0, 0)) == False
This is peculiar: converting any time object to a boolean yields True, except for midnight. (Worse, the rules for timezone-aware times are even stranger.)
I've been writing Python for more than a decade but I didn't discover this rule until last week. What kind of bugs can this odd behavior cause in users' code?
Consider a calendar application with a function that creates events. If an event has an end time, the function requires it to also have a start time.
def create_event(day,
                 start_time=None,
                 end_time=None):
    if end_time and not start_time:
        raise ValueError(""Can't pass end_time without start_time"")

# The coven meets from midnight until 4am.
create_event(datetime.date.today(),
             datetime.time(0, 0),
             datetime.time(4, 0))
Unfortunately for witches, an event starting at midnight fails this validation. A careful programmer who knows about the quirk at midnight can write this function correctly, of course.
def create_event(day,
                 start_time=None,
                 end_time=None):
    if end_time is not None and start_time is None:
        raise ValueError(""Can't pass end_time without start_time"")
But this subtlety is worrisome. If a library creator wanted to make an API that bites users, a ""feature"" like the boolean conversion of midnight works nicely.
The responsible creator's goal, however, is to make your library easy to use correctly.
This feature was written by Tim Peters when he first made the datetime module in 2002. Even founding Pythonistas like Tim make mistakes. The quirk was removed, and all times are True now.
# Python 3.5 and later.

bool(datetime.time(9, 30)) == True
bool(datetime.time(0, 0)) == True
Programmers who didn't know about the oddity of midnight are saved from obscure bugs, but it makes me nervous to think about any code that relies on the weird old behavior and didn't notice the change. It would have been better if this bad feature were never implemented at all. This leads us to the first promise of any library maintainer:
First covenant: Avoid bad features
The most painful change to make is when you have to delete a feature. One way to avoid bad features is to add few features in general! Make no public method, class, function, or property without a good reason. Thus:
Second covenant: Minimize features
Features are like children: conceived in a moment of passion, they must be supported for years. Don't do anything silly just because you can. Don't add feathers to a snake!
But of course, there are plenty of occasions when users need something from your library that it does not yet offer. How do you choose the right feature to give them? Here's another cautionary tale.
A cautionary tale from asyncio
As you may know, when you call a coroutine function, it returns a coroutine object:
async def my_coroutine():
    pass

print(my_coroutine())
<coroutine object my_coroutine at 0x10bfcbac8>
Your code must ""await"" this object to run the coroutine. It's easy to forget this, so asyncio's developers wanted a ""debug mode"" that catches this mistake. Whenever a coroutine is destroyed without being awaited, the debug mode prints a warning with a traceback to the line where it was created.
When Yury Selivanov implemented the debug mode, he added as its foundation a ""coroutine wrapper"" feature. The wrapper is a function that takes in a coroutine and returns anything at all. Yury used it to install the warning logic on each coroutine, but someone else could use it to turn coroutines into the string ""hi!""
import sys

def my_wrapper(coro):
    return 'hi!'

sys.set_coroutine_wrapper(my_wrapper)

async def my_coroutine():
    pass

print(my_coroutine())
hi!
That is one hell of a customization. It changes the very meaning of ""async."" Calling set_coroutine_wrapper once will globally and permanently change all coroutine functions. It is, as Nathaniel Smith wrote, ""a problematic API"" that is prone to misuse and had to be removed. The asyncio developers could have avoided the pain of deleting the feature if they'd better shaped it to its purpose. Responsible creators must keep this in mind:
Third covenant: Keep features narrow
Luckily, Yury had the good judgment to mark this feature provisional, so asyncio users knew not to rely on it. Nathaniel was free to replace set_coroutine_wrapper with a narrower feature that only customized the traceback depth.
import sys

sys.set_coroutine_origin_tracking_depth(2)

async def my_coroutine():
    pass

print(my_coroutine())
<coroutine object my_coroutine at 0x10bfcbac8>

RuntimeWarning:'my_coroutine' was never awaited

Coroutine created at (most recent call last)
  File ""script.py"", line 8, in <module>
    print(my_coroutine())
This is much better. There's no more global setting that can change coroutines' type, so asyncio users need not code as defensively. Deities should all be as farsighted as Yury.
Fourth covenant: Mark experimental features ""provisional""
If you have merely a hunch that your creature wants horns and a quadruple-forked tongue, introduce the features but mark them ""provisional.""
You might discover that the horns are extraneous but the quadruple-forked tongue is useful after all. In the next release of your library, you can delete the former and mark the latter official.
Deleting features
No matter how wisely we guide our creature's evolution, there may come a time when it's best to delete an official feature. For example, you might have created a lizard, and now you choose to delete its legs. Perhaps you want to transform this awkward creature into a sleek and modern python.
There are two main reasons to delete features. First, you might discover a feature was a bad idea, through user feedback or your own growing wisdom. That was the case with the quirky behavior of midnight. Or, the feature might have been well-adapted to your library's environment at first, but the ecology changes. Perhaps another deity invents mammals. Your creature wants to squeeze into the mammals' little burrows and eat the tasty mammal filling, so it has to lose its legs.
Similarly, the Python standard library deletes features in response to changes in the language itself. Consider asyncio's Lock. It has been awaitable ever since ""await"" was added as a keyword:
lock = asyncio.Lock()

async def critical_section():
    await lock
    try:
        print('holding lock')
    finally:
        lock.release()
But now, we can do ""async with lock.""
lock = asyncio.Lock()

async def critical_section():
    async with lock:
        print('holding lock')
The new style is much better! It's short and less prone to mistakes in a big function with other try-except blocks. Since ""there should be one and preferably only one obvious way to do it,"" the old syntax is deprecated in Python 3.7 and it will be banned soon.
It's inevitable that ecological change will have this effect on your code, too, so learn to delete features gently. Before you do so, consider the cost or benefit of deleting it. Responsible maintainers are reluctant to make their users change a large amount of their code or change their logic. (Remember how painful it was when Python 3 removed the ""u"" string prefix, before it was added back.) If the code changes are mechanical, however, like a simple search-and-replace, or if the feature is dangerous, it may be worth deleting.
Whether to delete a feature

In the case of our hungry lizard, we decide to delete its legs so it can slither into a mouse's hole and eat it. How do we go about this? We could just delete the walk method, changing code from this:
class Reptile:
    def walk(self):
        print('step step step')
to this:
class Reptile:
    def slither(self):
        print('slide slide slide')
That's not a good idea; the creature is accustomed to walking! Or, in terms of a library, your users have code that relies on the existing method. When they upgrade to the latest version of your library, their code will break.
# User's code. Oops!
Reptile.walk()
Therefore, responsible creators make this promise:
Fifth covenant: Delete features gently
There are a few steps involved in deleting a feature gently. Starting with a lizard that walks with its legs, you first add the new method, ""slither."" Next, deprecate the old method.
import warnings

class Reptile:
    def walk(self):
        warnings.warn(
            ""walk is deprecated, use slither"",
            DeprecationWarning, stacklevel=2)
        print('step step step')

    def slither(self):
        print('slide slide slide')
The Python warnings module is quite powerful. By default it prints warnings to stderr, only once per code location, but you can silence warnings or turn them into exceptions, among other options.
As soon as you add this warning to your library, PyCharm and other IDEs render the deprecated method with a strikethrough. Users know right away that the method is due for deletion.
Reptile().walk()
What happens when they run their code with the upgraded library?
$ python3 script.py

DeprecationWarning: walk is deprecated, use slither
  script.py:14: Reptile().walk()

step step step
By default, they see a warning on stderr, but the script succeeds and prints ""step step step."" The warning's traceback shows what line of the user's code must be fixed. (That's what the ""stacklevel"" argument does: it shows the call site that users need to change, not the line in your library where the warning is generated.) Notice that the error message is instructive, it describes what a library user must do to migrate to the new version.
Your users will want to test their code and prove they call no deprecated library methods. Warnings alone won't make unit tests fail, but exceptions will. Python has a command-line option to turn deprecation warnings into exceptions.
> python3 -Werror::DeprecationWarning script.py

Traceback (most recent call last):
  File ""script.py"", line 14, in <module>
    Reptile().walk()
  File ""script.py"", line 8, in walk
    DeprecationWarning, stacklevel=2)
DeprecationWarning: walk is deprecated, use slither
Now, ""step step step"" is not printed, because the script terminates with an error.
So, once you've released a version of your library that warns about the deprecated ""walk"" method, you can delete it safely in the next release. Right?
Consider what your library's users might have in their projects' requirements.
# User's requirements.txt has a dependency on the reptile package.
reptile
The next time they deploy their code, they'll install the latest version of your library. If they haven't yet handled all deprecations, then their code will break, because it still depends on ""walk."" You need to be gentler than this. There are three more promises you must keep to your users: maintain a changelog, choose a version scheme, and write an upgrade guide.
Sixth covenant: Maintain a changelog
Your library must have a changelog; its main purpose is to announce when a feature that your users rely on is deprecated or deleted.

Responsible creators use version numbers to express how a library has changed so users can make informed decisions about upgrading. A ""version scheme"" is a language for communicating the pace of change.
Seventh covenant: Choose a version scheme
There are two schemes in widespread use, semantic versioning and time-based versioning. I recommend semantic versioning for nearly any library. The Python flavor thereof is defined in PEP 440, and tools like pip understand semantic version numbers.
If you choose semantic versioning for your library, you can delete its legs gently with version numbers like:
1.0: First ""stable"" release, with walk()
1.1: Add slither(), deprecate walk()
2.0: Delete walk()
Your users should depend on a range of your library's versions, like so:
# User's requirements.txt.
reptile>=1,<2
This allows them to upgrade automatically within a major release, receiving bugfixes and potentially raising some deprecation warnings, but not upgrading to the next major release and risking a change that breaks their code.
If you follow time-based versioning, your releases might be numbered thus:
2017.06.0: A release in June 2017
2018.11.0: Add slither(), deprecate walk()
2019.04.0: Delete walk()
And users can depend on your library like:
# User's requirements.txt for time-based version.
reptile==2018.11.*
This is terrific, but how do your users know your versioning scheme and how to test their code for deprecations? You have to advise them how to upgrade.
Eighth covenant: Write an upgrade guide
Here's how a responsible library creator might guide users:

You must teach users how to handle deprecation warnings by showing them the command line options. Not all Python programmers know this—I certainly have to look up the syntax each time. And take note, you must release a version that prints warnings from each deprecated API so users can test with that version before upgrading again. In this example, version 1.1 is the bridge release. It allows your users to rewrite their code incrementally, fixing each deprecation warning separately until they have entirely migrated to the latest API. They can test changes to their code and changes in your library, independently from each other, and isolate the cause of bugs.
If you chose semantic versioning, this transitional period lasts until the next major release, from 1.x to 2.0, or from 2.x to 3.0, and so on. The gentle way to delete a creature's legs is to give it at least one version in which to adjust its lifestyle. Don't remove the legs all at once!
Version numbers, deprecation warnings, the changelog, and the upgrade guide work together to gently evolve your library without breaking the covenant with your users. The Twisted project's Compatibility Policy explains this beautifully:
""The First One's Always Free""
Any application which runs without warnings may be upgraded one minor version of Twisted.
In other words, any application which runs its tests without triggering any warnings from Twisted should be able to have its Twisted version upgraded at least once with no ill effects except the possible production of new warnings.
Now, we creator deities have gained the wisdom and power to add features by adding methods and to delete them gently. We can also add features by adding parameters, but this brings a new level of difficulty. Are you ready?
Adding parameters
Imagine that you just gave your snake-like creature a pair of wings. Now you must allow it the choice whether to move by slithering or flying. Currently its ""move"" function takes one parameter.
# Your library code.
def move(direction):
    print(f'slither {direction}')

# A user's application.
move('north')
You want to add a ""mode"" parameter, but this breaks your users' code if they upgrade, because they pass only one argument.
# Your library code.
def move(direction, mode):
    assert mode in ('slither', 'fly')
    print(f'{mode} {direction}')

# A user's application. Error!
move('north')
A truly wise creator promises not to break users' code this way.
Ninth covenant: Add parameters compatibly
To keep this covenant, add each new parameter with a default value that preserves the original behavior.
# Your library code.
def move(direction, mode='slither'):
    assert mode in ('slither', 'fly')
    print(f'{mode} {direction}')

# A user's application.
move('north')
Over time, parameters are the natural history of your function's evolution. They're listed oldest first, each with a default value. Library users can pass keyword arguments to opt into specific new behaviors and accept the defaults for all others.
# Your library code.
def move(direction,
         mode='slither',
         turbo=False,
         extra_sinuous=False,
         hail_lyft=False):
    # ...

# A user's application.
move('north', extra_sinuous=True)
There is a danger, however, that a user might write code like this:
# A user's application, poorly-written.
move('north', 'slither', False, True)
What happens if, in the next major version of your library, you get rid of one of the parameters, like ""turbo""?
# Your library code, next major version. ""turbo"" is deleted.
def move(direction,
         mode='slither',
         extra_sinuous=False,
         hail_lyft=False):
    # ...


# A user's application, poorly-written.
move('north', 'slither', False, True)
The user's code still compiles, and this is a bad thing. The code stopped moving extra-sinuously and started hailing a Lyft, which was not the intention. I trust that you can predict what I'll say next: Deleting a parameter requires several steps. First, of course, deprecate the ""turbo"" parameter. I like a technique like this one, which detects whether any user's code relies on this parameter.
# Your library code.
_turbo_default = object()

def move(direction,
         mode='slither',
         turbo=_turbo_default,
         extra_sinuous=False,
         hail_lyft=False):
    if turbo is not _turbo_default:
        warnings.warn(
            ""'turbo' is deprecated"",
            DeprecationWarning,
            stacklevel=2)
    else:
        # The old default.
        turbo = False
But your users might not notice the warning. Warnings are not very loud: they can be suppressed or lost in log files. Users might heedlessly upgrade to the next major version of your library, the version that deletes ""turbo."" Their code will run without error and silently do the wrong thing! As the Zen of Python says, ""Errors should never pass silently."" Indeed, reptiles hear poorly, so you must correct them very loudly when they make mistakes.
The best way to protect your users is with Python 3's star syntax, which requires callers to pass keyword arguments.
# Your library code.
# All arguments after ""*"" must be passed by keyword.
def move(direction,
         *,
         mode='slither',
         turbo=False,
         extra_sinuous=False,
         hail_lyft=False):
    # ...

# A user's application, poorly-written.
# Error! Can't use positional args, keyword args required.
move('north', 'slither', False, True)
With the star in place, this is the only syntax allowed:
# A user's application.
move('north', extra_sinuous=True)
Now when you delete ""turbo,"" you can be certain any user code that relies on it will fail loudly. If your library also supports Python 2, there's no shame in that; you can simulate the star syntax thus (credit to Brett Slatkin):
# Your library code, Python 2 compatible.
def move(direction, **kwargs):
    mode = kwargs.pop('mode', 'slither')
    turbo = kwargs.pop('turbo', False)
    sinuous = kwargs.pop('extra_sinuous', False)
    lyft = kwargs.pop('hail_lyft', False)

    if kwargs:
        raise TypeError('Unexpected kwargs: %r'
                        % kwargs)

    # ...
Requiring keyword arguments is a wise choice, but it requires foresight. If you allow an argument to be passed positionally, you cannot convert it to keyword-only in a later release. So, add the star now. You can observe in the asyncio API that it uses the star pervasively in constructors, methods, and functions. Even though ""Lock"" only takes one optional parameter so far, the asyncio developers added the star right away. This is providential.
# In asyncio.
class Lock:
    def __init__(self, *, loop=None):
        # ...
Now we've gained the wisdom to change methods and parameters while keeping our covenant with users. The time has come to try the most challenging kind of evolution: changing behavior without changing either methods or parameters.
Changing behavior
Let's say your creature is a rattlesnake, and you want to teach it a new behavior.
Sidewinding! The creature's body will appear the same, but its behavior will change. How can we prepare it for this step of its evolution?

neonate_sidewinder_sidewinding_with_tracks_unlabeled.png

Image by HCA [CC BY-SA 4.0], via Wikimedia Commons, modified by Opensource.com
A responsible creator can learn from the following example in the Python standard library, when behavior changed without a new function or parameters. Once upon a time, the os.stat function was introduced to get file statistics, like the creation time. At first, times were always integers.
>>> os.stat('file.txt').st_ctime
1540817862
One day, the core developers decided to use floats for os.stat times to give sub-second precision. But they worried that existing user code wasn't ready for the change. They created a setting in Python 2.3, ""stat_float_times,"" that was false by default. A user could set it to True to opt into floating-point timestamps.
>>> # Python 2.3.
>>> os.stat_float_times(True)
>>> os.stat('file.txt').st_ctime
1540817862.598021
Starting in Python 2.5, float times became the default, so any new code written for 2.5 and later could ignore the setting and expect floats. Of course, you could set it to False to keep the old behavior or set it to True to ensure the new behavior in all Python versions, and prepare your code for the day when stat_float_times is deleted.
Ages passed. In Python 3.1, the setting was deprecated to prepare people for the distant future and finally, after its decades-long journey, the setting was removed. Float times are now the only option. It's a long road, but responsible deities are patient because we know this gradual process has a good chance of saving users from unexpected behavior changes.
Tenth covenant: Change behavior gradually
Here are the steps:
Add a flag to opt into the new behavior, default False, warn if it's False
Change default to True, deprecate flag entirely
Remove the flag
If you follow semantic versioning, the versions might be like so:



You need two major releases to complete the maneuver. If you had gone straight from ""Add flag, default False, warn if it's False"" to ""Remove flag"" without the intervening release, your users' code would be unable to upgrade. User code written correctly for 1.1, which sets the flag to True and handles the new behavior, must be able to upgrade to the next release with no ill effect except new warnings, but if the flag were deleted in the next release, that code would break. A responsible deity never violates the Twisted policy: ""The First One's Always Free.""
The responsible creator
Our 10 covenants belong loosely in three categories:
Evolve cautiously
Avoid bad features
Minimize features
Keep features narrow
Mark experimental features ""provisional""
Delete features gently
Record history rigorously
Maintain a changelog
Choose a version scheme
Write an upgrade guide
Change slowly and loudly
Add parameters compatibly
Change behavior gradually
If you keep these covenants with your creature, you'll be a responsible creator deity. Your creature's body can evolve over time, forever improving and adapting to changes in its environment but without sudden changes the creature isn't prepared for. If you maintain a library, keep these promises to your users and you can innovate your library without breaking the code of the people who rely on you.


"
Python$attrs,"Say goodbye to boilerplate in Python with attrs
Learn more about solving common Python problems in our series covering seven PyPI libraries.
Python is one of the most popular programming languages in use today—and for good reasons: it's open source, it has a wide range of uses (such as web programming, business applications, games, scientific programming, and much more), and it has a vibrant and dedicated community supporting it. This community is the reason we have such a large, diverse range of software packages available in the Python Package Index (PyPI) to extend and improve Python and solve the inevitable glitches that crop up.
In this series, we'll look at seven PyPI libraries that can help you solve common Python problems. Today, we'll examine attrs, a Python package that helps you write concise, correct code quickly.
attrs
If you have been using Python for any length of time, you are probably used to writing code like:
class Book(object):

    def __init__(self, isbn, name, author):
        self.isbn = isbn
        self.name = name
        self.author = author
Then you write a __repr__ function; otherwise, it would be hard to log instances of Book:
    def __repr__(self):
        return f""Book({self.isbn}, {self.name}, {self.author})""
Next, you write a nice docstring documenting the expected types. But you notice you forgot to add the edition and published_year attributes, so you have to modify them in five places.
What if you didn't have to?
@attr.s(auto_attribs=True)
class Book(object):
    isbn: str
    name: str
    author: str
    published_year: int
    edition: int
Annotating the attributes with types using the new type annotation syntax, attrs detects the annotations and creates a class.
ISBNs have a specific format. What if we want to enforce that format?
@attr.s(auto_attribs=True)
class Book(object):
    isbn: str = attr.ib()
    @isbn.validator
    def pattern_match(self, attribute, value):
        m = re.match(r""^(\d{3}-)\d{1,3}-\d{2,3}-\d{1,7}-\d$"", value)
        if not m:
            raise ValueError(""incorrect format for isbn"", value)
    name: str 
    author: str
    published_year: int
    edition: int
The attrs library also has great support for immutability-style programming. Changing the first line to @attr.s(auto_attribs=True, frozen=True) means that Book is now immutable: trying to modify an attribute will raise an exception. Instead, we can get a new instance with modification using attr.evolve(old_book, published_year=old_book.published_year+1), for example, if we need to push publication forward by a year.
"
Python,"Format Python however you like with Black
Learn more about solving common Python problems in our series covering seven PyPI libraries.
Python is one of the most popular programming languages in use today—and for good reasons: it's open source, it has a wide range of uses (such as web programming, business applications, games, scientific programming, and much more), and it has a vibrant and dedicated community supporting it. This community is the reason we have such a large, diverse range of software packages available in the Python Package Index (PyPI) to extend and improve Python and solve the inevitable glitches that crop up.
In this series, we'll look at seven PyPI libraries that can help you solve common Python problems. In the first article, we learned about Cython; today, we'll examine the Black code formatter.
Black
Sometimes creativity can be a wonderful thing. Sometimes it is just a pain. I enjoy solving hard problems creatively, but I want my Python formatted as consistently as possible. Nobody has ever been impressed by code that uses ""interesting"" indentation.
But even worse than inconsistent formatting is a code review that consists of nothing but formatting nits. It is annoying to the reviewer—and even more annoying to the person whose code is reviewed. It's also infuriating when your linter tells you that your code is indented incorrectly, but gives no hint about the correct amount of indentation.
Enter Black. Instead of telling you what to do, Black is a good, industrious robot: it will fix your code for you.
To see how it works, feel free to write something beautifully inconsistent like:
def add(a, b): return a+b

def mult(a, b):
      return \
        a              *        b
Does Black complain? Goodness no, it just fixes it for you!
$ black math 
reformatted math
All done! ✨ 🍰 ✨
1 file reformatted.
$ cat math 
def add(a, b):
    return a + b


def mult(a, b):
    return a * b
Black does offer the option of failing instead of fixing and even outputting a diff-style edit. These options are great in a continuous integration (CI) system that enforces running Black locally. In addition, if the diff output is logged to the CI output, you can directly paste it into patch in the rare case that you need to fix your output but cannot install Black locally.
$ black --check --diff bad 
--- math 2019-04-09 17:24:22.747815 +0000
+++ math 2019-04-09 17:26:04.269451 +0000
@@ -1,7 +1,7 @@
-def add(a, b): return a + b
+def add(a, b):
+    return a + b
 
 
 def mult(a, b):
-          return \
-                  a             *             b
+    return a * b
 
would reformat math
All done! 💥 💔 💥
1 file would be reformatted.
$ echo $?
1
"
Python$MacOS,"The right and wrong way to set Python 3 as default on MacOS
There are several ways to get started with Python 3 on MacOS, but one way is better than the others.
I've been dipping my toe back into Python development as I get ready to head to PyCon US. (If you're headed there as well and want to share your Python story, let me know!) When I installed a module to tinker around with, I got a reminder that I needed to install Python 3 soon.
$ pip install todoist-python 
DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.
So, I did what any of us would do and googled around looking for a guide to update my development environment, which runs on MacOS (formerly known as OS X). To my surprise, I found only a handful of StackOverflow posts, and they pointed me to partial solutions. Here's the full story of how to set up your environment without breaking anything built into the MacOS operating system.
A word to the wise: if you're looking to install Python 3 the ""right"" way, skip down to that section. I will start by covering other ways that seem right but are not a good idea. Skip to the end of this article if you're short on time and just want the recommended way.
What's so hard about this?
The version of Python that ships with MacOS is well out of date from what Python recommends using for development. Python runtimes are also comically challenging at times, as noted by XKCD.
So what's the plan? I have dozens of Python interpreters on my computer already, and I have no idea how to manage them effectively. I didn't want to download the latest release, move it into my path, and call it a day (or use brew install python3, which would do something similar). I figured it would cause breakages down the line in a really frustrating way that I wouldn't know how to troubleshoot. I thought the best path forward was to rip and replace whatever version of Python I was running to make a clear and definitive switch to the latest and greatest.
What NOT to do
My first idea on how to make Python 3 the default Python on my system was to move the old version and add the new one:
# what I thought would work 
# first, I'll find my python binary 
$ which python 
/usr/bin/python 
# next, I'll move it to an unused name 
$ sudo mv /usr/bin/python /usr/bin/python2 
# lastly, I'll move the new binary to the previous path 
$ sudo mv $PATHTOBINARY/python3 /usr/bin/python
The pattern followed what /usr/bin/ usually does between major releases of Python, but I quickly learned it was the wrong move:
$ sudo mv /usr/bin/python /usr/bin/python2 
mv: rename /usr/bin/python to /usr/bin/python2: Operation not permitted
Thankfully, MacOS protected me from breaking something I don't fully understand. Further research proves this is exactly what we shouldn't do.
What we could do (but also shouldn't)
Now that we know what not to do, let's look at what we could do. There are a couple options when we think about common installation patterns for applications on MacOS.
Use Python 3 as the MacOS default
Python's website has a MacOS Python 3 installer we can download and use. If we use the package installation, a python3 fill will be at available in /usr/local/bin/.
Aliasing is a must since the Python binary stored in /usr/bin/ can't be changed. What's nice about an alias is that it's specific to our command-line shell. Since I use zsh by default, I put the following into the .zshrc file:
$ echo ""alias python=/usr/local/bin/python3.7"" >> ~/.zshrc
If you are using the default Bash shell, you can append this same text to your .bashrc:
$ echo ""alias python=/usr/local/bin/python3.7"" >> ~/.bashrc
This strategy works, but it isn't ideal for making future updates to Python. It means we have to remember to check the website and download the new files since Python doesn't include a command-line way to update.
Have Homebrew manage Python 3
The Homebrew project provides a free and open source package manager for MacOS that many people rely on. It gives Apple users a power similar to apt-get or yum. If you are a Homebrew user, you may already have Python installed. To quickly check, run:
$ brew list | grep python 
python
If Python shows up under the command, it's installed. What version is it? Let's check:
$ brew info python 
python: stable 3.7.3 (bottled), HEAD 
Interpreted, interactive, object-oriented programming language 
https://www.python.org/ 
/usr/local/Cellar/python/3.7.2_1 (8,437 files, 118MB) * 
## further output not included ##
Okay, great! The Homebrew maintainers have updated the default Python bottle to point to the latest release. Since the Homebrew maintainers are more dependable at updating the release than most of us, we can use Homebrew's version of Python 3 with the following command:
$ brew update && brew upgrade python
Now we want to point our alias (from above) to the copy of Python that Homebrew manages:
# If you added the previous alias, use a text editor to update the line to the following 
alias python=/usr/local/bin/python3
To make sure the path above points to where Homebrew installed Python in our environment, we can run brew info python and look for the path information.
This method, of using Homebrew to manage our Python environment, is a good starting place, and it made sense to me at the time.
What if we still need Python 2?
It makes sense for anyone new to Python to begin with Python 3. But those of us who still need Python 2—for example, to contribute to a Python project that's only available in Python 2—can continue to use the default MacOS Python binary available in /usr/bin/python:
$ /usr/bin/python 
>>> print(""This runtime still works!"") 
This runtime still works!
Homebrew is so wonderful, it even offers a different formula for Python 2:
# If you need Homebrew's Python 2.7 run 
$ brew install python@2
At any time, we can remove the aliases from our shell's configuration file to go back to using the default copy of Python on the system.
Don't forget to update pip to pip3!
The pip command is the default package manager specifically for Python packages. Although we changed our default Python command to be version 3, we have to alias our pip command separately if it's on the previous version. First, we need to check what version we're on:
# Note that this is a capital V (not lowercase) 
$ pip -V 
pip 19.0.3 from /Library/Python/2.7/site-packages/pip-19.0.3-py2.7.egg/pip (python 2.7)
To ensure we're installing packages compatible with our new version of Python, we'll use another alias to point to the compatible version of pip. Since we're using Homebrew as our package manager in this situation, we know it installed pip3 when we installed Python 3. The default path should be the same as Python 3, but we can confirm this by asking the shell to find it:
$ which pip3 
/usr/local/bin/pip3
Now that we know the location, we will add it to our shell configuration file, as we did before:
$ echo ""alias pip= /usr/local/bin/pip3"" >> ~/.zshrc  
# or for Bash 
$ echo ""alias pip= /usr/local/bin/pip3"" >> ~/.bashrc
Last, we can confirm that running pip points to pip3 by opening a new shell or by resetting our current shell and seeing what we point to:
# This command reloads my zsh without exiting the session 
$ exec /bin/zsh -l 
# Now we can look to see where pip points us 
$ which pip 
pip: aliased to /usr/local/bin/pip3
We can avoid using Homebrew to update pip, but that requires a much longer tutorial from the Python documentation.
What we should do
When asking for a technical review of this article, Moshe Zadka gave me a warning that my solution could result in an unreliable idea of which Python is running that depends too closely on shells loading aliases. I knew Moshe was familiar with Python, but I didn't know is that he is an author of many Python tutorials as well as an upcoming book on Python development on MacOS. He helped 40 colleagues develop Python safely and consistently on MacOS systems following one core principle:
""The basic premise of all Python development is to never use the system Python. You do not want the Mac OS X 'default Python' to be 'python3.' You want to never care about default Python.""
How do we stop caring about the default? Moshe recommends using pyenv to manage Python environments. This tool will manage multiple versions of Python and is described as ""simple, unobtrusive, and follows the Unix tradition of single-purpose tools that do one thing well.""
While other installation options are available, the easiest way to get started is with Homebrew:
$ brew install pyenv 
🍺  /usr/local/Cellar/pyenv/1.2.10: 634 files, 2.4MB
Now let's install the latest Python version (3.7.3 as of this writing):
$ pyenv install 3.7.3
python-build: use openssl 1.0 from homebrew
python-build: use readline from homebrew
Downloading Python-3.7.3.tar.xz...
-> https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tar.xz
Installing Python-3.7.3...
## further output not included ##
Now that Python 3 is installed through pyenv, we want to set it as our global default version for pyenv environments:
$ pyenv global 3.7.3
# and verify it worked 
$ pyenv version
3.7.3 (set by /Users/mbbroberg/.pyenv/version)
The power of pyenv comes from its control over our shell's path. In order for it to work correctly, we need to add the following to our configuration file (.zshrc for me, possibly .bash_profile for you):
$ echo -e 'if command -v pyenv 1>/dev/null 2>&1; then\n  eval ""$(pyenv init -)""\nfi' >> ~/.zshrc
We also need to remove the aliases we used in the sections above since they would prevent using pyenv correctly. After removing them, we can confirm pyenv is managing our Python 3 version:
# I start by resetting the shell
$ exec $SHELL 
$ which python
/Users/mbbroberg/.pyenv/shims/python
$ python -V
Python 3.7.3
$ pip -V
pip 19.0.3 from /Users/mbbroberg/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pip (python 3.7)
Now we know for certain that we're using Python 3.7.3 and pip will update alongside it without any manual aliasing between versions. Using Moshe's recommendation to use a version manager (pyenv) enables us to easily accept future upgrades without getting confused about which Python we are running at a given time.
Do it right from the start
If you are just getting started with Python development on a MacOS, do the necessary configurations to make sure you're using the right version of Python from the start. Installing Python 3, with or without Homebrew, and using alias will let you start coding, but it's not a good strategy for the long run. Using pyenv as a simple version management solution to get you off to a good start. We will go into more about pyenv next time.

"
Python$C,"Write faster C extensions for Python with Cython
Learn more about solving common Python problems in our series covering seven PyPI libraries.
Python is one of the most popular programming languages in use today—and for good reasons: it's open source, it has a wide range of uses (such as web programming, business applications, games, scientific programming, and much more), and it has a vibrant and dedicated community supporting it. This community is the reason we have such a large, diverse range of software packages available in the Python Package Index (PyPI) to extend and improve Python and solve the inevitable glitches that crop up.
In this series, we'll look at seven PyPI libraries that can help you solve common Python problems. First up: Cython, a language that simplifies writing C extensions for Python.

Cython
Python is fun to use, but sometimes, programs written in it can be slow. All the runtime dynamic dispatching comes with a steep price: sometimes it's up to 10-times slower than equivalent code written in a systems language like C or Rust.
Moving pieces of code to a completely new language can have a big cost in both effort and reliability: All that manual rewrite work will inevitably introduce bugs. Can we have our cake and eat it too?
To have something to optimize for this exercise, we need something slow. What can be slower than an accidentally exponential implementation of the Fibonacci sequence?
def fib(n):
    if n < 2:
        return 1
    return fib(n-1) + fib(n-2)
Since a call to fib results in two calls, this beautifully inefficient algorithm takes a long time to execute. For example, on my new laptop, fib(36) takes about 4.5 seconds. These 4.5 seconds will be our baseline as we explore how Python's Cython extension can help.
The proper way to use Cython is to integrate it into setup.py. However, a quick and easy way to try things out is with pyximport. Let's put the fib code above in fib.pyx and run it using Cython.
>>> import pyximport; pyximport.install()
>>> import fib
>>> fib.fib(36)
Just using Cython with no code changes reduced the time the algorithm takes on my laptop to around 2.5 seconds. That's a reduction of almost 50% runtime with almost no effort; certainly, a scrumptious cake to eat and have!
Putting in a little more effort, we can make things even faster.
cpdef int fib(int n):
    if n < 2:
        return 1
    return fib(n - 1) + fib(n - 2)
We moved the code in fib to a function defined with cpdef and added a couple of type annotations: it takes an integer and returns an integer.
This makes it much faster—around 0.05 seconds. It's so fast that I may start suspecting my measurement methods contain noise: previously, this noise was lost in the signal.
So, the next time some of your Python code spends too long on the CPU, maybe spinning up some fans in the process, why not see if Cython can fix things?
In the next article in this series, we'll look at Black, a project that automatically corrects format errors in your code."
OpenStack$DNS,"Building a DNS-as-a-service with OpenStack Designate
Learn how to install and configure Designate, a multi-tenant DNS-as-a-service (DNSaaS) for OpenStack.
Designate is a multi-tenant DNS-as-a-service that includes a REST API for domain and record management, a framework for integration with Neutron, and integration support for Bind9.
You would want to consider a DNSaaS for the following:
A clean REST API for managing zones and records
Automatic records generated (with OpenStack integration)
Support for multiple authoritative name servers
Hosting multiple projects/organizations
This article explains how to manually install and configure the latest release of Designate service on CentOS or Red Hat Enterprise Linux 7 (RHEL 7), but you can use the same configuration on other distributions.
Install Designate on OpenStack
I have Ansible roles for bind and Designate that demonstrate the setup in my GitHub repository.
This setup presumes bind service is external (even though you can install bind locally) on the OpenStack controller node.
Install Designate's packages and bind (on OpenStack controller): # yum install openstack-designate-* bind bind-utils -y 
Create the Designate database and user: MariaDB [(none)]> CREATE DATABASE designate CHARACTER SET utf8 COLLATE utf8_general_ci;
       
MariaDB [(none)]> GRANT ALL PRIVILEGES ON designate.* TO \
'designate'@'localhost' IDENTIFIED BY 'rhlab123';

MariaDB [(none)]> GRANT ALL PRIVILEGES ON designate.* TO 'designate'@'%' \
IDENTIFIED BY 'rhlab123';  
Note: Bind packages must be installed on the controller side for Remote Name Daemon Control (RNDC) to function properly.
Configure bind (DNS server)
Generate RNDC files: rndc-confgen -a -k designate -c /etc/rndc.key -r /dev/urandom

cat <<EOF> etcrndc.conf
include ""/etc/rndc.key"";
options {
   default-key ""designate"";
   default-server {{ DNS_SERVER_IP }};
   default-port 953;
};
EOF  
Add the following into named.conf: include ""/etc/rndc.key""; controls { inet {{ DNS_SERVER_IP }} allow { localhost;{{ CONTROLLER_SERVER_IP }}; } keys { ""designate""; }; };  In the option section, add: options {
...
 allow-new-zones yes;
 request-ixfr no;
 listen-on port 53 { any; };
 recursion no;
 allow-query { 127.0.0.1; {{ CONTROLLER_SERVER_IP }}; };
};   Add the right permissions:  chown named:named /etc/rndc.key
 chown named:named /etc/rndc.conf
 chmod 600 /etc/rndc.key
 chown -v root:named /etc/named.conf
 chmod g+w /var/named


# systemctl restart named
# setsebool named_write_master_zones 1  
Push rndc.key and rndc.conf into the OpenStack controller: # scp -r /etc/rndc* {{ CONTROLLER_SERVER_IP }}:/etc/ 
Create OpenStack Designate service and endpoints
Enter:
# openstack user create --domain default --password-prompt designate
# openstack role add --project services --user designate admin
# openstack service create --name designate --description ""DNS"" dns

# openstack endpoint create --region RegionOne dns public http://{{ CONTROLLER_SERVER_IP }}:9001/
# openstack endpoint create --region RegionOne dns internal http://{{ CONTROLLER_SERVER_IP }}:9001/  
# openstack endpoint create --region RegionOne dns admin http://{{ CONTROLLER_SERVER_IP }}:9001/
Configure Designate service
Edit /etc/designate/designate.conf:
In the [service:api] section, configure auth_strategy: [service:api]
listen = 0.0.0.0:9001
auth_strategy = keystone
api_base_uri = http://{{ CONTROLLER_SERVER_IP }}:9001/
enable_api_v2 = True
enabled_extensions_v2 = quotas, reports  
In the [keystone_authtoken] section, configure the following options: [keystone_authtoken]
auth_type = password
username = designate
password = rhlab123
project_name = service
project_domain_name = Default
user_domain_name = Default
www_authenticate_uri = http://{{ CONTROLLER_SERVER_IP }}:5000/
auth_url = http://{{ CONTROLLER_SERVER_IP }}:5000/  
In the [service:worker] section, enable the worker model: enabled = True
notify = True  
In the [storage:sqlalchemy] section, configure database access: [storage:sqlalchemy]
connection = mysql+pymysql://designate:rhlab123@{{ CONTROLLER_SERVER_IP }}/designate  
Populate the Designate database: # su -s /bin/sh -c ""designate-manage database sync"" designate 
Create Designate's pools.yaml file (has target and bind details):
Edit /etc/designate/pools.yaml: - name: default
  # The name is immutable. There will be no option to change the name after
  # creation and the only way will to change it will be to delete it
  # (and all zones associated with it) and recreate it.
  description: Default Pool

  attributes: {}

  # List out the NS records for zones hosted within this pool
  # This should be a record that is created outside of designate, that
  # points to the public IP of the controller node.
  ns_records:
    - hostname: {{Controller_FQDN}}. # Thisis mDNS
      priority: 1

  # List out the nameservers for this pool. These are the actual BIND servers.
  # We use these to verify changes have propagated to all nameservers.
  nameservers:
    - host: {{ DNS_SERVER_IP }}
      port: 53

  # List out the targets for this pool. For BIND there will be one
  # entry for each BIND server, as we have to run rndc command on each server
  targets:
    - type: bind9
      description: BIND9 Server 1

      # List out the designate-mdns servers from which BIND servers should
      # request zone transfers (AXFRs) from.
      # This should be the IP of the controller node.
      # If you have multiple controllers you can add multiple masters
      # by running designate-mdns on them, and adding them here.
      masters:
        - host: {{ CONTROLLER_SERVER_IP }}
          port: 5354

      # BIND Configuration options
      options:
        host: {{ DNS_SERVER_IP }}
        port: 53
        rndc_host: {{ DNS_SERVER_IP }}
        rndc_port: 953
        rndc_key_file: /etc/rndc.key
        rndc_config_file: /etc/rndc.conf  
Populate Designate's pools: su -s /bin/sh -c ""designate-manage pool update"" designate 
Start Designate central and API services: systemctl enable --now designate-central designate-api 
Verify Designate's services are up: # openstack dns service list

+--------------+--------+-------+--------------+
| service_name | status | stats | capabilities |
+--------------+--------+-------+--------------+
| central      | UP     | -     | -            |
| api          | UP     | -     | -            |
| mdns         | UP     | -     | -            |
| worker       | UP     | -     | -            |
| producer     | UP     | -     | -            |
+--------------+--------+-------+--------------+  
Configure OpenStack Neutron with external DNS
Configure iptables for Designate services: # iptables -I INPUT -p tcp -m multiport --dports 9001 -m comment --comment ""designate incoming"" -j ACCEPT
       
# iptables -I INPUT -p tcp -m multiport --dports 5354 -m comment --comment ""Designate mdns incoming"" -j ACCEPT
       
# iptables -I INPUT -p tcp -m multiport --dports 53 -m comment --comment ""bind incoming"" -j ACCEPT
       
        
# iptables -I INPUT -p udp -m multiport --dports 53 -m comment --comment ""bind/powerdns incoming"" -j ACCEPT
       
# iptables -I INPUT -p tcp -m multiport --dports 953 -m comment --comment ""rndc incoming - bind only"" -j ACCEPT
       
# service iptables save; service iptables restart
# setsebool named_write_master_zones 1  
Edit the [default] section of /etc/neutron/neutron.conf: external_dns_driver = designate 
Add the [designate] section in /_etc/_neutron/neutron.conf: [designate]
url = http://{{ CONTROLLER_SERVER_IP }}:9001/v2  ## This end point of designate
auth_type = password
auth_url = http://{{ CONTROLLER_SERVER_IP }}:5000
username = designate
password = rhlab123
project_name = services
project_domain_name = Default
user_domain_name = Default
allow_reverse_dns_lookup = True
ipv4_ptr_zone_prefix_size = 24
ipv6_ptr_zone_prefix_size = 116  
Edit dns_domain in neutron.conf: dns_domain = rhlab.dev.

# systemctl restart neutron-*  
Add dns to the list of Modular Layer 2 (ML2) drivers in /etc/neutron/plugins/ml2/ml2_conf.ini: extension_drivers=port_security,qos,dns 
Add zone in Designate: # openstack zone create –email=admin@rhlab.dev rhlab.dev.  Add a new record in zone rhlab.dev: # openstack recordset create --record '192.168.1.230' --type A rhlab.dev. Test 
Designate should now be installed and configured.

Topics"
OpenStack$cloud,"What's happening in the OpenStack community?
In many ways, 2018 was a transformative year for the OpenStack Foundation.
Since 2010, the OpenStack community has been building open source software to run cloud computing infrastructure. Initially, the focus was public and private clouds, but open infrastructure has been pulled into many new important use cases like telecoms, 5G, and manufacturing IoT.
As OpenStack software matured and grew in scope to support new technologies like bare metal provisioning and container infrastructure, the community widened its thinking to embrace users who deploy and run the software in addition to the developers who build the software. Questions like, ""What problems are users trying to solve?"" ""Which technologies are users trying to integrate?"" and ""What are the gaps?"" began to drive the community's thinking and decision making.
In response to those questions, the OSF reorganized its approach and created a new ""open infrastructure"" framework focused on use cases, including edge, container infrastructure, CI/CD, and private and hybrid cloud. And, for the first time, the OSF is hosting open source projects outside of the OpenStack project.
Following are three highlights from the OSF 2018 Annual Report; I encourage you to read the entire report for more detailed information about what's new.
Pilot projects
On the heels of launching Kata Containers in December 2017, the OSF launched three pilot projects in 2018—Zuul, StarlingX, and Airship—that help further our goals of taking our technology into additional relevant markets. Each project follows the tenets we consider key to the success of true open source, the Four Opens: open design, open collaboration, open development, and open source. While these efforts are still new, they have been extremely valuable in helping us learn how we should expand the scope of the OSF, as well as showing others the kind of approach we will take.
While the OpenStack project remained at the core of the team's focus, pilot projects are helping expand usage of open infrastructure across markets and already benefiting the OpenStack community. This has attracted dozens of new developers to the open infrastructure community, which will ultimately benefit the OpenStack community and users.
There is direct benefit from these contributors working upstream in OpenStack, such as through StarlingX, as well as indirect benefit from the relationships we've built with the Kubernetes community through the Kata Containers project. Airship is similarly bridging the gaps between the Kubernetes and OpenStack ecosystems. This shows users how the technologies work together. A rise in contributions to Zuul has provided the engine for OpenStack CI and keeps our development running smoothly.
Containers collaboration
In addition to investing in new pilot projects, we continued efforts to work with key adjacent projects in 2018, and we made particularly good progress with Kubernetes. OSF staffer Chris Hoge helps lead the cloud provider special interest group, where he has helped standardize how Kubernetes deployments expect to run on top of various infrastructure. This has clarified OpenStack's place in the Kubernetes ecosystem and led to valuable integration points, like having OpenStack as part of the Kubernetes release testing process.

Additionally, OpenStack Magnum was certified as a Kubernetes installer by the CNCF. Through the Kata Containers community, we have deepened these relationships into additional areas within the container ecosystem resulting in a number of companies getting involved for the first time.
Evolving events
We knew heading into 2018 that the environment around our events was changing and we needed to respond. During the year, we held two successful project team gatherings (PTGs) in Dublin and Denver, reaching capacity for both events while also including new projects and OpenStack operators. We held OpenStack Summits in Vancouver and Berlin, both experiencing increases in attendance and project diversity since Sydney in 2017, with each Summit including more than 30 open source projects. Recognizing this broader audience and the OSF's evolving strategy, the OpenStack Summit was renamed the Open Infrastructure Summit, beginning with the Denver event coming up in April.
In 2018, we boosted investment in China, onboarding a China Community Manager based in Shanghai and hosting a strategy day in Beijing with 30+ attendees from Gold and Platinum Members in China. This effort will continue in 2019 as we host our first Summit in China: the Open Infrastructure Summit Shanghai in November.
We also worked with the community in 2018 to define a new model for events to maximize participation while saving on travel and expenses for the individuals and companies who are increasingly stretched across multiple open source communities. We arrived at a plan that we will implement and iterate on in 2019 where we will collocate PTGs as standalone events adjacent to our Open Infrastructure Summits.
Looking ahead
We've seen impressive progress, but the biggest accomplishment might be in establishing a framework for the future of the foundation itself. In 2018, we advanced the open infrastructure mission by establishing OSF as an effective place to collaborate for CI/CD, container infrastructure, and edge computing, in addition to the traditional public and private cloud use cases. The open infrastructure approach opened a lot of doors in 2018, from the initial release of software from each pilot project, to live 5G demos, to engagement with hyperscale public cloud providers.
Ultimately, our value comes from the effectiveness of our communities and the software they produce. As 2019 unfolds, our community is excited to apply learnings from 2018 to the benefit of developers, users, and the commercial ecosystem across all our projects.

"
cloud$MySQL,"Meet TiDB: An open source NewSQL database
5 key differences between MySQL and TiDB for scaling in the cloud
As businesses adopt cloud-native architectures, conversations will naturally lead to what we can do to make the database horizontally scalable. The answer will likely be to take a closer look at TiDB.
TiDB is an open source NewSQL database released under the Apache 2.0 License. Because it speaks the MySQL protocol, your existing applications will be able to connect to it using any MySQL connector, and most SQL functionality remains identical (joins, subqueries, transactions, etc.).
Step under the covers, however, and there are differences. If your architecture is based on MySQL with Read Replicas, you'll see things work a little bit differently with TiDB. In this post, I'll go through the top five key differences I've found between TiDB and MySQL.
1. TiDB natively distributes query execution and storage
With MySQL, it is common to scale-out via replication. Typically you will have one MySQL master with many slaves, each with a complete copy of the data. Using either application logic or technology like ProxySQL, queries are routed to the appropriate server (offloading queries from the master to slaves whenever it is safe to do so).

Scale-out replication works very well for read-heavy workloads, as the query execution can be divided between replication slaves. However, it becomes a bottleneck for write-heavy workloads, since each replica must have a full copy of the data. Another way to look at this is that MySQL Replication scales out SQL processing, but it does not scale out the storage. (By the way, this is true for traditional replication as well as newer solutions such as Galera Cluster and Group Replication.)

TiDB works a little bit differently:
Query execution is handled via a layer of TiDB servers. Scaling out SQL processing is possible by adding new TiDB servers, which is very easy to do using Kubernetes ReplicaSets. This is because TiDB servers are stateless; its TiKV storage layer is responsible for all of the data persistence.
The data for tables is automatically sharded into small chunks and distributed among TiKV servers. Three copies of each data region (the TiKV name for a shard) are kept in the TiKV cluster, but no TiKV server requires a full copy of the data. To use MySQL terminology: Each TiKV server is both a master and a slave at the same time, since for some data regions it will contain the primary copy, and for others, it will be secondary.
TiDB supports queries across data regions or, in MySQL terminology, cross-shard queries. The metadata about where the different regions are located is maintained by the Placement Driver, the management server component of any TiDB Cluster. All operations are fully ACID compliant, and an operation that modifies data across two regions uses a two-phase commit.
For MySQL users learning TiDB, a simpler explanation is the TiDB servers are like an intelligent proxy that translates SQL into batched key-value requests to be sent to TiKV. TiKV servers store your tables with range-based partitioning. The ranges automatically balance to keep each partition at 96MB (by default, but configurable), and each range can be stored on a different TiKV server. The Placement Driver server keeps track of which ranges are located where and automatically rebalances a range if it becomes too large or too hot.
This design has several advantages of scale-out replication:
It independently scales the SQL Processing and Data Storage tiers. For many workloads, you will hit one bottleneck before the other.
It incrementally scales by adding nodes (for both SQL and Data Storage).
It utilizes hardware better. To scale out MySQL to one master and four replicas, you would have five copies of the data. TiDB would use only three replicas, with hotspots automatically rebalanced via the Placement Driver.
2. TiDB's storage engine is RocksDB
MySQL's default storage engine has been InnoDB since 2010. Internally, InnoDB uses a B+tree data structure, which is similar to what traditional commercial databases use.
By contrast, TiDB uses RocksDB as the storage engine with TiKV. RocksDB has advantages for large datasets because it can compress data more effectively and insert performance does not degrade when indexes can no longer fit in memory.
Note that both MySQL and TiDB support an API that allows new storage engines to be made available. For example, Percona Server and MariaDB both support RocksDB as an option.
3. TiDB gathers metrics in Prometheus/Grafana
Tracking key metrics is an important part of maintaining database health. MySQL centralizes these fast-changing metrics in Performance Schema. Performance Schema is a set of in-memory tables that can be queried via regular SQL queries.
With TiDB, rather than retaining the metrics inside the server, a strategic choice was made to ship the information to a best-of-breed service. Prometheus+Grafana is a common technology stack among operations teams today, and the included graphs make it easy to create your own or configure thresholds for alarms.
4. TiDB handles DDL significantly better
If we ignore for a second that not all data definition language (DDL) changes in MySQL are online, a larger challenge when running a distributed MySQL system is externalizing schema changes on all nodes at the same time. Think about a scenario where you have 10 shards and add a column, but each shard takes a different length of time to complete the modification. This challenge still exists without sharding, since replicas will process DDL after a master.
TiDB implements online DDL using the protocol introduced by the Google F1 paper. In short, DDL changes are broken up into smaller transition stages so they can prevent data corruption scenarios, and the system tolerates an individual node being behind up to one DDL version at a time.
5. TiDB is designed for HTAP workloads
The MySQL team has traditionally focused its attention on optimizing performance for online transaction processing (OLTP) queries. That is, the MySQL team spends more time making simpler queries perform better instead of making all or complex queries perform better. There is nothing wrong with this approach since many applications only use simple queries.
TiDB is designed to perform well across hybrid transaction/analytical processing (HTAP) queries. This is a major selling point for those who want real-time analytics on their data because it eliminates the need for batch loads between their MySQL database and an analytics database.
Conclusion
These are my top five observations based on 15 years in the MySQL world and coming to TiDB. While many of them refer to internal differences, I recommend checking out the TiDB documentation on MySQL Compatibility. It describes some of the finer points about any differences that may affect your applications.

"
DevOps$cloud,"How to develop functions-as-a-service with Apache OpenWhisk
Write your functions in popular languages and build components using containers.
Apache OpenWhisk is a serverless, open source cloud platform that allows you to execute code in response to events at any scale. Apache OpenWhisk offers developers a straightforward programming model based on four concepts: actions, packages, triggers, and rules.
Actions are stateless code snippets that run on the Apache OpenWhisk platform. You can develop an action (or function) via JavaScript, Swift, Python, PHP, Java, or any binary-compatible executable, including Go programs and custom executables packaged as Linux containers. Actions can be explicitly invoked or run in response to an event. In either case, each run of an action results in an activation record that is identified by a unique activation ID. The input to an action and the result of an action are a dictionary of key-value pairs, where the key is a string and the value a valid JSON value.
Packages provide event feeds; anyone can create a new package for others to use.
Triggers associated with those feeds fire when an event occurs, and developers can map actions (or functions) to triggers using rules.
The following commands are used to create, update, delete, and list an action in Apache OpenWhisk:
Usage:
  wsk action [command]

Available Commands:
  create      create a new action
  update      update an existing action, or create an action if it does not exist
  invoke      invoke action
  get         get action
  delete      delete action
  list        list all actions in a namespace or actions contained in a package
Set up OpenWhisk
Let’s explore how that works in action. First, download Minishift to create a single-node local OKD (community distribution of Kubernetes that powers Red Hat OpenShift) cluster on your workstation:
$ minishift start --vm-driver=virtualbox --openshift-version=v3.10.0
Once Minishift is up and running, you can log in with admin /admin and create a new project (namespace). The project OpenWhisk on OpenShift provides the OpenShift templates required to deploy Apache OpenWhisk:
$ eval $(minishift oc-env) && eval $(minishift docker-env)
$ oc login $(minishift ip):8443 -u admin -p admin
$ oc new-project faas
$ oc project -q
$ oc process -f https://git.io/openwhisk-template | oc create -f -
Apache OpenWhisk is comprised of many components that must start up and sync with each other, and this process can take several minutes to stabilize. The following command will wait until the component pods are running:
$ while $(oc get pods -n faas controller-0 | grep 0/1 > /dev/null); do sleep 1; done
You can also watch the status with this:
$ while [ -z ""`oc logs controller-0 -n faas 2>&1 | grep ""invoker status changed""`"" ]; do sleep 1; done
Develop a simple Java Action
Maven archetype is a Maven project templating toolkit. In order to create a sample Java Action project, you won't refer to central Maven archetype, but you need to generate your own Maven archetype first as below:
$ git clone https://github.com/apache/incubator-openwhisk-devtools
$ cd incubator-openwhisk-devtools/java-action-archetype
$ mvn -DskipTests clean install
$ cd $PROJECT_HOM
Let’s now create a simple Java Action, deploy it to OpenWhisk, and finally, invoke it to see the result. Create the Java Action as shown below:
$ mvn archetype:generate \
  -DarchetypeGroupId=org.apache.openwhisk.java \
  -DarchetypeArtifactId=java-action-archetype \
  -DarchetypeVersion=1.0-SNAPSHOT \
  -DgroupId=com.example \
  -DartifactId=hello-openwhisk \
  -Dversion=1.0-SNAPSHOT \
  -DinteractiveMode=false
Next, build the Java application and deploy to OpenWhisk on Minishift locally:
$ cd hello-openwhisk
$ mvn clean package
$ wsk -i action create hello-openwhisk target/hello-openwhisk.jar --main com.example.FunctionApp
Having created the function hello-openwhisk, verify the function by invoking it:
$ wsk -i action invoke hello-openwhisk --result
As all the OpenWhisk actions are asynchronous, you need to add --result to get the result shown on the console. Successful execution of the command will show the following output:
{""greetings"":  ""Hello! Welcome to OpenWhisk"" }
Conclusion
With Apache OpenWhisk, you can write your functions in popular languages such as NodeJS, Swift, Java, Go, Scala, Python, PHP, and Ruby and build components using containers. It easily supports many deployment options, both locally and within cloud infrastructures such as Kubernetes and OpenShift.
"
DevOps$Agile,"Testing Small Scale Scrum in the real world
We plan to test the Small Scale Scrum framework in real-world projects involving small teams.
Scrum is built on the three pillars of inspection, adaptation, and transparency. Our empirical research is really the starting point in bringing scrum, one of the most popular agile implementations, to smaller teams. As presented in the diagram below, we are now taking time to inspect this framework and principles by testing them in real-world projects.
We plan to implement Small Scale Scrum in several upcoming projects. Our test candidates are customers with real projects where teams of one to three people will undertake short-lived projects (ranging from a few weeks to three months) with an emphasis on quality and outputs. Individual projects, such as final-year projects (over 24 weeks) that are a capstone project after four years in a degree program, are almost exclusively completed by a single person. In projects of this nature, there is an emphasis on the project plan and structure and on maximizing the outputs that a single person can achieve.
[Download the Introduction to Small Scale Scrum guide]
We plan to metricize and publish the results of these projects and hold several retrospectives with the teams involved. We are particularly interested in metrics centered around quality, with a particular emphasis on quality in a software engineering context and management, both project management through the lifecycle with a customer and management of the day-to-day team activities and the delivery, release, handover, and signoff process.
Ultimately, we will retrospectively analyze the overall framework and principles and see if the Manifesto we envisioned holds up to the reality of executing a project with small numbers. From this data, we will produce the second version of Small Scale Scrum and begin a cyclic pattern of inspecting the model in new projects and adapting it again.
We want to do all of this transparently. This series of articles is one window into the data, the insights, the experiences, and the reality of running scrum for small teams whose everyday challenges include context switching, communication, and the need for a quality delivery. A follow-up series of articles is planned to examine the outputs and help develop the second edition of Small Scale Scrum entirely in the community.
We also plan to attend conferences and share our knowledge with the Agile community. Our first conference will be Agile 2019 where the evolution of Small Scale Scrum will be further explored as an Experience Report. We are advising colleges and sharing our structure and approach to managing and executing final-year projects. All our outputs will be freely available in the open source way.
Given the changes to recommended team sizes in the Scrum Guide, our long-term goal and vision is to have the Scrum Guide reflect that teams of one or more people occupying one or more roles within a project are capable of following scrum.

"
RaspberryPi$gaming,"Resurrecting the Amiga on the Raspberry Pi
Take a trip back to your childhood games by emulating the Amiga on Linux.
I am a fan of retro gaming and preserving the computer history that lead us to where we are today. I think most programmers have a machine that helped develop their love for programming; for me that was the Commodore Amiga. I am currently restoring a couple of Amigas to get them back to working order. This will take me some time, but I wanted something I could get up and running quickly so I can play with the OS and finish a couple of games I never completed as a kid.
When I started working at Red Hat, I found a passionate community around retro systems and especially the Amiga. We have a breakout room in our head office dedicated to old systems, and my fellow Red Hatters have donated games and consoles to it.
Can I put an Amiga on that?
As a Linux fan, I often find myself looking at some obscure bit of hardware and thinking, ""I wonder if I can use it with Linux?"" or more often, ""I wonder if I can put Linux on that?"" I blame my youth of playing adventure games and solving puzzles for that mentality. Recently, I thought, ""I wonder if I can put an Amiga emulator on that?"" This article is the result of a weekend spent doing just that. It is by no means a complete guide to all things you can do with emulation for the Amiga, but it should be a good start to get you going. I'll provide links to all the software and go over some of the things I learned on my retro weekend, as well as some guides to take it to the next step.
Here are the devices I got Amiga emulation to run on:
Lenovo 500e Chromebook
Nexus 7
Raspberry Pi
Samsung Galaxy Tab 10
Ubuntu PC
Windows PC
Amiga emulators
UAE is an open source emulator released in 1995. The acronym initially stood for Unusable Amiga Emulator, but now it's generally called Universal Amiga Emulator, Ultimate Amiga Emulator, or Ubiquitous Amiga Emulator. Or more simply, UAE. Most of the emulators for Android, Linux, Chrome, MacOS, and Windows are ports or forks from this project or grew from the emulation community. I also use UAE on my Chromebook, which supports Android apps, to emulate my Amiga.

I prefer FS-UAE, which is available on a number of platforms. On Crostini (Linux on Chromebook) or Ubuntu, you can install it with the command:
sudo apt-get install fs-uae fs-uae-arcade fs-uae-launcher
A caveat on Chromebooks and Crostini: Chromebooks don't yet support audio on Linux apps, but it is a feature that's coming in ChromeOS 74. I prefer Linux-based emulators, so I am using the Amiga Forever Essentials Android app for now. I also have the Linux app installed, ready for when audio and GPU acceleration for Linux apps become available.
Amiga kickstarts
All the emulators I used are open source. However, in order to run any Amiga software, you need a kickstart ROM that is copyrighted and only available under license. You can buy one from Cloanto and they are bundled with Amiga Forever. To get up and running quickly, you can buy the Android app Amiga Forever Essentials, which was only $1.99 when I wrote this. If you are an Amiga fan, it's worth buying Amiga Forever, as it contains a wealth of demos, games, and pre-configured systems and has everything you could want to emulate a complete system.
The FS-UAE emulator can scan for your install CD of Amiga Forever and automatically copy the kickstart ROMs to their own configuration folder. On my Ubuntu desktop, FS-UAE creates configuration folders under Documents.
Amiga on Android and Chromebooks
Setting up the emulator on my Samsung Tab 10, Nexus 7, and Chromebook was straightforward. There are a number of emulators for Android, and if you buy the Amiga Forever Essentials app, it will install the commercial kickstart ROMs to a shared folder that the emulators can access. Just download an emulator and you are ready to start configuring your system.
The Amiga Forever Essentials app works on Android with the UAE4Droid and Uae4arm emulators (I prefer Uae4arm). One advantage of emulating the Amiga on Android is controller support. I use my 8bitdo controller on my tablet and it works with no fuss. 8bitdo works well on RetroPie but can be unreliable through Bluetooth on the Amiga distros Amibian and DietPi. I prefer a controller with a dongle or wire, as it minimizes lag and works every time.
Amiga on Raspberry Pi
For the Raspberry Pi, there are two dedicated distros for Amiga: Amiberry, running on DietPi, and Amibian. Or you can install your emulator of choice from the standard Raspbian repo; you are not limited to either of these options. Configuration is almost identical on both. I tried both, and my preference is Amibian, for no other reasons than it's closer to Raspbian, I am more comfortable with it, and configuration is well thought out. Both distros are very lean and don't contain a lot of packages to ""bloat"" the installation. They generally use SDL to run with no need for a window manager or running an X Window session. They are fast to boot and lean on resources. Perfect!
Hardware
Raspberry Pi 3B+
Logitech F71 gamepad (I think a dongle is easier than Bluetooth on RetroPie)
Smraza Case
HDMI & USB cables
Speakers
MicroSD Card (I used a 32GB Samsung MicroSD)
Flash drive (optional)
Keyboard
Mouse or trackball
Getting started
The first step is installing Amibian. Download the disk image from the Amibian website—click Download from the menu and save the file to your machine.
Now, flash this image to the MicroSD card. The easiest way is to use Etcher, which is available on Linux, Windows, and MacOS.
Download and install Etcher
Put your MicroSD card into your machine
Start Etcher
Select the unzipped image file you downloaded from Amibian
Select your MicroSD card
Click the Flash button
This will flash the MicroSD card with the Amibian image. I use a USB card reader so I can plug it in as a normal flash drive.
Put your Raspberry Pi together and insert the MicroSD card. Then power it up!
Initial setup
Next, expand the filesystem to use the entire MicroSD card. Click the Quit button and drop back to a command prompt.
Since you are logged in as root, open the Raspbian config tool:
raspi-config

Must-do changes:
Go to the Advanced Options menu
Choose Expand Filesystem
Optional quality-of-life changes
Set up your wireless network by following this guide from the Raspberry Pi Foundation
Enable SSH in Network Options » SSH, as this is handy for adding files to your Pi using an SFTP client such as FileZilla
In Localization options, change your localization and keyboard layout if needed; the default is the UK (Great Britain)
Update to the latest version, always a good thing to do
Configure the emulator
Copy the kickstart ROMs to your Raspberry Pi. If you have a working network, you can use FileZilla or another SFTP client to transfer the files. (Since I'm running Amiga Forever under Windows, I use it copy the ROMs from my Windows machine to my Pi.) Amiga Forever also comes with a large selection of games and demos. They are stored in the RP9 format, which are simply ZIP files; you can extract them and copy the ADF (Amiga Disk Format) file to your Pi to play them from there.
If you are storing the kickstart files and your games on a USB flash drive, you can configure the paths to your kickstarts from the main configuration window.
If you purchased your kickstarts from Cloanto, you must copy the rom.key file with the kickstart ROMs. They are encrypted and the key unlocks them and acts as your license. The ROMs won't work without the key.
If your kickstarts are stored on a USB drive, you can change the System ROMs folder to point to your USB. The path starts with /media/usb0 followed by the folder on the drive where your kickstarts are stored. So, if you have a folder called kickstarts on your flash drive, the path would be /media/usb0/kickstarts.
Dan Wood has an amazing walkthrough video on how to get to this point. His YouTube channel has great content on original Amigas and emulation and is well worth checking out if you are an Amiga fan.
RetroPie
In addition to the emulators mentioned above, for the Raspberry Pi, I also recommend RetroPie, which includes Amiberry for the Amiga and makes it easier to boot games. Also, FS-UAE-Arcade is a nice addition to any setup, as it creates a kiosk around your game profiles, making it easier to launch them.
To get them running, use the UAE launcher to configure a default system in RetroPie. Then pop back to RetroPie and launch an Amiga game. Before it loads, you'll get a launcher screen with the option to configure settings by pressing the Enter key. Choose your profile, then all games will use that profile to launch your games.
Play games
The reason we went through all this is so we can play games, right? So how do we add them? I normally store my games in the floppies folder on my MicroSD card, under amiga_files. Games are in ADF (Amiga Disk Format). Think of them as a snapshot of an Amiga diskette. Some games require multiple disks, and there are two ways to handle them: you can load each disk under the ""floppies"" section of the emulator, or you can ZIP the files and load the ZIP file onto the ""floppy."" They will automatically be extracted when you hit the Reset button. If you ever need to add a floppy mid-game, press F4 to pause the game and return you to the emulator window. From there, you can add the new disk under ""floppies"" and hit the Resume button.
The Amiga has a rich history and a very active community—there are still games being made for these amazing machines. Aquabyss is a new game released in 2019, and demos are still being developed.
Beyond games
My interest in the Amiga goes beyond playing games. Since the days of using my A2000 to learn programming, I've been interested in the operating system. Here's some places online to learn more about this iconic system and how to push it further.
AmigaOS Workbench 4.1
Installing Workbench 3.1 / 3.5 on a Raspberry Pi
Amiga Forever
Classic Amiga Workbench
Amiga.org
DHL's Amiga Archive 
I hope you have fun playing with your Amiga on your Raspberry Pi! I'd love to hear your feedback in the comments.

"
RaspberryPi$gaming,"Emulators and Native Linux games on the Raspberry Pi
The Raspberry Pi is a great platform for gaming; learn how in the ninth article in our series on getting started with the Raspberry Pi.
Back in the fifth article in our series on getting started with the Raspberry Pi, I mentioned Minecraft as a way to teach kids to program using a gaming platform. Today we'll talk about other ways you can play games on your Raspberry Pi, as it's a great platform for gaming—with and without emulators.
Gaming with emulators
Emulators are software that allow you to play games from different systems and different decades on your Raspberry Pi. Of the many emulators available today, the most popular for the Raspberry Pi is RetroPi. You can use it to play games from systems such as Apple II, Amiga, Atari 2600, Commodore 64, Game Boy Advance, and many others.
If RetroPi sounds interesting, check out these instructions on how to get started, and start having fun today!
Native Linux games
There are also plenty of native Linux games available on Raspbian, Raspberry Pi's operating system. Make Use Of has a great article on how to play 10 old favorites like Doom and Nuke Dukem 3D on the Raspberry Pi.
You can also use your Raspberry Pi as a game server. For example, you can set up Terraria, Minecraft, and QuakeWorld servers on the Raspberry Pi.

"
Python$gaming,"PyGame Zero: Games without boilerplate
Say goodbye to boring boilerplate in your game development with PyGame Zero.
Python is a good beginner programming language. And games are a good beginner project: they are visual, self-motivating, and fun to show off to friends and family. However, the most common library to write games in Python, PyGame, can be frustrating for beginners because forgetting seemingly small details can easily lead to nothing rendering.
Until people understand why all the parts are there, they treat many of them as ""mindless boilerplate""—magic paragraphs that need to be copied and pasted into their program to make it work.
PyGame Zero is intended to bridge that gap by putting a layer of abstraction over PyGame so it requires literally no boilerplate.
When we say literally, we mean it.
This is a valid PyGame Zero file:
# This comment is here for clarity reasons
We can run put it in a game.py file and run:
$ pgzrun game.py
This will show a window and run a game loop that can be shut down by closing the window or interrupting the program with CTRL-C.
This will, sadly, be a boring game. Nothing happens.
To make it slightly more interesting, we can draw a different background:
def draw():
    screen.fill((255, 0, 0))
This will make the background red instead of black. But it is still a boring game. Nothing is happening. We can make it slightly more interesting:
colors = [0, 0, 0]

def draw():
    screen.fill(tuple(colors))

def update():
    colors[0] = (colors[0] + 1) % 256
This will make a window that starts black, becomes brighter and brighter red, then goes back to black, over and over again.
The update function updates parameters, while the draw function renders the game based on these parameters.
However, there is no way for the player to interact with the game! Let's try something else:
colors = [0, 0, 0]

def draw():
    screen.fill(tuple(colors))

def update():
    colors[0] = (colors[0] + 1) % 256

def on_key_down(key, mod, unicode):
    colors[1] = (colors[1] + 1) % 256
Now pressing keys on the keyboard will increase the ""greenness.""
These comprise the three important parts of a game loop: respond to user input, update parameters, and re-render the screen.
PyGame Zero offers much more, including functions for drawing sprites and playing sound clips.
Try it out and see what type of game you can come up with!

"